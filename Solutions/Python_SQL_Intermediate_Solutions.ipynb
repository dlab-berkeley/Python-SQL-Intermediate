{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1a0aee",
   "metadata": {},
   "source": [
    "# SQL for Data Analysis  \n",
    "## Intermediate Workshop\n",
    "*D‚ÄëLab, UC¬†Berkeley*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7ed73-be41-41e1-a27b-75904a6b04eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Prerequisites for SQL Intermediate Workshop</b><br><br>\n",
    "Completion of \"SQL for Data Analysis: Introductory Workshop\" or equivalent experience:\n",
    "<ul>\n",
    "<li>Understanding of basic SQL syntax including SELECT, FROM, WHERE, GROUP BY</li>\n",
    "<li>Familiarity with basic data filtering and sorting in SQL</li>\n",
    "<li>Experience with simple aggregations (COUNT, SUM, AVG)</li>\n",
    "</ul>\n",
    "    \n",
    "These prerequisites ensure participants have the foundational knowledge needed to succeed in learning the more advanced concepts like JOINs, subqueries, CTEs, and window functions covered in the second workshop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528f6e4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Relational Joins](#joins)\n",
    "3. [Subqueries](#subqueries)\n",
    "4. [Common Table Expressions (CTEs)](#ctes)\n",
    "5. [Pivoting and Unpivoting](#pivot)\n",
    "6. [Window Functions](#window)\n",
    "7. [Key Points](#keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5e214-796a-48e2-8af6-5f8e39ddec79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Learning Goals</b><br><br>\n",
    "By the end of this workshop you will be able to:\n",
    "<ul>\n",
    "<li>Combine data from multiple tables using different types of JOINs (INNER, LEFT, SELF).</li>\n",
    "<li>Understand the role of primary and foreign keys in establishing table relationships.</li>\n",
    "<li>Write and use subqueries to break down complex queries with multiple logical steps.</li>\n",
    "<li>Simplify complex queries using Common Table Expressions (CTEs).</li>\n",
    "<li>Transform data between row and column orientations with pivoting and unpivoting techniques.</li>\n",
    "<li>Apply window functions to perform calculations across specified sets of rows.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb5e2e4-0c1e-48c0-9fef-029bcf52fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some packages that we will need:\n",
    "\n",
    "from IPython.display import SVG\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad7acb",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1 ¬∑ Introduction \n",
    "\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "In the last workshop we covered many different operations one can perform in a given table. However, going back to the idea of Relational Databases, we often won't have all of the information we need on a single table, and we will need to find a way of cross-referencing the information from two (or more) different tables together. This processing is called \"Joining\", and it is an essential aspect of querying with SQL. \n",
    "\n",
    "Another common procedure is to first modify the data, and then perform some operations in the modified data. This is what Subqueries are used for - they allow us to quickly reference a modified version of the dataset we are querying, or compare information between two tables without joining them. Common Table Expressions are a way of doing many subqueries simultaneously, or very complex subqueries, while keeping things readable and organized.\n",
    "\n",
    "Lastly, we will cover a class of operations called Window Functions, which are some of the most powerful tools in SQL. Instead of performing the same function to all rows, they allow the functions to only be applied to *windows*, which are a subset of the observations that are related in a prespecified way. This allows us to calculate summary statistics similar to what we did using ` GROUP BY `, but still retain the original disaggregated information.\n",
    "\n",
    "üìù **Poll 1:** How confident do you feel about everything we covered in Part 1 (basic SELECT / JOIN / GROUP BY)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b63d8",
   "metadata": {},
   "source": [
    "<a id='joins'></a>\n",
    "## 2 ¬∑ Relational¬†Joins \n",
    "\n",
    "Estimated Time: 35 minutes\n",
    "\n",
    "\n",
    "**Purpose:** ```JOINs``` allow us to combine information from multiple different tables into one. When used with Querying, this also provides a way of retaining only the information required for a particular analysis into a single, organized table, even if originally this information was spread out  \n",
    "\n",
    "**Example:** Say that we have two tables, one recording the information about Managers, and another one with the information about Employees. Each employee produces a certain amount of revenue to the firm, but a higher-up is interested in understanding what Manager had direct employees that produce the most revenue. \n",
    "\n",
    "If we had a table that listed each manager, their respective employees and how much they earned, we could use ```SUM``` + ```GROUP BY``` to quickly do this analysis. But we don't - the information is stored in two separate tables. This is exactly the context of `JOIN`s - creating a new table combining the information from two (or more) other tables. Here is a quick diagram to understand what is happening:\n",
    "\n",
    "![Entity Relationship diagram linking Employees, Departments and Managers tables with Primary and Foreign Key arrows to show one-to-many relationships.](../Images/database-relationship-diagram2.svg)\n",
    "---\n",
    "\n",
    "Notice that in this example, employee_id would be the *primary key* of each table - a unique identifier for each row - while the column \"report_to\" on the Employees table serves as a *foreign key* - it serves as a reference to values on the table Managers which can be used to cross-reference information.\n",
    "\n",
    "Notice also that we didn't keep all of the information from each table in the end result. By querying the table after joining, we can decide what to retain, and even perform operations or filtering directly! This again illustrates the principle we laid out at the beginning - SQL is a tool capable of preparing large amounts of data to be analyzed with libraries such as pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9266c29-4d4f-442d-8eaf-86ecd62d131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables available in database:\n",
      "\n",
      "['customers', 'income', 'demo_customers', 'demo_orders']\n",
      "\n",
      " JOIN Example - Customers with Income: \n",
      "\n",
      "    customer_name       city  total_spent income_source  income_amount\n",
      "0    Ahmed Hassan     Sydney       177.45    Consulting         1500.0\n",
      "1    Ahmed Hassan     Sydney       177.45        Rental          800.0\n",
      "2    Ahmed Hassan     Sydney       177.45        Salary         3500.0\n",
      "3  Diego Martinez  Amsterdam        13.09        Salary         2700.0\n",
      "4     Elena Popov     Madrid        56.37       Pension         1800.0\n",
      "5   Lars Andersen       None       125.88        Salary         4500.0\n",
      "6          Li Wei      Tokyo       155.98    Investment          200.0\n",
      "7          Li Wei      Tokyo       155.98        Salary            NaN\n",
      "8   Mary Williams       None      1183.71     Part-time         1200.0\n",
      "9   Sarah Johnson     Berlin       301.15        Salary            NaN\n"
     ]
    }
   ],
   "source": [
    "# Using the unified dataset for JOIN examples\n",
    "# First, ensure we have both customers and income tables loaded\n",
    "print(\"Tables available in database:\\n\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print([table[0] for table in cursor.fetchall()])\n",
    "\n",
    "# Basic JOIN example - customers with their income\n",
    "join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name AS customer_name,\n",
    "    c.city,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    i.income_source,\n",
    "    i.amount AS income_amount\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    income i ON c.name = i.name\n",
    "WHERE \n",
    "    c.items_purchased IS NOT NULL \n",
    "    AND c.price_per_item IS NOT NULL\n",
    "ORDER BY \n",
    "    c.name, i.income_source\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n JOIN Example - Customers with Income: \\n\")\n",
    "result = pd.read_sql_query(join_query, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c194c-2a3c-4b7d-a453-95fe9a552a71",
   "metadata": {},
   "source": [
    "Let's Break down exactly what is happening as we join tables using the example below::\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    m.name AS \"Manager Name\",\n",
    "    e.name AS \"Employee Name\",\n",
    "    e.revenue AS \"Revenue\"\n",
    "FROM \n",
    "    Managers m \n",
    "JOIN \n",
    "    Employees e \n",
    "ON \n",
    "    m.employee_id = e.reports_to\n",
    "ORDER BY\n",
    "    m.name, e.name\n",
    "```\n",
    "\n",
    "Let's start from the inside out:\n",
    "- The first operation being performed is \"FROM\". This is telling us which Table we will be using as our \"main\" table - which in SQL is usually referred to as the Left Table. It is useful to think of any JOIN operation as acting \"to the right\" of this table.\n",
    "    - Notice that we include a \"m\" right next to the name of the table we are importing - this is what we call an alias. While this is not necessary for a simple JOIN statement like this, it help keep the query clean and organized. And, as we will see later on, it is necessary for more advanced JOIN statements (such as self-joins).\n",
    "- The second operation being performed is \"JOIN\". This is essentially telling SQL to consider a table that is a combination of the Left and Right tables.\n",
    "    - One very important aspect is that we must tell SQL how to merge these two tables - which is what we do by using the ON statement.\n",
    "    - In more advanced queries, ON statements can use very complex conditions, including compound ones using logical operators such as AND/OR/NOT, but for now we are just saying \"I want a row that combines the information from the two tables whenever the column \"employeed_id\" on the Manager table matches the \"reports_to\" column on the Employees table.\n",
    "    - Notice that we had to specify which table each column came from here - and we can already see why aliases can come in handy.\n",
    "    - A very important thing to notice is that, if there are multiple matches to the condition on the ON statement, any rows that satisfy it will be joined. This is what happened in our example - there are two rows in the Merged table that have Bob as the Manager Name, since he had two employees that reported to him.\n",
    "    - Another important thing to know is that, in a basic JOIN statement, any rows that go unmatched are not included in the final result. We will see how more advanced JOIN statements - such as LEFT JOIN - allow us to bypass this.\n",
    "- We then go to our SELECT statement. As before, this is just telling SQL which information we actually want it to retrieve for us. But, since the information now might come from more than one source, we again need to specify from which table each column is coming from.\n",
    "- The rest of the statements is similar to a basic Query - we can filter, order, limit/offset, etc - just like we did in the previous workshop\n",
    "\n",
    "\n",
    "![Flow diagram with FROM, JOIN, WHERE, SELECT and ORDER BY blocks connected by arrows to show query processing order.](../Images/sql-query-flow.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac684b1-e579-4026-9f06-aaa1f3802080",
   "metadata": {
    "language": "sql",
    "scrolled": true
   },
   "source": [
    "üèãÔ∏è‚Äç‚ôÇÔ∏è **Challenge:** Your First Real JOIN \n",
    "\n",
    "Individuall, the tables \"customers\" tells us who our clients are and \"orders\" tells us what they bought and when.\n",
    "\n",
    "Task: Using a single JOIN, produce a table that answers\n",
    "‚ÄúWhich orders were placed by which customer, in what country?‚Äù\n",
    "\n",
    "```sql\n",
    "-- Fill in the JOIN condition\n",
    "SELECT\n",
    "       o.order_id,\n",
    "       o.order_date,\n",
    "       c.name,\n",
    "       c.country\n",
    "FROM   orders    AS o\n",
    "JOIN   customers AS c\n",
    "       ON  _____________          \n",
    "ORDER  BY o.order_date;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b1c6d-555b-4fa2-a179-64c9463f4655",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Always make sure that your joining condition is present and accurate. Otherwise SQL will perform what we call a Cartesian Product - making every possible combination between the rows - which can become huge really fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adf4c70-2b4d-4565-9fe6-00e1a2e5e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartesian Product Demonstration:\n",
      "\n",
      "Number of customers: 25\n",
      "Number of income records: 22\n",
      "Cartesian product rows (WRONG): 550 (25 √ó 22 = every possible combination) \n",
      "\n",
      "Correct JOIN rows: 16 (only matching names)\n",
      "\n",
      "         name      city income_source  amount\n",
      "0  John Smith  New York        Salary  3000.0\n",
      "1  John Smith  New York     Freelance   500.0\n",
      "2  John Smith  New York        Salary  2500.0\n",
      "3  John Smith  New York         Bonus  1000.0\n",
      "4  John Smith  New York        Salary     NaN\n",
      "5  John Smith  New York    Investment   200.0\n",
      "6  John Smith  New York        Salary  3500.0\n",
      "7  John Smith  New York    Consulting  1500.0\n",
      "8  John Smith  New York        Rental   800.0\n",
      "9  John Smith  New York        Salary     NaN\n"
     ]
    }
   ],
   "source": [
    "# Example showing cartesian product vs correct join\n",
    "# Using our existing customers and income tables\n",
    "\n",
    "# Cartesian product (WRONG - no join condition)\n",
    "cartesian_query = \"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM customers, income;\n",
    "\"\"\"\n",
    "cartesian_result = pd.read_sql_query(cartesian_query, conn)\n",
    "\n",
    "# Correct join\n",
    "correct_join_query = \"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM customers c\n",
    "JOIN income i ON c.name = i.name;\n",
    "\"\"\"\n",
    "correct_result = pd.read_sql_query(correct_join_query, conn)\n",
    "\n",
    "# Get actual counts\n",
    "customer_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM customers\", conn).iloc[0,0]\n",
    "income_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM income\", conn).iloc[0,0]\n",
    "\n",
    "print(\"Cartesian Product Demonstration:\\n\")\n",
    "print(f\"Number of customers: {customer_count}\")\n",
    "print(f\"Number of income records: {income_count}\")\n",
    "print(f\"Cartesian product rows (WRONG): {cartesian_result.iloc[0,0]} ({customer_count} √ó {income_count} = every possible combination) \\n\")\n",
    "print(f\"Correct JOIN rows: {correct_result.iloc[0,0]} (only matching names)\\n\")\n",
    "\n",
    "# Show a sample of what cartesian product looks like\n",
    "cartesian_detail = \"\"\"\n",
    "SELECT c.name, c.city, i.income_source, i.amount\n",
    "FROM customers c, income i\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(cartesian_detail, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29c624-9513-47cc-ab73-84786761110e",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Always verify your JOIN produces the expected number of rows! A missing or incorrect JOIN condition can create a Cartesian product, multiplying your data exponentially. For example, joining two 1,000-row tables without proper conditions creates 1,000,000 rows! Always check row counts after JOINs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b9dd7",
   "metadata": {},
   "source": [
    "## 2.1¬†`ON` versus `USING`\n",
    "While the ON clause is extremely flexible, if the two tables share a column, we can use the simpler USING() method.\n",
    "\n",
    "‚ö†Ô∏è  **Warning:** When using \"USING\" to JOIN tables, always make sure that the names are identical in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d969b9",
   "metadata": {
    "language": "sql",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOIN with USING clause:\n",
      "\n",
      "             name       city  account_balance  total_income\n",
      "0    Ahmed Hassan     Sydney           179.64        5800.0\n",
      "1   Lars Andersen       None           588.43        4500.0\n",
      "2      John Smith   New York           945.55        3500.0\n",
      "3    Maria Garcia     London           905.34        3500.0\n",
      "4     Yuki Tanaka   Shanghai           344.21        3200.0\n",
      "5  Diego Martinez  Amsterdam           821.98        2700.0\n",
      "6     Elena Popov     Madrid           845.86        1800.0\n",
      "7   Mary Williams       None           795.02        1200.0\n",
      "8          Li Wei      Tokyo           638.11         200.0\n",
      "\n",
      " Same query with ON clause (identical results):\n",
      "\n",
      "             name       city  account_balance  total_income\n",
      "0    Ahmed Hassan     Sydney           179.64        5800.0\n",
      "1   Lars Andersen       None           588.43        4500.0\n",
      "2      John Smith   New York           945.55        3500.0\n",
      "3    Maria Garcia     London           905.34        3500.0\n",
      "4     Yuki Tanaka   Shanghai           344.21        3200.0\n",
      "5  Diego Martinez  Amsterdam           821.98        2700.0\n",
      "6     Elena Popov     Madrid           845.86        1800.0\n",
      "7   Mary Williams       None           795.02        1200.0\n",
      "8          Li Wei      Tokyo           638.11         200.0\n"
     ]
    }
   ],
   "source": [
    "# USING clause with a subquery that creates matching column names\n",
    "# Create a subquery that has a 'name' column matching customers table\n",
    "using_join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance,\n",
    "    i.total_income\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    (SELECT name, SUM(amount) AS total_income \n",
    "     FROM income \n",
    "     GROUP BY name) i\n",
    "USING (name)\n",
    "WHERE \n",
    "    c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"JOIN with USING clause:\\n\")\n",
    "print(pd.read_sql_query(using_join_query, conn))\n",
    "\n",
    "# Compare with ON clause - same results, different syntax\n",
    "on_join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance,\n",
    "    i.total_income\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    (SELECT name, SUM(amount) AS total_income \n",
    "     FROM income \n",
    "     GROUP BY name) i\n",
    "ON c.name = i.name\n",
    "WHERE \n",
    "    c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Same query with ON clause (identical results):\\n\")\n",
    "print(pd.read_sql_query(on_join_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d8dfb-2270-4252-905b-ebe641481b1d",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Ambiguous column names in JOINs\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Unclear which table's 'name' column\n",
    "SELECT name, amount\n",
    "FROM customers\n",
    "JOIN income ON customers.name = income.name\n",
    "\n",
    "-- ‚úÖ CORRECT: Qualify columns with table names or aliases\n",
    "SELECT c.name, i.amount\n",
    "FROM customers c\n",
    "JOIN income i ON c.name = i.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d6294-f9ec-4cef-929f-23c10ec5f100",
   "metadata": {},
   "source": [
    "But is important to be careful - using a non-primary key column can lead to issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "158883ad-c959-4172-a097-ba763b526d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problematic USING() JOIN Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>13.09</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>56.37</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>125.88</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>155.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>155.98</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>301.15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>301.15</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>299.36</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name       city  total_spent  amount\n",
       "0     Ahmed Hassan     Sydney       177.45   800.0\n",
       "1     Ahmed Hassan     Sydney       177.45  1500.0\n",
       "2     Ahmed Hassan     Sydney       177.45  3500.0\n",
       "3   Diego Martinez  Amsterdam        13.09  2700.0\n",
       "4      Elena Popov     Madrid        56.37  1800.0\n",
       "5       John Smith   New York          NaN   500.0\n",
       "6       John Smith   New York          NaN  3000.0\n",
       "7    Lars Andersen       None       125.88  4500.0\n",
       "8           Li Wei      Tokyo       155.98     NaN\n",
       "9           Li Wei      Tokyo       155.98   200.0\n",
       "10    Maria Garcia     London          NaN  1000.0\n",
       "11    Maria Garcia     London          NaN  2500.0\n",
       "12   Mary Williams       None      1183.71  1200.0\n",
       "13   Sarah Johnson     Berlin       301.15     NaN\n",
       "14   Sarah Johnson     Berlin       301.15   400.0\n",
       "15     Yuki Tanaka   Shanghai       299.36  3200.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "problematic_using_join = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    i.amount\n",
    "FROM   customers AS c\n",
    "INNER JOIN income AS i            -- multiple income rows per customer\n",
    "USING  (name)                     -- alias *i* is required for i.amount\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    \"\\nProblematic USING() JOIN Results:\"\n",
    ")\n",
    "display(pd.read_sql_query(problematic_using_join, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142be246-ebcc-43ea-bc9d-1e2a163ad391",
   "metadata": {},
   "source": [
    "üîî **Question:** Can anyone figure out what went wrong here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ed1ad-fb26-4ec4-aadf-df51ef64784a",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** Join customers with income to list each customer‚Äôs name, country, and total income (sum of amount) sorted by total income descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a0926c-6e13-4a46-8570-bcb639a37be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT name, country, SUM(amount) AS total_income\nFROM customers\nJOIN income\n  ON name = name          -- ‚ùå ambiguous columns!\nGROUP BY name, country\nORDER BY total_income DESC;\n': ambiguous column name: name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: ambiguous column name: name",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debug this intentional error\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bad_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT name, country, SUM(amount) AS total_income\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM customers\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mORDER BY total_income DESC;\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_sql_query(bad_query, conn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    527\u001b[0m         sql,\n\u001b[0;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    535\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT name, country, SUM(amount) AS total_income\nFROM customers\nJOIN income\n  ON name = name          -- ‚ùå ambiguous columns!\nGROUP BY name, country\nORDER BY total_income DESC;\n': ambiguous column name: name"
     ]
    }
   ],
   "source": [
    "# Debug this intentional error\n",
    "\n",
    "bad_query = \"\"\"\n",
    "SELECT name, country, SUM(amount) AS total_income\n",
    "FROM customers\n",
    "JOIN income\n",
    "  ON name = name          -- ‚ùå ambiguous columns!\n",
    "GROUP BY name, country\n",
    "ORDER BY total_income DESC;\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf25be",
   "metadata": {},
   "source": [
    "## 2.2¬†Multiple¬†Joins - Advanced Topic, Time Permitting üï§\n",
    "We don't have to stop at two - we can use multiple joins at once. And what is interesting is that we can get pretty creative with the ON conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615bd16e",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-join demonstration ‚Äì customers plus derived totals:\n",
      "\n",
      "                name country  total_spent  total_income  avg_balance\n",
      "0       Ahmed Hassan      AU       177.45        5800.0       179.64\n",
      "1      Lars Andersen    None       125.88        4500.0          NaN\n",
      "2         John Smith      US          NaN        3500.0       945.55\n",
      "3       Maria Garcia      GB          NaN        3500.0       905.34\n",
      "4        Yuki Tanaka      CN       299.36        3200.0       344.21\n",
      "5     Diego Martinez      NL        13.09        2700.0       821.98\n",
      "6        Elena Popov      ES        56.37        1800.0       845.86\n",
      "7      Mary Williams    None      1183.71        1200.0          NaN\n",
      "8      Sarah Johnson      DE       301.15         400.0          NaN\n",
      "9             Li Wei      JP       155.98         200.0       638.11\n",
      "10        Emma Brown      FR       517.44           NaN       929.69\n",
      "11  Carlos Rodriguez      IN      1049.40           NaN       140.70\n",
      "12     Anna Kowalski      BR      1066.01           NaN       392.80\n",
      "13      James Wilson      CA       331.04           NaN       449.81\n",
      "14     Michel Dubois      RU       143.16           NaN       421.08\n",
      "15      Sofia Santos      AE       595.32           NaN       352.84\n",
      "16       Aisha Patel      MX       109.14           NaN       226.83\n",
      "17         Lucy Chen      EG      1102.08           NaN       167.10\n",
      "18       Ivan Petrov      SE       399.48           NaN       988.20\n",
      "19         Raj Kumar      US       380.50           NaN       945.55\n",
      "20      Hans Schmidt      IT          NaN           NaN       104.97\n",
      "21    Isabella Silva      HK       888.00           NaN       833.92\n",
      "22    Fatima Al-Said      TR          NaN           NaN       736.17\n",
      "23          Jun Park      KR      1847.94           NaN       756.11\n",
      "24      Anna Ivanova      TH       957.12           NaN       794.14\n"
     ]
    }
   ],
   "source": [
    "multi_join_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "\n",
    "    /* total income per customer, pre-aggregated in an inline view */\n",
    "    it.total_income,\n",
    "\n",
    "    /* country-level average balance for extra JOIN practice */\n",
    "    cs.avg_balance\n",
    "\n",
    "FROM   customers AS c\n",
    "\n",
    "/* 1) Join to per-customer income totals */\n",
    "LEFT JOIN (\n",
    "    SELECT name,\n",
    "           SUM(amount) AS total_income\n",
    "    FROM   income\n",
    "    GROUP  BY name\n",
    ") AS it\n",
    "  ON c.name = it.name\n",
    "\n",
    "/* 2) Join to per-country balance stats */\n",
    "LEFT JOIN (\n",
    "    SELECT country,\n",
    "           AVG(account_balance) AS avg_balance\n",
    "    FROM   customers\n",
    "    WHERE  account_balance IS NOT NULL\n",
    "    GROUP  BY country\n",
    ") AS cs\n",
    "  ON c.country = cs.country\n",
    "\n",
    "ORDER BY it.total_income DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nMulti-join demonstration ‚Äì customers plus derived totals:\\n\")\n",
    "print(pd.read_sql_query(multi_join_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d742f-354f-44d7-acd0-4b99a82654e4",
   "metadata": {},
   "source": [
    "üîî **Question:** In the results above, notice how some customers have NULL values for `total_income` while others have NULL for `total_spent`. Why is this happening?\n",
    "\n",
    "This demonstrates the difference between ```INNER JOIN``` and ```LEFT JOIN```.\n",
    "\n",
    "If we had used ```INNER JOIN``` for both joins, we would only see customers who have entries in ALL three tables. But the query above uses ```LEFT JOIN```, which preserves all rows from the left table (customers) even when there's no match in the joined tables.\n",
    "\n",
    "Looking at our results:\n",
    "- Emma Brown, Carlos Rodriguez, and others have `total_spent` values but NULL for `total_income` - they made purchases but have no income records\n",
    "- Hans Schmidt and Fatima Al-Said have NULL for both - they're in the customers table but have neither purchase nor income data\n",
    "- The first 10 customers have values for both - they exist in all three derived tables\n",
    "\n",
    "This flexibility of ```LEFT JOIN``` is exactly why it's so useful - it helps us identify missing data relationships while still showing all our customers.\n",
    "\n",
    "In this workshop we will explore both ```INNER JOIN``` (which we saw in the basic example) and ```LEFT JOIN``` in detail, along with ```SELF JOIN``` - but for those curious, there are others, such as ```RIGHT JOIN```, ```FULL JOIN``` AND ```CROSS JOIN```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfc799-e0cc-4276-9f5b-2163d390fa23",
   "metadata": {},
   "source": [
    "üìù **Poll 2:** You want *all* customers, even those with no orders. Which join keeps them? \n",
    "- customers `LEFT JOIN` orders\n",
    "- orders `LEFT JOIN` customers\n",
    "- Either order gives same result\n",
    "- Neither command would return all customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8d04f",
   "metadata": {},
   "source": [
    "## 2.3¬†Advanced Join Variants\n",
    "- **LEFT JOIN**: retains all rows from the left table.\n",
    "    - This is why it is important to distinguish which table is being used in the FROM statement, and which is being brought by the JOIN statement\n",
    "    - Specially useful to handle missing data \n",
    "- **SELF¬†JOIN**: the table is joined to itself\n",
    "    - We essentially deal with two tables - one of them being a duplicate of the first - and then JOIN them\n",
    "    - Very useful as a filtering tool\n",
    "\n",
    "üîî **Question:** Can anyone think of an example in which we might want to join a table with itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff72c16-fd7f-459d-b479-af7d0f14e3c3",
   "metadata": {},
   "source": [
    "![Set-logic Venn diagrams for LEFT, INNER and FULL joins with tiny code snippets and labelled A/B table overlaps.](../Images/sql-joins-venn-diagram.svg)\n",
    "---\n",
    "\n",
    "Let's understand what is going on with the different type of joins:\n",
    "- In a ```LEFT JOIN```, we start with all rows from table 1 - the \"left\" table. If there is a match in table 2, bring the extra columns, otherwise leave these columns ```NULL```\n",
    "- In a ```INNER JOIN```, we only keep the records that are on both tables! In other words, if one row is in one table but not the other, do not bring it to the merged table.\n",
    "- For a ```FULL JOIN```, we treat both tables as the \"left\" one - we keep every row from both sides, and if it does not have a matching record, fill the gaps with ```NULL```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4dcf00-96a2-4306-9dda-9090bf1aff07",
   "metadata": {},
   "source": [
    "üôã **Hands-Up:** True or False ‚Äî `INNER JOIN` can drop rows that exist in only one table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43d623-1e4a-4720-ba7c-5b171bab3fd3",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Why `LEFT JOIN` Order Matters\n",
    "\n",
    "`customers` lists everyone in our sample.  \n",
    "`income` records dollar amounts **only for those customers who reported income**.\n",
    "\n",
    "1. **Write two queries**  \n",
    "   * **Query A**: `customers  LEFT JOIN income`  \n",
    "   * **Query B**: `income     LEFT JOIN customers`\n",
    "2. For each query return just one column: `COUNT(*) AS row_count`.\n",
    "3. Compare the two row counts‚Äîexplain the difference in one sentence *(code comment is fine)*.\n",
    "\n",
    "> **Hint**  \n",
    "> In a `LEFT JOIN`, **all** rows from the table **before** `LEFT JOIN` are preserved, even when no match exists in the table **after** `LEFT JOIN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf27cc1f-a3da-4c75-b54c-50355784aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Query A rows: 31  |  Query B rows: 22 ‚Äî order matters!\n"
     ]
    }
   ],
   "source": [
    "qA = \"\"\"SELECT COUNT(*) AS n FROM customers\n",
    "        LEFT JOIN income ON customers.name = income.name\"\"\"\n",
    "qB = \"\"\"SELECT COUNT(*) AS n FROM income\n",
    "        LEFT JOIN customers ON customers.name = income.name\"\"\"\n",
    "nA = pd.read_sql_query(qA, conn)[\"n\"][0]\n",
    "nB = pd.read_sql_query(qB, conn)[\"n\"][0]\n",
    "\n",
    "assert nA >= nB, textwrap.dedent(f\"\"\"\n",
    "    Expected Query A (customers left) to have ‚â• rows than Query B,\n",
    "    but got {nA} vs {nB}.\n",
    "\"\"\")\n",
    "print(f\"‚úî Query A rows: {nA}  |  Query B rows: {nB} ‚Äî order matters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4bf71-1050-4c9b-b537-1c52b1a9d5a1",
   "metadata": {},
   "source": [
    "One consideration when using ```LEFT JOIN``` is that it returns ```NULL``` values for unmatched records - but we might want to handle these NULLs differently depending on our analysis needs. \n",
    "\n",
    "For example, in our data:\n",
    "- When Li Wei has a NULL amount in some income records, it means the income amount is unknown\n",
    "- When Emma Brown has NULL for total_income after our JOIN, it means she has no income records at all\n",
    "\n",
    "Sometimes we want to treat these NULLs as zeros (e.g., \"no income records\" = \"0 total income\" for a spending analysis), while other times we want to preserve them as unknown values.\n",
    "\n",
    "Luckily, SQL includes a function that allows us to tailor the behavior of ```NULL``` entries for a given query - the ```COALESCE``` command. The first argument of the function tells us which column to analyze, and the second entry what to replace ```NULL``` values by. Let's take a look at an example in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc6b67c5-bb7d-4f91-949d-a2afce6deb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name country  amount\n",
      "0       Ahmed Hassan      AU   800.0\n",
      "1       Ahmed Hassan      AU  1500.0\n",
      "2       Ahmed Hassan      AU  3500.0\n",
      "3        Aisha Patel      MX     NaN\n",
      "4       Anna Ivanova      TH     NaN\n",
      "5      Anna Kowalski      BR     NaN\n",
      "6   Carlos Rodriguez      IN     NaN\n",
      "7     Diego Martinez      NL  2700.0\n",
      "8        Elena Popov      ES  1800.0\n",
      "9         Emma Brown      FR     NaN\n",
      "10    Fatima Al-Said      TR     NaN\n",
      "11      Hans Schmidt      IT     NaN\n",
      "12    Isabella Silva      HK     NaN\n",
      "13       Ivan Petrov      SE     NaN\n",
      "14      James Wilson      CA     NaN\n",
      "15        John Smith      US   500.0\n",
      "16        John Smith      US  3000.0\n",
      "17          Jun Park      KR     NaN\n",
      "18     Lars Andersen    None  4500.0\n",
      "19            Li Wei      JP     NaN\n"
     ]
    }
   ],
   "source": [
    "# LEFT JOIN without COALESCE ‚Äì observe the NULL \"amount\" rows\n",
    "no_coalesce_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    i.amount            -- will be NULL if the customer has no income rows\n",
    "FROM   customers AS c\n",
    "LEFT JOIN income AS i\n",
    "       ON c.name = i.name\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(no_coalesce_query, conn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a611c37b-5dbb-4257-af31-8cb6d63e21f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name country  amount_received\n",
      "0       Ahmed Hassan      AU            800.0\n",
      "1       Ahmed Hassan      AU           1500.0\n",
      "2       Ahmed Hassan      AU           3500.0\n",
      "3        Aisha Patel      MX              0.0\n",
      "4       Anna Ivanova      TH              0.0\n",
      "5      Anna Kowalski      BR              0.0\n",
      "6   Carlos Rodriguez      IN              0.0\n",
      "7     Diego Martinez      NL           2700.0\n",
      "8        Elena Popov      ES           1800.0\n",
      "9         Emma Brown      FR              0.0\n",
      "10    Fatima Al-Said      TR              0.0\n",
      "11      Hans Schmidt      IT              0.0\n",
      "12    Isabella Silva      HK              0.0\n",
      "13       Ivan Petrov      SE              0.0\n",
      "14      James Wilson      CA              0.0\n",
      "15        John Smith      US            500.0\n",
      "16        John Smith      US           3000.0\n",
      "17          Jun Park      KR              0.0\n",
      "18     Lars Andersen    None           4500.0\n",
      "19            Li Wei      JP              0.0\n"
     ]
    }
   ],
   "source": [
    "# Same JOIN, but fill the missing income amounts with 0\n",
    "coalesce_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    COALESCE(i.amount, 0) AS amount_received   -- replaces NULL with 0\n",
    "FROM   customers AS c\n",
    "LEFT JOIN income AS i\n",
    "       ON c.name = i.name\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(coalesce_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de03177-d5d9-4548-8ca8-d23b76e3f101",
   "metadata": {},
   "source": [
    "### SELF JOIN - Advanced Topic, Time Permitting üï§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37315fe8-7ed7-4c04-85fc-b9a32020ca84",
   "metadata": {},
   "source": [
    "Now let's talk a bit about SELF JOIN. As we mentioned before, the idea here is to join a table with itself. This is one of the cases in which using aliases is extremely important, since, by construction, both of the tables will have all columns in common!\n",
    "\n",
    "Let's see an example on how this might work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "907ffd7a-e45e-4321-b17c-9ca66e7cca2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers with Similar Spending (within $50):\n",
      "\n",
      "        customer_1        customer_2  customer_1_spent  customer_2_spent  \\\n",
      "0    Sarah Johnson       Yuki Tanaka            301.15            299.36   \n",
      "1           Li Wei     Michel Dubois            155.98            143.16   \n",
      "2    Anna Kowalski  Carlos Rodriguez           1066.01           1049.40   \n",
      "3      Aisha Patel     Lars Andersen            109.14            125.88   \n",
      "4    Lars Andersen     Michel Dubois            125.88            143.16   \n",
      "5      Ivan Petrov         Raj Kumar            399.48            380.50   \n",
      "6     Ahmed Hassan            Li Wei            177.45            155.98   \n",
      "7     James Wilson     Sarah Johnson            331.04            301.15   \n",
      "8    Lars Andersen            Li Wei            125.88            155.98   \n",
      "9     James Wilson       Yuki Tanaka            331.04            299.36   \n",
      "10     Aisha Patel     Michel Dubois            109.14            143.16   \n",
      "11    Ahmed Hassan     Michel Dubois            177.45            143.16   \n",
      "12   Anna Kowalski         Lucy Chen           1066.01           1102.08   \n",
      "13  Diego Martinez       Elena Popov             13.09             56.37   \n",
      "14     Aisha Patel            Li Wei            109.14            155.98   \n",
      "15    James Wilson         Raj Kumar            331.04            380.50   \n",
      "\n",
      "    spending_difference  \n",
      "0                  1.79  \n",
      "1                 12.82  \n",
      "2                 16.61  \n",
      "3                 16.74  \n",
      "4                 17.28  \n",
      "5                 18.98  \n",
      "6                 21.47  \n",
      "7                 29.89  \n",
      "8                 30.10  \n",
      "9                 31.68  \n",
      "10                34.02  \n",
      "11                34.29  \n",
      "12                36.07  \n",
      "13                43.28  \n",
      "14                46.84  \n",
      "15                49.46  \n"
     ]
    }
   ],
   "source": [
    "# A self-join example: customers with similar spending levels\n",
    "spending_similarity_query = \"\"\"\n",
    "SELECT \n",
    "    c1.name AS customer_1,\n",
    "    c2.name AS customer_2,\n",
    "    c1.items_purchased * c1.price_per_item AS customer_1_spent,\n",
    "    c2.items_purchased * c2.price_per_item AS customer_2_spent,\n",
    "    ABS((c1.items_purchased * c1.price_per_item) - \n",
    "        (c2.items_purchased * c2.price_per_item)) AS spending_difference\n",
    "FROM \n",
    "    customers c1\n",
    "JOIN \n",
    "    customers c2 ON c1.name < c2.name\n",
    "WHERE \n",
    "    c1.items_purchased IS NOT NULL \n",
    "    AND c1.price_per_item IS NOT NULL\n",
    "    AND c2.items_purchased IS NOT NULL \n",
    "    AND c2.price_per_item IS NOT NULL\n",
    "    AND ABS((c1.items_purchased * c1.price_per_item) - \n",
    "            (c2.items_purchased * c2.price_per_item)) < 50\n",
    "ORDER BY \n",
    "    spending_difference;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Customers with Similar Spending (within $50):\\n\")\n",
    "print(pd.read_sql_query(spending_similarity_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9bb68-b741-4363-8a54-2e1ae415c1e8",
   "metadata": {},
   "source": [
    "There are a few things worth noticing here:\n",
    "- When we selected the columns from each table, it was important to rename them - since they had the same original names!\n",
    "- Notice that John did not report to anyone - so he was not included as an employee in the merged table. This could be adapted by using a LEFT JOIN\n",
    "- Some managers appear many times, since more than one employee reports to them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d9ff0-f6d2-46ed-8c95-1a26a3a013f4",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Customers Who Out-Spend Everyone Else in Their Country\n",
    "‚ÄúWho is the top spender in each country?‚Äù\n",
    "\n",
    "Idea:\n",
    "Self-JOIN customers to itself on country.\n",
    "For each row c1, look for a matching row c2 in the same country whose total_spent is higher.\n",
    "If no such c2 exists, c1 must be that country‚Äôs top spender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922c9ca4-1047-4d72-9416-b4bf989274af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>total_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>125.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sofia Santos</td>\n",
       "      <td>AE</td>\n",
       "      <td>595.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>AU</td>\n",
       "      <td>177.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Kowalski</td>\n",
       "      <td>BR</td>\n",
       "      <td>1066.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>James Wilson</td>\n",
       "      <td>CA</td>\n",
       "      <td>331.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>CN</td>\n",
       "      <td>299.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>DE</td>\n",
       "      <td>301.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lucy Chen</td>\n",
       "      <td>EG</td>\n",
       "      <td>1102.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>ES</td>\n",
       "      <td>56.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Emma Brown</td>\n",
       "      <td>FR</td>\n",
       "      <td>517.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Isabella Silva</td>\n",
       "      <td>HK</td>\n",
       "      <td>888.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Carlos Rodriguez</td>\n",
       "      <td>IN</td>\n",
       "      <td>1049.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>JP</td>\n",
       "      <td>155.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jun Park</td>\n",
       "      <td>KR</td>\n",
       "      <td>1847.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Aisha Patel</td>\n",
       "      <td>MX</td>\n",
       "      <td>109.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>NL</td>\n",
       "      <td>13.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Michel Dubois</td>\n",
       "      <td>RU</td>\n",
       "      <td>143.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ivan Petrov</td>\n",
       "      <td>SE</td>\n",
       "      <td>399.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Anna Ivanova</td>\n",
       "      <td>TH</td>\n",
       "      <td>957.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Raj Kumar</td>\n",
       "      <td>US</td>\n",
       "      <td>380.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name country  total_spent\n",
       "0      Mary Williams    None      1183.71\n",
       "1      Lars Andersen    None       125.88\n",
       "2       Sofia Santos      AE       595.32\n",
       "3       Ahmed Hassan      AU       177.45\n",
       "4      Anna Kowalski      BR      1066.01\n",
       "5       James Wilson      CA       331.04\n",
       "6        Yuki Tanaka      CN       299.36\n",
       "7      Sarah Johnson      DE       301.15\n",
       "8          Lucy Chen      EG      1102.08\n",
       "9        Elena Popov      ES        56.37\n",
       "10        Emma Brown      FR       517.44\n",
       "11    Isabella Silva      HK       888.00\n",
       "12  Carlos Rodriguez      IN      1049.40\n",
       "13            Li Wei      JP       155.98\n",
       "14          Jun Park      KR      1847.94\n",
       "15       Aisha Patel      MX       109.14\n",
       "16    Diego Martinez      NL        13.09\n",
       "17     Michel Dubois      RU       143.16\n",
       "18       Ivan Petrov      SE       399.48\n",
       "19      Anna Ivanova      TH       957.12\n",
       "20         Raj Kumar      US       380.50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_spenders_query = \"\"\"\n",
    "-- 1) Build total spending per customer\n",
    "WITH customer_spending AS (\n",
    "    SELECT\n",
    "           name,\n",
    "           country,\n",
    "           items_purchased * price_per_item AS total_spent\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    "       AND price_per_item  IS NOT NULL\n",
    ")\n",
    "\n",
    "-- 2) Self-JOIN filter: keep only rows with no higher spender in the same country\n",
    "SELECT\n",
    "       c1.name,\n",
    "       c1.country,\n",
    "       c1.total_spent\n",
    "FROM   customer_spending AS c1\n",
    "LEFT JOIN customer_spending AS c2\n",
    "       ON  c1.country      = c2.country\n",
    "       AND c2.total_spent  > c1.total_spent\n",
    "WHERE  c2.name IS NULL              -- ‚Üê nobody beats c1 in that country\n",
    "ORDER  BY c1.country, c1.total_spent DESC;\n",
    "\"\"\"\n",
    "pd.read_sql_query(top_spenders_query, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927941-cc29-4616-881f-09d8000a270a",
   "metadata": {},
   "source": [
    "Of course, SELF JOINs can be used for purposes other than filtering. Another classic application is to find pairs that satisfy some criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f04d4262-d602-4e13-8c2e-ebb1819b64c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF JOIN Example - Customers from the Same City:\n",
      "Empty DataFrame\n",
      "Columns: [customer_1, customer_2, shared_city, country]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "self_join_query = \"\"\"\n",
    "SELECT \n",
    "    c1.name AS customer_1,\n",
    "    c2.name AS customer_2,\n",
    "    c1.city AS shared_city,\n",
    "    c1.country\n",
    "FROM \n",
    "    customers c1\n",
    "JOIN \n",
    "    customers c2 ON c1.city = c2.city \n",
    "                 AND c1.name < c2.name  -- Avoid duplicates and self-matches\n",
    "WHERE \n",
    "    c1.city IS NOT NULL\n",
    "ORDER BY \n",
    "    c1.city, c1.name;\n",
    "\"\"\"\n",
    "\n",
    "print(\"SELF JOIN Example - Customers from the Same City:\")\n",
    "result = pd.read_sql_query(self_join_query, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a43e1-367f-4754-9f42-300fe2846509",
   "metadata": {},
   "source": [
    "üìù **Poll 3:** When self-joining `customers` to find same-country pairs, what extra condition avoids duplicate & mirror rows? - `c1.id <> c2.id` - `c1.id < c2.id` - `c1.country IS NOT NULL`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1071b0",
   "metadata": {},
   "source": [
    "<a id='subqueries'></a>\n",
    "## 3¬†¬∑¬†Subqueries \n",
    "\n",
    "Estimated Time: 20 minutes\n",
    "\n",
    "**Definition.** As the name says, a subquery is a query contained in another query. This allows us to perform auxiliary queries, and then use the results of these queries in our main query. Unlike the main query, subqueries are temporary - they only exist while the instance of the query is being worked out. \n",
    "\n",
    "**Purpose.** Subqueries are very useful when one needs to break down a complex question into multiple manageable individual parts.For example, one might want to summarize or filter a given table, and use the summarized/filtered results as the input of another query. \n",
    "\n",
    "**Common Use¬†Case.** Getting summary statistics for each individual, while keeping information unrelated from the variable we are using to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db71b04-8952-42cb-a923-bda8226e739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers ranked by total recorded income:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>total_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>AU</td>\n",
       "      <td>5800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>GB</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>CN</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>NL</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>ES</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>DE</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>JP</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name       city country  total_income\n",
       "0    Ahmed Hassan     Sydney      AU        5800.0\n",
       "1   Lars Andersen       None    None        4500.0\n",
       "2      John Smith   New York      US        3500.0\n",
       "3    Maria Garcia     London      GB        3500.0\n",
       "4     Yuki Tanaka   Shanghai      CN        3200.0\n",
       "5  Diego Martinez  Amsterdam      NL        2700.0\n",
       "6     Elena Popov     Madrid      ES        1800.0\n",
       "7   Mary Williams       None    None        1200.0\n",
       "8   Sarah Johnson     Berlin      DE         400.0\n",
       "9          Li Wei      Tokyo      JP         200.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Total income per person, then attach all customer metadata\n",
    "income_totals_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    it.total_income\n",
    "FROM   customers AS c\n",
    "JOIN  ( SELECT name,\n",
    "               SUM(amount) AS total_income\n",
    "        FROM   income\n",
    "        WHERE  amount IS NOT NULL      -- defensive\n",
    "        GROUP  BY name ) AS it\n",
    "      ON c.name = it.name\n",
    "ORDER BY it.total_income DESC;\n",
    "\"\"\"\n",
    "print(\"Customers ranked by total recorded income:\")\n",
    "display(pd.read_sql_query(income_totals_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4fe0-ab4a-4038-a0e8-49574684cb0d",
   "metadata": {},
   "source": [
    "There are two main ways of using subqueries:\n",
    "\n",
    "1) To Filter results.\n",
    "\n",
    "A classic example is to find all customers who spend more than average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a013097e-4c6d-479f-b147-74f3f89b9cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers who spend more than average:\n",
      "\n",
      "               name       city country  total_spent  average_spending\n",
      "0          Jun Park      Seoul      KR      1847.94            557.12\n",
      "1     Mary Williams       None    None      1183.71            557.12\n",
      "2         Lucy Chen      Cairo      EG      1102.08            557.12\n",
      "3     Anna Kowalski  S√£o Paulo      BR      1066.01            557.12\n",
      "4  Carlos Rodriguez     Mumbai      IN      1049.40            557.12\n",
      "5      Anna Ivanova    Bangkok      TH       957.12            557.12\n",
      "6    Isabella Silva  Hong Kong      HK       888.00            557.12\n",
      "7      Sofia Santos      Dubai      AE       595.32            557.12\n",
      "\n",
      " Average spending: $557.12\n"
     ]
    }
   ],
   "source": [
    "# Subquery example - Find customers who spend more than average\n",
    "subquery_example = \"\"\"\n",
    "WITH spending_stats AS (\n",
    "    SELECT \n",
    "        AVG(items_purchased * price_per_item) AS avg_spent,\n",
    "        MAX(items_purchased * price_per_item) AS max_spent,\n",
    "        MIN(items_purchased * price_per_item) AS min_spent\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL \n",
    "      AND price_per_item IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    ROUND((SELECT avg_spent FROM spending_stats), 2) AS average_spending\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.items_purchased IS NOT NULL \n",
    "    AND c.price_per_item IS NOT NULL\n",
    "    AND (c.items_purchased * c.price_per_item) > (\n",
    "        SELECT avg_spent FROM spending_stats\n",
    "    )\n",
    "ORDER BY \n",
    "    total_spent DESC;\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(subquery_example, conn)\n",
    "print(\"Customers who spend more than average:\\n\")\n",
    "print(result)\n",
    "\n",
    "avg_spent = pd.read_sql_query(\"\"\"\n",
    "    SELECT ROUND(AVG(items_purchased * price_per_item), 2) as avg_spent\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL AND price_per_item IS NOT NULL\n",
    "\"\"\", conn).iloc[0,0]\n",
    "print(f\"\\n Average spending: ${avg_spent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114d258-6a7e-41a5-9b01-68943d9cac65",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Incorrect subquery references\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Cannot reference CTE columns directly\n",
    "WITH high_spenders AS (\n",
    "    SELECT name, SUM(amount) as total FROM income GROUP BY name\n",
    ")\n",
    "SELECT * FROM customers\n",
    "WHERE account_balance > high_spenders.total\n",
    "\n",
    "-- ‚úÖ CORRECT: Use subquery or JOIN with CTE\n",
    "WITH high_spenders AS (\n",
    "    SELECT name, SUM(amount) as total FROM income GROUP BY name\n",
    ")\n",
    "SELECT c.* FROM customers c\n",
    "JOIN high_spenders h ON c.name = h.name\n",
    "WHERE c.account_balance > h.total\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaab118-4ade-400c-8b7e-31666ee71539",
   "metadata": {},
   "source": [
    "2) As a derived table to query from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eb7cad2-619f-402c-b6df-7e9ff50b6296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>city_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  city  items_purchased  city_avg\n",
       "0  Mary Williams  None             17.0      11.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_query_sqlite = \"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    city,\n",
    "    items_purchased,\n",
    "    city_avg\n",
    "FROM (\n",
    "    SELECT\n",
    "        name,\n",
    "        city,\n",
    "        items_purchased,\n",
    "        AVG(items_purchased) OVER (PARTITION BY city) AS city_avg\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    ")\n",
    "WHERE  items_purchased > city_avg\n",
    "ORDER  BY city, items_purchased DESC;\n",
    "\"\"\"\n",
    "\n",
    "display(pd.read_sql_query(window_query_sqlite, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b4160-6b11-42f1-a8bb-20bf35c00d8f",
   "metadata": {},
   "source": [
    "As promised in the first workshop, we can also use subqueries in combination with IN to check for membership against entire tables - usually the result of a subquery.\n",
    "\n",
    "For example, let's say that we want to select the names of all customers who have purchased on Electronic item, but the information between consumers, products and purchases are all on separate tables. We could first JOIN them, then use a WHERE statement to filter them. But this would create a very large merged table. Instead, we can just use an IN statement with a subquery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656e9337-d9ab-4329-ba83-cf6c5217cea0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High income earners (>$5000):\n",
      "\n",
      "           name  total_income\n",
      "0  Ahmed Hassan        5800.0\n",
      "\n",
      " Customer details for high income earners: \n",
      "\n",
      "           name    city country  account_balance  total_spent\n",
      "0  Ahmed Hassan  Sydney      AU           179.64       177.45\n",
      "\n",
      " Customers with income data but earning <= $5000:\n",
      "\n",
      "             name       city  account_balance\n",
      "0      John Smith   New York           945.55\n",
      "1    Maria Garcia     London           905.34\n",
      "2     Elena Popov     Madrid           845.86\n",
      "3  Diego Martinez  Amsterdam           821.98\n",
      "4   Mary Williams       None           795.02\n"
     ]
    }
   ],
   "source": [
    "# Using IN with subquery to find customers with high income\n",
    "# First, let's see what income levels we have\n",
    "income_summary = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(amount) as total_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING SUM(amount) > 5000\n",
    "ORDER BY total_income DESC;\n",
    "\"\"\"\n",
    "print(\"High income earners (>$5000):\\n\")\n",
    "print(pd.read_sql_query(income_summary, conn))\n",
    "\n",
    "# Find customer details for high income earners\n",
    "high_income_customers_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    c.account_balance,\n",
    "    c.items_purchased * c.price_per_item as total_spent\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.name IN (\n",
    "        SELECT name\n",
    "        FROM income\n",
    "        WHERE amount IS NOT NULL\n",
    "        GROUP BY name\n",
    "        HAVING SUM(amount) > 5000\n",
    "    )\n",
    "    AND c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    c.account_balance DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Customer details for high income earners: \\n\")\n",
    "result = pd.read_sql_query(high_income_customers_query, conn)\n",
    "print(result)\n",
    "\n",
    "# Compare with NOT IN - customers without high income\n",
    "low_income_customers_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.name NOT IN (\n",
    "        SELECT name\n",
    "        FROM income\n",
    "        WHERE amount IS NOT NULL\n",
    "        GROUP BY name\n",
    "        HAVING SUM(amount) > 5000\n",
    "    )\n",
    "    AND c.name IN (SELECT DISTINCT name FROM income)  -- Only those with some income data\n",
    "    AND c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    c.account_balance DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Customers with income data but earning <= $5000:\\n\")\n",
    "print(pd.read_sql_query(low_income_customers_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b55343-835f-406a-958c-3fcc323ee2bf",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Using NOT IN with subqueries that may contain NULL values can return no results! This is because NULL comparisons are undefined. Use NOT EXISTS or filter out NULLs in your subquery with WHERE column IS NOT NULL to avoid this trap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bbebf-9552-4dd0-a4fe-947840ca76e4",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** List the name, country, and total_income of every customer whose total income is below the overall average customer income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b63d566f-6e4f-4668-bdc7-eebf010c3257",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT c.name, c.country, SUM(i.amount) AS total_income\nFROM customers AS c\nJOIN income AS i\n  ON c.name = i.name\nGROUP BY c.name, c.country\nWHERE SUM(i.amount) <\n      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n': near \"WHERE\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \"WHERE\": syntax error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debug this intentional error\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bad_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT c.name, c.country, SUM(i.amount) AS total_income\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM customers AS c\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_sql_query(bad_query, conn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    527\u001b[0m         sql,\n\u001b[0;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    535\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT c.name, c.country, SUM(i.amount) AS total_income\nFROM customers AS c\nJOIN income AS i\n  ON c.name = i.name\nGROUP BY c.name, c.country\nWHERE SUM(i.amount) <\n      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n': near \"WHERE\": syntax error"
     ]
    }
   ],
   "source": [
    "# Debug this intentional error\n",
    "\n",
    "bad_query = \"\"\"\n",
    "SELECT c.name, c.country, SUM(i.amount) AS total_income\n",
    "FROM customers AS c\n",
    "JOIN income AS i\n",
    "  ON c.name = i.name\n",
    "GROUP BY c.name, c.country\n",
    "WHERE SUM(i.amount) <\n",
    "      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b769b",
   "metadata": {},
   "source": [
    "<a id='ctes'></a>\n",
    "## 4¬†¬∑¬†Common¬†Table¬†Expressions (CTEs) \n",
    "\n",
    "Estimated Time: 15 minutes\n",
    "\n",
    "Subqueries are a great way of breaking down a complex query into smaller, more manageable subparts. But very often these subqueries can become quite long, and given that one must include the full query inside another query, they can become very hard to read. \n",
    "\n",
    "CTEs are a way of solving this issue. Instead of rewriting the entire subquery inside the main query, we first give aliases to our subqueries, and then refer to them in the main query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fe831-d7d0-49b6-9900-441a4dd7b0da",
   "metadata": {},
   "source": [
    "![Infographic ‚ÄúCTEs vs Subqueries‚Äù showing when to use a readable CTE versus an inline subquery, with tiny code examples.](../Images/ctesubquery.svg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5921-5505-443d-95cf-71fb99815845",
   "metadata": {},
   "source": [
    "\n",
    "Basic Syntax   \n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte_name AS (\n",
    "    SELECT column1, column2, ...\n",
    "    FROM table\n",
    "    WHERE condition)\n",
    "SELECT * \n",
    "FROM cte_name;\n",
    "Key Components\n",
    "```\n",
    "\n",
    "WITH clause: Introduces one or more CTEs\n",
    "cte_name: Gives a name to the temporary result set\n",
    "Main query: References the CTE like a regular table\n",
    "\n",
    "üîî **Question:** Would you say that CTEs improve readability? Can you think of an example in which the code is easier to read using subqueries instead?\n",
    "\n",
    "Multiple CTEs\n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte1 AS (\n",
    "    SELECT column1 FROM table1\n",
    "),\n",
    "cte2 AS (\n",
    "    SELECT column2 FROM table2\n",
    ")\n",
    "SELECT *\n",
    "FROM cte1\n",
    "JOIN cte2 ON cte1.column = cte2.column;\n",
    "```\n",
    "\n",
    "üí° **Tip:** CTE's can be referenced multiple times in the same query, which improve readability and prevents errors!\n",
    "\n",
    "üôã **Hands-Up:** Which style feels clearer for multi-step queries so far? A. A nested subquery‚ÄÉB. A CTE (`WITH ‚Ä¶`) above the main query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "defeea3d",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "# CTE example using customers and income data\n",
    "cte_query = \"\"\"\n",
    "WITH customer_spending AS (\n",
    "    -- Calculate total spending per customer\n",
    "    SELECT \n",
    "        name,\n",
    "        items_purchased * price_per_item AS total_spent,\n",
    "        city,\n",
    "        country\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL \n",
    "      AND price_per_item IS NOT NULL\n",
    "),\n",
    "customer_income AS (\n",
    "    -- Calculate total income per customer\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(amount) AS total_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    "),\n",
    "spending_analysis AS (\n",
    "    -- Combine spending and income data\n",
    "    SELECT \n",
    "        cs.name,\n",
    "        cs.city,\n",
    "        cs.country,\n",
    "        cs.total_spent,\n",
    "        ci.total_income,\n",
    "        ROUND(cs.total_spent * 100.0 / ci.total_income, 2) AS spending_rate\n",
    "    FROM customer_spending cs\n",
    "    JOIN customer_income ci ON cs.name = ci.name\n",
    "    WHERE ci.total_income > 0\n",
    ")\n",
    "-- Final query using the CTEs\n",
    "SELECT \n",
    "    name,\n",
    "    city,\n",
    "    country,\n",
    "    total_spent,\n",
    "    total_income,\n",
    "    spending_rate,\n",
    "    CASE \n",
    "        WHEN spending_rate > 50 THEN 'High Spender'\n",
    "        WHEN spending_rate > 20 THEN 'Moderate Spender'\n",
    "        ELSE 'Low Spender'\n",
    "    END AS spending_category\n",
    "FROM spending_analysis\n",
    "ORDER BY spending_rate DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87d802-6a5f-4140-8cc4-9901cc65b6af",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** We cannot reference CTEs directly in a query - this is what we call (lack of) \"portability\".\n",
    "\n",
    "```sql\n",
    "WITH average_data AS (\n",
    "    SELECT AVG(value) AS avg_value FROM table\n",
    ")\n",
    "SELECT *\n",
    "FROM other_table\n",
    "WHERE value > average_data.avg_value \n",
    "```\n",
    "Whenever we want to reference a CTE, we must either use a subquery:\n",
    "\n",
    "```sql\n",
    "WHERE value > (SELECT avg_value FROM average_data)\n",
    "```\n",
    "\n",
    "Or JOIN with the CTE:\n",
    "\n",
    "```sql\n",
    "JOIN average_data ON 1=1\n",
    "WHERE value > average_data.avg_value\n",
    "```\n",
    "\n",
    "üí° **Tip:** In this last example, we used a common trick: choose an expression that is always true to add a constant column to the table.\n",
    "\n",
    "üôã **Hands-Up:** Which keyword starts a Common Table Expression? A. `WITH`‚ÄÉB. `WHERE`‚ÄÉC. `AS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5ac0",
   "metadata": {},
   "source": [
    "<a id='pivot'></a>\n",
    "## 5¬†¬∑¬†Pivoting and Unpivoting (Or Melting) \n",
    "\n",
    "Estimated Time: 15 minutes\n",
    "\n",
    "Pivoting is the process of turning rows into columns, Unpivoting (also called Melting) is the inverse process. \n",
    "\n",
    "A common application is when we would like to group information that applies to the same individual. For example, we might have a list of all the different transactions that different clients made, including amount and date. But maybe we would like to understand how purchases vary across the days of the week for each given customer. So we can turn a table from having many rows and three columns, to having one row for each consumer, and 8 columns: one representing the customer name (or any other identification), and one with the transaction amount for that customer on each day of the week. \n",
    "\n",
    "üìù **Poll 4:** In data-reshaping lingo, which operation produces a **tall (long-format)** table, and which yields a **wide (spread)** table?\n",
    "\n",
    "- Melt (gather) ‚Üí tall‚ÄÉ|‚ÄÉPivot (spread) ‚Üí wide  \n",
    "- Melt (gather) ‚Üí wide‚ÄÉ|‚ÄÉPivot (spread) ‚Üí tall  \n",
    "- Both operations create tall tables  \n",
    "- Both operations create wide tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1cb8c-c211-4a9d-8a2e-dbf1865208c6",
   "metadata": {},
   "source": [
    "![Side-by-side pivot vs melt diagram converting wide daily-sales columns into tall name-date-sales rows and back again.](../Images/pivot-melt-diagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6310f20-e0f7-482e-9b02-fb108c55722f",
   "metadata": {},
   "source": [
    "This is done by using the ```UNION``` (or `UNION ALL`) functions - which are analogues to `JOIN`s, but instead of putting different columns side by side, they combine results from different queries on top of each other. \n",
    "\n",
    "The main difference between `UNION` and `UNION ALL` is that the former removes duplicate rows, which increases the computational cost of the function.\n",
    "\n",
    "Two additional observations:\n",
    "- The column names will be taken from the first one selected\n",
    "- All `SELECT` statements must have the same number of columns, and corresponding columns must have the same data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd85fc76-f262-4ceb-8811-34eef58e8412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Income Data (first 10 rows):\n",
      "            name income_source  amount        day\n",
      "0     John Smith        Salary  3000.0     Monday\n",
      "1     John Smith     Freelance   500.0     Friday\n",
      "2   Maria Garcia        Salary  2500.0     Monday\n",
      "3   Maria Garcia         Bonus  1000.0     Friday\n",
      "4         Li Wei        Salary     NaN     Monday\n",
      "5         Li Wei    Investment   200.0  Wednesday\n",
      "6   Ahmed Hassan        Salary  3500.0     Monday\n",
      "7   Ahmed Hassan    Consulting  1500.0    Tuesday\n",
      "8   Ahmed Hassan        Rental   800.0   Thursday\n",
      "9  Sarah Johnson        Salary     NaN     Monday\n",
      "\\nPivoted Income Data (by day of week):\n",
      "               name  Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  \\\n",
      "0      Ahmed Hassan  3500.0   1500.0        0.0     800.0     0.0       0.0   \n",
      "1     Lars Andersen  4500.0      0.0        0.0       0.0     0.0       0.0   \n",
      "2    Patricia Davis     0.0      0.0     4000.0       0.0     0.0       0.0   \n",
      "3     Robert Taylor  2900.0      0.0        0.0       0.0     0.0     600.0   \n",
      "4      Maria Garcia  2500.0      0.0        0.0       0.0  1000.0       0.0   \n",
      "5        John Smith  3000.0      0.0        0.0       0.0   500.0       0.0   \n",
      "6     Michael Brown  3400.0      0.0        0.0       0.0     0.0       0.0   \n",
      "7       Yuki Tanaka  3200.0      0.0        0.0       0.0     0.0       0.0   \n",
      "8   Jennifer Wilson  3100.0      0.0        0.0       0.0     0.0       0.0   \n",
      "9    Diego Martinez  2700.0      0.0        0.0       0.0     0.0       0.0   \n",
      "10       Amanda Lee     0.0      0.0        0.0    2200.0     0.0       0.0   \n",
      "11      Elena Popov     0.0      0.0        0.0       0.0  1800.0       0.0   \n",
      "12    Mary Williams     0.0   1200.0        0.0       0.0     0.0       0.0   \n",
      "13    Sarah Johnson     0.0      0.0      400.0       0.0     0.0       0.0   \n",
      "14           Li Wei     0.0      0.0      200.0       0.0     0.0       0.0   \n",
      "\n",
      "    total_weekly_income  \n",
      "0                5800.0  \n",
      "1                4500.0  \n",
      "2                4000.0  \n",
      "3                3500.0  \n",
      "4                3500.0  \n",
      "5                3500.0  \n",
      "6                3400.0  \n",
      "7                3200.0  \n",
      "8                3100.0  \n",
      "9                2700.0  \n",
      "10               2200.0  \n",
      "11               1800.0  \n",
      "12               1200.0  \n",
      "13                400.0  \n",
      "14                200.0  \n"
     ]
    }
   ],
   "source": [
    "# Using the income table for pivot example\n",
    "print(\"Original Income Data (first 10 rows):\")\n",
    "income_sample = pd.read_sql_query(\"SELECT * FROM income LIMIT 10\", conn)\n",
    "print(income_sample)\n",
    "\n",
    "# Pivot income by day of week\n",
    "pivot_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(CASE WHEN day = 'Monday' THEN amount ELSE 0 END) AS Monday,\n",
    "    SUM(CASE WHEN day = 'Tuesday' THEN amount ELSE 0 END) AS Tuesday,\n",
    "    SUM(CASE WHEN day = 'Wednesday' THEN amount ELSE 0 END) AS Wednesday,\n",
    "    SUM(CASE WHEN day = 'Thursday' THEN amount ELSE 0 END) AS Thursday,\n",
    "    SUM(CASE WHEN day = 'Friday' THEN amount ELSE 0 END) AS Friday,\n",
    "    SUM(CASE WHEN day = 'Saturday' THEN amount ELSE 0 END) AS Saturday,\n",
    "    SUM(amount) AS total_weekly_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING total_weekly_income > 0\n",
    "ORDER BY total_weekly_income DESC;\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(pivot_query, conn)\n",
    "print(\"\\\\nPivoted Income Data (by day of week):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b69a3-735b-415d-ae8d-97dac450b683",
   "metadata": {},
   "source": [
    "Melting, the opposite process, is very useful when we want to do analysis regarding a variable that is not the one determining the rows. For example, say that we have sales data in which each row corresponds to a different product, and different columns represent different years. If we instead want to analyze the expenditure in different years, we can melt the table, and then use grouping or filtering to select a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bdaf070-917d-4960-b7b0-3dabfb81ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted Income Data (Wide Format):\n",
      "\n",
      "              name  salary_income  freelance_income  investment_income  \\\n",
      "0     Ahmed Hassan         3500.0               0.0                  0   \n",
      "1       Amanda Lee            0.0            2200.0                  0   \n",
      "2   Diego Martinez         2700.0               0.0                  0   \n",
      "3  Jennifer Wilson         3100.0               0.0                  0   \n",
      "4       John Smith         3000.0             500.0                  0   \n",
      "\n",
      "   consulting_income  \n",
      "0             1500.0  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n",
      "\n",
      " Melted Income Data (Long Format):\n",
      "\n",
      "              name income_type  amount\n",
      "0     Ahmed Hassan  Consulting  1500.0\n",
      "1     Ahmed Hassan      Salary  3500.0\n",
      "2       Amanda Lee   Freelance  2200.0\n",
      "3   Diego Martinez      Salary  2700.0\n",
      "4  Jennifer Wilson      Salary  3100.0\n",
      "5       John Smith   Freelance   500.0\n",
      "6       John Smith      Salary  3000.0\n",
      "7    Lars Andersen      Salary  4500.0\n",
      "8           Li Wei  Investment   200.0\n",
      "9     Maria Garcia      Salary  2500.0\n",
      "\n",
      " This demonstrates how UNION ALL can transform wide data back to long format!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a pivoted view of income by source\n",
    "pivoted_income = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(CASE WHEN income_source = 'Salary' THEN amount ELSE 0 END) AS salary_income,\n",
    "    SUM(CASE WHEN income_source = 'Freelance' THEN amount ELSE 0 END) AS freelance_income,\n",
    "    SUM(CASE WHEN income_source = 'Investment' THEN amount ELSE 0 END) AS investment_income,\n",
    "    SUM(CASE WHEN income_source = 'Consulting' THEN amount ELSE 0 END) AS consulting_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING (salary_income + freelance_income + investment_income + consulting_income) > 0\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pivoted Income Data (Wide Format):\\n\")\n",
    "pivoted_df = pd.read_sql_query(pivoted_income, conn)\n",
    "print(pivoted_df)\n",
    "\n",
    "# Now melt it back to long format using UNION ALL\n",
    "melt_query = \"\"\"\n",
    "WITH pivoted_data AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(CASE WHEN income_source = 'Salary'    THEN amount ELSE 0 END) AS salary_income,\n",
    "        SUM(CASE WHEN income_source = 'Freelance' THEN amount ELSE 0 END) AS freelance_income,\n",
    "        SUM(CASE WHEN income_source = 'Investment' THEN amount ELSE 0 END) AS investment_income,\n",
    "        SUM(CASE WHEN income_source = 'Consulting' THEN amount ELSE 0 END) AS consulting_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    "),\n",
    "melted AS (\n",
    "    SELECT name, 'Salary'    AS income_type, salary_income    AS amount FROM pivoted_data WHERE salary_income    > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Freelance' AS income_type, freelance_income AS amount FROM pivoted_data WHERE freelance_income > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Investment' AS income_type, investment_income AS amount FROM pivoted_data WHERE investment_income > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Consulting' AS income_type, consulting_income AS amount FROM pivoted_data WHERE consulting_income > 0\n",
    ")\n",
    "SELECT *\n",
    "FROM   melted\n",
    "ORDER  BY name, income_type\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Melted Income Data (Long Format):\\n\")\n",
    "melted_df = pd.read_sql_query(melt_query, conn)\n",
    "print(melted_df)\n",
    "\n",
    "print(\"\\n This demonstrates how UNION ALL can transform wide data back to long format!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c8c90",
   "metadata": {},
   "source": [
    "<a id='window'></a>\n",
    "## 6¬†¬∑¬†Window Functions \n",
    "\n",
    "Estimated Time: 25 minutes\n",
    "\n",
    "**Definition.** A window is a subset of the table that is related in some prespecified way, in a very similar way that `GROUP BY` operates - with the crucial difference that it does not collapse the different rows into one, which allows us to preserve information. \n",
    "\n",
    "Window Functions operate separately into different windows. Common examples are\n",
    "\n",
    "- `RANK` - allow us to obtain the rank for different observations inside a given group. For example, we might want to see how much a consumer spent on his first purchase. So we would create a window for each consumer, apply the RANK window function, and then select those that have rank = 1.\n",
    "- `DENSE_RANK` - Similar to rank, but RANK jumps ranks if there are ties, while DENSE_RANK always has rankings as consecutive numbers. You might have seen this happening in college rankings!\n",
    "- `ROW_NUMBER()` - returns the row number of each observation. Very useful when the table lacks a primary key.\n",
    "- `SUM` - when used as a window function, allow us to perform cumulative sums - for example, cumulative sales up to a given date by each salesperson.\n",
    "- `AVG`/`MAX`/`MIN` - same as their aggregate versions, but allowing to keep information rather than collapsing rows.\n",
    "- `LAG`/`LEAD` - very useful in the context of time series, allow us to look at the previous/next value of a series.\n",
    "\n",
    "Basic Syntax: `WINDOW_FUNCTION() OVER (PARTITION BY columns ORDER BY columns)`\n",
    "\n",
    "Exception: `SUM(column) OVER (PARTITION BY columns)`\n",
    "\n",
    "üôã **Hands-Up:** Is the difference between a *window function* and an *aggregate function* clear? A. Yes‚ÄÉB. Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb058ef2-7ac0-4545-b861-9b596ad2e0bc",
   "metadata": {},
   "source": [
    "![Table annotated with running totals and row numbers to visualize how SUM() OVER and RANK window functions scan partitions.\n",
    "](../Images/window-function-visualization.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51754da-779a-4dd4-a8e4-1db0f519ab41",
   "metadata": {},
   "source": [
    "### Example 1: ```ROW_NUMBER``` - Rank customers by account balance within each country\n",
    "\n",
    "Unfortunately, sometimes we work with tables that do not have a primary key. A classic example is one listing many different transactions, including information such as customer, store, amount, method of payment - but no order id. ```ROW_NUMBER``` is a useful window function in this scenario, as it creates an additional column with, as the name suggests, the numbers of each column in the particular ordering the table is in. This works as a \"fake\" primary key that we can still use to perform operations such as ```JOIN```, even without a true primary key.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Forgetting the PARTITION BY clause in window functions will calculate across the entire table, not within groups. This is a common mistake that can lead to incorrect business logic. Always double-check that your window function includes the appropriate PARTITION BY when you need group-wise calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "346024b6-6310-4d46-af29-b399023b4ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ROW_NUMBER Example - Customer Rankings by Country:\n",
      "                name country  account_balance  balance_rank_in_country\n",
      "0       Sofia Santos      AE           352.84                        1\n",
      "1       Ahmed Hassan      AU           179.64                        1\n",
      "2      Anna Kowalski      BR           392.80                        1\n",
      "3       James Wilson      CA           449.81                        1\n",
      "4        Yuki Tanaka      CN           344.21                        1\n",
      "5          Lucy Chen      EG           167.10                        1\n",
      "6        Elena Popov      ES           845.86                        1\n",
      "7         Emma Brown      FR           929.69                        1\n",
      "8       Maria Garcia      GB           905.34                        1\n",
      "9     Isabella Silva      HK           833.92                        1\n",
      "10  Carlos Rodriguez      IN           140.70                        1\n",
      "11      Hans Schmidt      IT           104.97                        1\n",
      "12            Li Wei      JP           638.11                        1\n",
      "13          Jun Park      KR           756.11                        1\n",
      "14       Aisha Patel      MX           226.83                        1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_number_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    country,\n",
    "    account_balance,\n",
    "    ROW_NUMBER() OVER (PARTITION BY country ORDER BY account_balance DESC) as balance_rank_in_country\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL \n",
    "  AND account_balance IS NOT NULL\n",
    "ORDER BY country, balance_rank_in_country\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "print(\"1. ROW_NUMBER Example - Customer Rankings by Country:\")\n",
    "print(pd.read_sql_query(row_number_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde79739-2e95-4867-8ce8-d7d8dffb00ab",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Missing PARTITION BY in window functions\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Ranks ALL rows together, not by group\n",
    "SELECT name, country, balance,\n",
    "       RANK() OVER (ORDER BY balance DESC) as rank\n",
    "FROM customers\n",
    "\n",
    "-- ‚úÖ CORRECT: Use PARTITION BY to rank within groups\n",
    "SELECT name, country, balance,\n",
    "       RANK() OVER (PARTITION BY country ORDER BY balance DESC) as rank\n",
    "FROM customers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b75f2-922e-4309-91f1-2c6d11513b33",
   "metadata": {},
   "source": [
    "### Example 2: Running total - Cumulative spending by purchase date\n",
    "\n",
    "A very common application in business is to understand not just the spending or revenue in a given day, but the total amount spent/received up to a given date. This is the idea of a cumulative sum - the running total including previous purchases. In SQL, we can use the Window Function versiom of ```SUM``` - which is just like its aggregate function counterpart, but includes the characteristic OVER () component of window functions. This can be used to indicate both how we want to partition the dataset, and how we would like to order the subgroups, since the order matters when calculating cumulative sums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6062ae42-4f68-4db2-91eb-ba52042d6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Running Total Example - Cumulative Spending Over Time:\n",
      "            name last_purchase  purchase_amount  running_total\n",
      "0     Emma Brown    2024-01-28           517.44         517.44\n",
      "1   James Wilson    2024-02-02           331.04         848.48\n",
      "2    Ivan Petrov    2024-02-04           399.48        1247.96\n",
      "3         Li Wei    2024-02-10           155.98        1403.94\n",
      "4    Yuki Tanaka    2024-02-17           299.36        1703.30\n",
      "5    Elena Popov    2024-03-02            56.37        1759.67\n",
      "6  Lars Andersen    2024-04-08           125.88        1885.55\n",
      "7      Raj Kumar    2024-04-10           380.50        2266.05\n",
      "8   Ahmed Hassan    2024-05-14           177.45        2443.50\n",
      "9    Aisha Patel    2024-06-20           109.14        2552.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "running_total_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    last_purchase,\n",
    "    items_purchased * price_per_item as purchase_amount,\n",
    "    SUM(items_purchased * price_per_item) \n",
    "        OVER (ORDER BY last_purchase ROWS UNBOUNDED PRECEDING) as running_total\n",
    "FROM customers\n",
    "WHERE last_purchase IS NOT NULL \n",
    "  AND items_purchased IS NOT NULL \n",
    "  AND price_per_item IS NOT NULL\n",
    "ORDER BY last_purchase\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n2. Running Total Example - Cumulative Spending Over Time:\")\n",
    "print(pd.read_sql_query(running_total_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862a74c-002b-42f7-9488-76c22bea1a52",
   "metadata": {},
   "source": [
    "### Example 3: AVG with PARTITION BY - Compare to group average\n",
    "\n",
    "One of the main motivations for the use of window functions is precisely to have an alternative to ```GROUP BY``` that still allow us to retain individual level information. This is very useful when trying to compare each individual element to a group it belongs - for example, compare the customer balance with the average balance for customers in his country. Without window functions, we would need to first aggregate the data using an aggregate function with ```GROUP BY```, and then use ```JOIN``` to make a new table, and then perform the comparison. Window functions allow us to do this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd58ac6-6561-4202-a116-527659c8a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3. Window AVG Example - Customer Balance vs Country Average:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>account_balance</th>\n",
       "      <th>country_avg_balance</th>\n",
       "      <th>diff_from_country_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sofia Santos</td>\n",
       "      <td>AE</td>\n",
       "      <td>352.84</td>\n",
       "      <td>352.84</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>AU</td>\n",
       "      <td>179.64</td>\n",
       "      <td>179.64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anna Kowalski</td>\n",
       "      <td>BR</td>\n",
       "      <td>392.80</td>\n",
       "      <td>392.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James Wilson</td>\n",
       "      <td>CA</td>\n",
       "      <td>449.81</td>\n",
       "      <td>449.81</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>CN</td>\n",
       "      <td>344.21</td>\n",
       "      <td>344.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lucy Chen</td>\n",
       "      <td>EG</td>\n",
       "      <td>167.10</td>\n",
       "      <td>167.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>ES</td>\n",
       "      <td>845.86</td>\n",
       "      <td>845.86</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Emma Brown</td>\n",
       "      <td>FR</td>\n",
       "      <td>929.69</td>\n",
       "      <td>929.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>GB</td>\n",
       "      <td>905.34</td>\n",
       "      <td>905.34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Isabella Silva</td>\n",
       "      <td>HK</td>\n",
       "      <td>833.92</td>\n",
       "      <td>833.92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name country  account_balance  country_avg_balance  \\\n",
       "0    Sofia Santos      AE           352.84               352.84   \n",
       "1    Ahmed Hassan      AU           179.64               179.64   \n",
       "2   Anna Kowalski      BR           392.80               392.80   \n",
       "3    James Wilson      CA           449.81               449.81   \n",
       "4     Yuki Tanaka      CN           344.21               344.21   \n",
       "5       Lucy Chen      EG           167.10               167.10   \n",
       "6     Elena Popov      ES           845.86               845.86   \n",
       "7      Emma Brown      FR           929.69               929.69   \n",
       "8    Maria Garcia      GB           905.34               905.34   \n",
       "9  Isabella Silva      HK           833.92               833.92   \n",
       "\n",
       "   diff_from_country_avg  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "5                    0.0  \n",
       "6                    0.0  \n",
       "7                    0.0  \n",
       "8                    0.0  \n",
       "9                    0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_comparison_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    country,\n",
    "    account_balance,\n",
    "    ROUND(AVG(account_balance) OVER (PARTITION BY country), 2) as country_avg_balance,\n",
    "    ROUND(account_balance - AVG(account_balance) OVER (PARTITION BY country), 2) as diff_from_country_avg\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL \n",
    "  AND account_balance IS NOT NULL\n",
    "ORDER BY country, diff_from_country_avg DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n 3. Window AVG Example - Customer Balance vs Country Average:\")\n",
    "pd.read_sql_query(avg_comparison_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a5d20-deb4-4493-9782-005b44b74094",
   "metadata": {},
   "source": [
    "### Example 4: RANK vs DENSE_RANK - Ranking with ties\n",
    "\n",
    "Another common example is to obtain the relative rank of observations in a given group. This is an incredibly complex task without window function - one would need to find all the possible groups, use ```WHERE``` commands to isolate them, order them, and then merge all of the isolated groups together. ```RANK``` allows us to perform this operation in a single command. \n",
    "\n",
    "One interesting feature of ```RANK``` is that it leaves gaps after ties. This is common procedure in some applications - for example college rankings. However we might want to not allow for these gaps, and just allow for multiple entries at the same rank. This is exactly what ```DENSE_RANK``` does - dense here referring to the fact that there are no gaps in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55f5cacb",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4. RANK vs DENSE_RANK Example:\n",
      "              name  total_income  rank_with_gaps  dense_rank_no_gaps\n",
      "0     Ahmed Hassan        5800.0               1                   1\n",
      "1    Lars Andersen        4500.0               2                   2\n",
      "2   Patricia Davis        4000.0               3                   3\n",
      "3       John Smith        3500.0               4                   4\n",
      "4     Maria Garcia        3500.0               4                   4\n",
      "5    Robert Taylor        3500.0               4                   4\n",
      "6    Michael Brown        3400.0               7                   5\n",
      "7      Yuki Tanaka        3200.0               8                   6\n",
      "8  Jennifer Wilson        3100.0               9                   7\n",
      "9   Diego Martinez        2700.0              10                   8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rank_comparison_query = \"\"\"\n",
    "WITH income_totals AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(amount) as total_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    ")\n",
    "SELECT \n",
    "    name,\n",
    "    total_income,\n",
    "    RANK() OVER (ORDER BY total_income DESC) as rank_with_gaps,\n",
    "    DENSE_RANK() OVER (ORDER BY total_income DESC) as dense_rank_no_gaps\n",
    "FROM income_totals\n",
    "ORDER BY total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n 4. RANK vs DENSE_RANK Example:\")\n",
    "print(pd.read_sql_query(rank_comparison_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba51a9-2c3d-4851-861f-92e3a59ebc90",
   "metadata": {},
   "source": [
    "üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Customers Who Out-Spend Their Country Average\n",
    "(Window-Function Edition)\n",
    "\n",
    "In the previous challenge you solved this task with the standard method:\n",
    "GROUP BY ‚Üí sub-query ‚Üí JOIN.\n",
    "\n",
    "Now let‚Äôs do the same calculation with a single window function.\n",
    "\n",
    "Goal: List every customer whose total spending ( items_purchased √ó price_per_item ) is strictly higher than the average spending of customers in the same country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdbbdd0c-11be-4a7d-8dd6-28e51d3e9070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>country_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "      <td>654.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name country  total_spent  country_avg\n",
       "0  Mary Williams    None      1183.71      654.795"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_avg_window_sql = \"\"\"\n",
    "WITH spending AS (\n",
    "    SELECT\n",
    "           name,\n",
    "           country,\n",
    "           items_purchased * price_per_item AS total_spent,\n",
    "           AVG(items_purchased * price_per_item)\n",
    "                 OVER (PARTITION BY country) AS country_avg\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    "      AND  price_per_item  IS NOT NULL\n",
    ")\n",
    "SELECT *\n",
    "FROM   spending\n",
    "WHERE  total_spent > country_avg\n",
    "ORDER  BY country, total_spent DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(country_avg_window_sql, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5bb3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "## Key Points\n",
    "\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "* **JOINs combine tables on matching values** - Use ON or USING to specify the relationship between tables\n",
    "* **LEFT JOIN preserves all rows from the left table** - Even when there's no match in the right table, making it ideal for finding missing relationships\n",
    "* **CTEs improve readability** - Use WITH clauses to break complex queries into named, reusable components\n",
    "* **Window functions maintain row-level detail** - Unlike GROUP BY, they calculate aggregates while keeping all original rows\n",
    "* **PARTITION BY creates calculation groups** - Similar to GROUP BY but for window functions\n",
    "* **Subqueries can filter or transform data** - Use them in WHERE clauses for filtering or FROM clauses as derived tables\n",
    "* **COALESCE handles NULL values gracefully** - Replace NULLs with meaningful defaults in JOINs and calculations\n",
    "* **Self-JOINs compare rows within the same table** - Essential for finding relationships between records in a single table\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
