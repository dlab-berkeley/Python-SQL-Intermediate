{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe48a7aa",
   "metadata": {},
   "source": [
    "# SQL for Data Analysis  \n",
    "### Introductory Workshop\n",
    "*D‚ÄëLab, UC¬†Berkeley*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bfe0c-a0aa-44a6-8afa-5ed8e5d23c40",
   "metadata": {},
   "source": [
    "### Welcome & Environment Check \n",
    "\n",
    "Estimated time: 8 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69798bf",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. [Why SQL?](#why-sql)\n",
    "3. [Relational Databases](#relational)    \n",
    "4. [Importing CSV ‚Üí SQLite](#sqlite)  \n",
    "5. [SQLite Data Types & NULLs](#types)  \n",
    "6. [SELECT & Derived Columns](#select)  \n",
    "7. [Filtering Rows with WHERE](#where)  \n",
    "8. [Aggregates & GROUP¬†BY](#groupby)  \n",
    "9. [Sorting & Paging Results](#orderby)   \n",
    "10. [Key Points & Next Steps](#keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c357b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Learning Goals</b><br><br>\n",
    "By the end of this workshop you will be able to:\n",
    "<ul>\n",
    "<li>Understand why one would use SQL and why and how it is complementary to Pandas</li>\n",
    "<li>Write basic queries with <code>SELECT</code>, create aliases, and build derived columns.</li>\n",
    "<li>Filter rows using <code>WHERE</code>, sort and paginate result sets with <code>ORDER¬†BY</code>, <code>LIMIT</code>, <code>OFFSET</code>.</li>\n",
    "<li>Summarise data with aggregates &nbsp;(<code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>)&nbsp; and <code>GROUP¬†BY</code>.</li>\n",
    "<li>Filter groups with <code>HAVING</code> and understand why it is different from <code>WHERE</code></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18846946",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Prerequisites</b><br><br>\n",
    "Before starting this workshop, you should:\n",
    "<ul>\n",
    "<li>Have basic familiarity with data analysis concepts</li>\n",
    "<li>Have Python and Jupyter Notebook installed (optional, but helpful for following along)</li>\n",
    "<li>Download the workshop materials (we'll provide these)</li>\n",
    "<li>Not strictly necessary, but it would be very useful to have a notion of how to do operations on Pandas (or any other similar library/package/software) to better contextualize the material.</li>\n",
    "</ul>\n",
    "\n",
    "No prior SQL experience is required!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Setup Note</b><br><br>\n",
    "If you want to follow along with the exercises: <br>\n",
    "1. Make sure you have the required Python packages installed:<br>\n",
    "   - pandas<br>\n",
    "   - sqlite3<br>\n",
    "   - sqlalchemy<br>\n",
    "2. Download the example database file we'll be using\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee7c91-54b9-494e-8fab-4042f404677a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce8be261-d760-4db0-b353-865dc1e52418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The sqlite3 library is imported and available.\n",
      "‚úÖ The database file is available and in the appropriate folder\n"
     ]
    }
   ],
   "source": [
    "# Importing packages that we will need\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd         \n",
    "import os, pathlib, sys\n",
    "\n",
    "# The first thing we need to do is connect to our SQLite database, which is the file that stores the information we will be using.\n",
    "# If there is no pre-existing database with this name, it automatically creates one.\n",
    "# Every SQL query will be executed through this `conn` object - which can be thought as a channel to the SQLite file.\n",
    "conn = sqlite3.connect('../Data/customers.sqlite')\n",
    "engine = create_engine('sqlite:///customers.sqlite')\n",
    "\n",
    "# Load the customers table into a DataFrame for displ\n",
    "\n",
    "\n",
    "# Troubleshooting: Please run the following cells to check whether sqlite is imported and the database file is in its appropriate location!\n",
    "\n",
    "try:\n",
    "    import sqlite3                        \n",
    "    print(\"‚úÖ The sqlite3 library is imported and available.\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå sqlite3 library not found.\")\n",
    "\n",
    "# 2Ô∏è‚É£ Check that the workshop DB file exists\n",
    "from pathlib import Path\n",
    "print(\"‚úÖ The database file is available and in the appropriate folder\" if Path(\"../Data/customers.sqlite\").exists()\n",
    "      else \"‚ùå ../Data/customers.sqlite is missing‚Äîdownload the data bundle and place it there.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d556f-308a-42d0-8f4d-3e34434c0180",
   "metadata": {},
   "source": [
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üìù **Poll:** A Zoom poll to help you learn!<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "üôã Hands-Up: Quick pulse-check or mini-quiz. Respond by choosing the option that matches how you feel/what you think."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03e0eb",
   "metadata": {},
   "source": [
    "<a id='why-sql'></a>\n",
    "## 1‚ÄØ¬∑‚ÄØWhy SQL? \n",
    "\n",
    "Estimated time: 15 minutes\n",
    "\n",
    "A very common question when people learn about the existence of SQL is: Why bother learning it? Can't I just use pandas and pandas dataframes?\n",
    "\n",
    "### Learning Objective\n",
    "Understand when and why to use SQL instead of pandas, and how these tools complement each other in data analysis workflows.\n",
    "\n",
    "üìù **Poll 1:** What is the usual size of databases you use? How large do you think databases can get?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06897baf",
   "metadata": {},
   "source": [
    "### Bookcase¬†vs¬†Desk Analogy  \n",
    "Think of your computer‚Äôs RAM as the desk where you spread papers you‚Äôre actively working on, and the hard‚Äëdrive as the bookcase that stores all your books.\n",
    "\n",
    "Working at your desk is fast, but at the same time it doesn't hold nearly as much material as a bookshelf. So if you are working with very large datasets, you are at risk of overloading your desk with material. \n",
    "\n",
    "* **Pandas** is excellent for analysis of *smaller* data that fits comfortably on the desk.  \n",
    "* **SQL** is the tool we use to selectively bring only the information we need from the bookcases to the desk.\n",
    "\n",
    "üôã **Hands-Up:** Did the analogy make sense? A. Yes‚ÄÉB. Still fuzzy\n",
    "\n",
    " **Example:** A 20‚ÄØGB transaction table can be grouped and aggregated with a single SQL statement, whereas pandas would first need 20‚ÄØGB of memory just to read the file.\n",
    "\n",
    "### Rough size sweet-spots on a single machine\n",
    "\n",
    "- **Pandas (all in RAM)** ‚Äì ideal up to **‚âà 5 million rows** (around **500 MB** of CSV). Beyond that, Python overhead (‚âà 5‚Äì10√ó the raw file size) quickly exhausts memory.  \n",
    "- **SQLite (single-file engine)** ‚Äì stays responsive with **tens or even hundreds of millions of rows** (roughly **10‚Äì500 GB**). Because it streams pages from disk, RAM is rarely the bottleneck. File-system ceiling: ‚âà 280 TB.  \n",
    "- **PostgreSQL (server engine)** ‚Äì comfortably handles **100 GB to several terabyte tables** on industrial hardware; technical cap is **32 TB per table**, but overall database size is **unlimited**\n",
    "\n",
    "\n",
    "### Complements, not Substitutes \n",
    "In practice we often:\n",
    "\n",
    "1. Use SQL to slice/aggregate huge tables, producing a manageable result set  \n",
    "2. Pull that into pandas for plotting or modelling\n",
    "\n",
    "**üí°Tip:** When working with large datasets, try to do as much filtering and aggregation as possible in SQL before pulling data into pandas. This can dramatically improve performance!\n",
    "\n",
    "üîî **Question:**  Have you ever dealt with a problem in which SQL was necessary? If not, can you think of one real life application that might require it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f7c30",
   "metadata": {},
   "source": [
    "<a id='relational'></a>\n",
    "## 2‚ÄØ¬∑‚ÄØRelational Databases \n",
    "\n",
    "Estimated time: 12 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Grasp the fundamental concepts of relational databases, including tables, keys, and relationships between data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4020f",
   "metadata": {},
   "source": [
    "## What is a Relational Database?\n",
    "\n",
    "<div class=\"alert alert-primary\">\n",
    "<b>üîë Key Concept: Relational Databases</b><br><br>\n",
    "A relational database organizes data into tables (sometimes called relations), which consist of rows and columns:\n",
    "\n",
    "- **Tables**: Collections of related data (e.g., Customers, Orders)\n",
    "- **Columns**: Specific attributes (e.g., CustomerID, FirstName)\n",
    "- **Rows**: Individual records in the table\n",
    "- **Primary Keys**: Unique identifiers for each row\n",
    "- **Foreign Keys**: References to primary keys in other tables that create relationships\n",
    "\n",
    "![Database sample of Customers, Orders and OrderDetails tables, highlighting how relational data splits across tables.](../Images/relational-databases.svg)\n",
    "\n",
    "Let's take a look at these example:\n",
    "- Combined, these two tables form a database - a collection of data organized in tables.\n",
    "- In this particular case, there are two tables, each one storing one type of information - either about Customers, or about Orders. But databases usually consist of many tables!\n",
    "- Each table consists of rows - identifying individual records of information - and columns, which provide a piece of information for a given field corresponding to each row.\n",
    "- Both of these tables have primary keys - columns that uniquely identify each row, i.e, each row has one, and no two rows share a primary key. Notice however the two tables do not have the same primary key!\n",
    "- Further, the Order table has a foreign key \"Customer ID\" - a column linking to the primary key of the Customers table. Each order can be associated to a single customer through this relationship. But that doesn't mean each customer has just one order!\n",
    "\n",
    "üìù **Poll 2:** Can you think of a table that would not have a Primary Key?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a9e10-a518-48ee-9baa-0e42f5a2088f",
   "metadata": {},
   "source": [
    "<a id='sqlite'></a>\n",
    "## 3 ¬∑ Getting Started with SQLite \n",
    "\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Understand how to set up SQLite, import data from CSV files, and establish a database connection for querying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b316844-fb7e-458a-84ea-31be758e93f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What is SQLite?\n",
    "\n",
    "SQLite is a lightweight relational database. It is easy to install, use, and even though it doesn't have as many advanced capabilities as PostgreSQL or MySQL, it is a great way to start learning SQL. \n",
    "\n",
    "SQL queries are very similar (if not identical) across these programs, so there is a lot of transfer of knowledge if you decide to move on to another one in the future.\n",
    "\n",
    "### When to Use Different Databases\n",
    "\n",
    "| What You're Doing | SQLite | MySQL/PostgreSQL |\n",
    "|-------------------|--------|------------------|\n",
    "| Getting Started | Great for learning SQL | More complex to set up |\n",
    "| Working Locally |  Just a file you can copy | Needs server installation |\n",
    "| Small-Medium Projects |  Up to ~1TB of data | Any size |\n",
    "| Data Science Projects |  Works great with Python | Needs more setup |\n",
    "| Team Projects | Limited sharing options | Better for collaboration |\n",
    "\n",
    "üí° **Real-World Examples**:\n",
    "- **SQLite**: Your Jupyter notebooks, research projects, personal data analysis\n",
    "- **MySQL/PostgreSQL**: Company databases, shared research data, production systems\n",
    "\n",
    "The key takeaway: SQLite is perfect for learning SQL and doing your own data analysis. When you need to share data with a team or work on very large datasets, you can easily switch to MySQL or PostgreSQL - the SQL commands will be almost exactly the same!\n",
    "\n",
    "\n",
    "üìù **Poll 3:** What's your primary reason for learning SQL today? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560fa93-c30b-4f59-8ac8-d33a117bf070",
   "metadata": {},
   "source": [
    "### SQLite and Python\n",
    "\n",
    "In this workshop, we'll work with SQLite directly in a Jupyter notebook using the `sqlite3` Python library, which comes built into Python.\n",
    "\n",
    "We could create tables with `CREATE TABLE` statements, as well as add new data (INSERT), change existing data (UPDATE), and remove data (DELETE) directly on SQL, but in this workshop we will focus on querying data from a pre-existing database. I have added an example of how can this be done in an auxiliary file that I used to create the database for this workshop, if you would like to explore that later!\n",
    "\n",
    "\n",
    "üí°**Tip:** In this workshop we will be focusing on databases that contain a single table. In the next workshop, we will learn how to combine information from multiple tables. \n",
    "\n",
    "Let's upload our sql database as a dataframe in pandas:\n",
    "\n",
    "customers_df = pd.read_sql_query('SELECT * FROM customers', conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ae643-8dd7-4618-a9cf-f2f6b212db02",
   "metadata": {},
   "source": [
    "ü•ä **Challenge**: Can you use pandas to take a look at what this dataset looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6975afe-2062-48b0-880a-ede98f40b3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>price_per_item</th>\n",
       "      <th>last_purchase</th>\n",
       "      <th>account_balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.28</td>\n",
       "      <td>None</td>\n",
       "      <td>945.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>GB</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>905.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>JP</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.18</td>\n",
       "      <td>2024-02-10</td>\n",
       "      <td>638.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emma Brown</td>\n",
       "      <td>Paris</td>\n",
       "      <td>FR</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64.68</td>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>929.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>AU</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.35</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>179.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name      city country  items_purchased  price_per_item  \\\n",
       "0    John Smith  New York      US              NaN           56.28   \n",
       "1  Maria Garcia    London      GB             15.0             NaN   \n",
       "2        Li Wei     Tokyo      JP             11.0           14.18   \n",
       "3    Emma Brown     Paris      FR              8.0           64.68   \n",
       "4  Ahmed Hassan    Sydney      AU              7.0           25.35   \n",
       "\n",
       "  last_purchase  account_balance  \n",
       "0          None           945.55  \n",
       "1    2024-08-18           905.34  \n",
       "2    2024-02-10           638.11  \n",
       "3    2024-01-28           929.69  \n",
       "4    2024-05-14           179.64  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SOLUTION:\n",
    "customers_df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897e82d",
   "metadata": {},
   "source": [
    "<a id='types'></a>\n",
    "## 4‚ÄØ¬∑‚ÄØSQLite Data Types & `NULL` \n",
    "\n",
    "Estimated time: 7 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Understand SQLite's data types and how NULL values are handled in SQL databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538c404",
   "metadata": {},
   "source": [
    "SQLite has a flexible approach to data types. While columns are designed with a recommended data type (like TEXT or INTEGER), SQLite can actually store any type of data in any column.\n",
    "\n",
    "* **INTEGER** ‚Äì whole numbers \n",
    "* **REAL** ‚Äì floating‚Äëpoint  \n",
    "* **TEXT** ‚Äì strings    \n",
    "* **NULL** ‚Äì missing / undefined  \n",
    "\n",
    "Unlike PostgreSQL/MySQL, SQLite does NOT have a date format, but it does have dedicated functions that interpret strings as dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fe208",
   "metadata": {},
   "source": [
    "<a id='select'></a>\n",
    "## 5‚ÄØ¬∑‚ÄØSELECT & Derived Columns \n",
    "\n",
    "Estimated time: 18 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Master the fundamental SQL SELECT statement, including column selection, aliasing, and creating derived columns using basic SQL functions.\n",
    "\n",
    "Real-World Application: Extract the relevant customer information for a marketing campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de336ab3-a620-4815-9435-ef9a1884a575",
   "metadata": {},
   "source": [
    "### The SELECT statement is the foundation of SQL queries. We'll learn it step by step:\n",
    "\n",
    "1. Basic SELECT - Retrieving data from tables\n",
    "2. Column Aliases - Giving friendly names to columns\n",
    "3. Derived Columns - Creating new columns from existing ones\n",
    "4. SQL Functions - Transforming data in useful ways\n",
    "\n",
    "üí° Tip: We'll build from simple queries to more complex ones, making sure each concept is clear before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1d2c2-434b-4bd5-be25-ae31e59278e4",
   "metadata": {},
   "source": [
    "At its most basic version, a query looks like this:\n",
    "\n",
    "```sql\n",
    "SELECT column1, column2, ‚Ä¶\n",
    "FROM   table;\n",
    "```\n",
    "\n",
    "- One of the most confusing aspects of SQL, especially in the beginning, is understading the order in which SQL reads the instructions of a query - which unfortunately is not the order in which the commands are written.\n",
    "- Here it is better to think that the first command is \"FROM\" - which tells us where to draw the information from\n",
    "- After that, we can select only the columns we will need by listing them after the SELECT statement.\n",
    "- Notice that:\n",
    "    - We don't need to put names of columns and tables between quotes\n",
    "    - We do need to separate different columns by commas - but not one at the end!\n",
    "\n",
    "üôã **Hands-Up:** Which clause executes *first* in SQL‚Äôs logical order? A. `SELECT`‚ÄÉB. `FROM`‚ÄÉC. `WHERE`\n",
    "\n",
    "üí° **Tip**: When selecting multiple columns, use line breaks after commas for better readability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb48082-f50e-4e31-b15d-beef77b47e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SELECT:\n",
      "\n",
      "                name         city country  items_purchased\n",
      "0         John Smith     New York      US              NaN\n",
      "1       Maria Garcia       London      GB             15.0\n",
      "2             Li Wei        Tokyo      JP             11.0\n",
      "3         Emma Brown        Paris      FR              8.0\n",
      "4       Ahmed Hassan       Sydney      AU              7.0\n",
      "5      Sarah Johnson       Berlin      DE             19.0\n",
      "6   Carlos Rodriguez       Mumbai      IN             11.0\n",
      "7      Anna Kowalski    S√£o Paulo      BR             11.0\n",
      "8       James Wilson      Toronto      CA              4.0\n",
      "9        Yuki Tanaka     Shanghai      CN              8.0\n",
      "10       Elena Popov       Madrid      ES              3.0\n",
      "11     Michel Dubois       Moscow      RU              2.0\n",
      "12      Sofia Santos        Dubai      AE             12.0\n",
      "13     Lars Andersen         None    None              6.0\n",
      "14       Aisha Patel  Mexico City      MX              2.0\n",
      "15    Diego Martinez    Amsterdam      NL              1.0\n",
      "16         Lucy Chen        Cairo      EG             12.0\n",
      "17       Ivan Petrov    Stockholm      SE             12.0\n",
      "18     Mary Williams         None    None             17.0\n",
      "19         Raj Kumar  Los Angeles      US             10.0\n",
      "20      Hans Schmidt         Rome      IT             16.0\n",
      "21    Isabella Silva    Hong Kong      HK             15.0\n",
      "22    Fatima Al-Said     Istanbul      TR              NaN\n",
      "23          Jun Park        Seoul      KR             19.0\n",
      "24      Anna Ivanova      Bangkok      TH             12.0\n"
     ]
    }
   ],
   "source": [
    "# Basic SELECT query\n",
    "query = \"\"\"\n",
    "SELECT name, city, country, items_purchased\n",
    "FROM customers\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "print(\"Basic SELECT:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78021ea7-f2b9-4f1b-909b-33fcd1a479d7",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Common Mistake - Missing commas between columns\n",
    "```sql\n",
    "-- ‚ùå WRONG: Missing commas\n",
    "SELECT name city country\n",
    "FROM customers\n",
    "\n",
    "-- ‚úÖ CORRECT: Columns separated by commas\n",
    "SELECT name, city, country\n",
    "FROM customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a20c8-208c-442b-a578-af51f14d7cd4",
   "metadata": {},
   "source": [
    "However, queries are extremely flexible and allow us to combine/summarize information in many ways.\n",
    "\n",
    "Let's take a look at some examples of more interesting queries\n",
    "\n",
    "### Step‚Äëby‚ÄëStep Examples\n",
    "\n",
    "1. **Select everything:**\n",
    "\n",
    "\n",
    "   If you want to keep all columns from a table, you can just use * instead of naming the columns.\n",
    "\n",
    "\n",
    "   ```sql\n",
    "   SELECT * \n",
    "   FROM customers;\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9626d61-2d8d-49cc-ba3f-8451edefc1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * - All columns:\n",
      "\n",
      "                name         city country  items_purchased  price_per_item  \\\n",
      "0         John Smith     New York      US              NaN           56.28   \n",
      "1       Maria Garcia       London      GB             15.0             NaN   \n",
      "2             Li Wei        Tokyo      JP             11.0           14.18   \n",
      "3         Emma Brown        Paris      FR              8.0           64.68   \n",
      "4       Ahmed Hassan       Sydney      AU              7.0           25.35   \n",
      "5      Sarah Johnson       Berlin      DE             19.0           15.85   \n",
      "6   Carlos Rodriguez       Mumbai      IN             11.0           95.40   \n",
      "7      Anna Kowalski    S√£o Paulo      BR             11.0           96.91   \n",
      "8       James Wilson      Toronto      CA              4.0           82.76   \n",
      "9        Yuki Tanaka     Shanghai      CN              8.0           37.42   \n",
      "10       Elena Popov       Madrid      ES              3.0           18.79   \n",
      "11     Michel Dubois       Moscow      RU              2.0           71.58   \n",
      "12      Sofia Santos        Dubai      AE             12.0           49.61   \n",
      "13     Lars Andersen         None    None              6.0           20.98   \n",
      "14       Aisha Patel  Mexico City      MX              2.0           54.57   \n",
      "15    Diego Martinez    Amsterdam      NL              1.0           13.09   \n",
      "16         Lucy Chen        Cairo      EG             12.0           91.84   \n",
      "17       Ivan Petrov    Stockholm      SE             12.0           33.29   \n",
      "18     Mary Williams         None    None             17.0           69.63   \n",
      "19         Raj Kumar  Los Angeles      US             10.0           38.05   \n",
      "20      Hans Schmidt         Rome      IT             16.0             NaN   \n",
      "21    Isabella Silva    Hong Kong      HK             15.0           59.20   \n",
      "22    Fatima Al-Said     Istanbul      TR              NaN           26.64   \n",
      "23          Jun Park        Seoul      KR             19.0           97.26   \n",
      "24      Anna Ivanova      Bangkok      TH             12.0           79.76   \n",
      "\n",
      "   last_purchase  account_balance  \n",
      "0           None           945.55  \n",
      "1     2024-08-18           905.34  \n",
      "2     2024-02-10           638.11  \n",
      "3     2024-01-28           929.69  \n",
      "4     2024-05-14           179.64  \n",
      "5     2024-07-19              NaN  \n",
      "6     2024-11-23           140.70  \n",
      "7     2024-09-24           392.80  \n",
      "8     2024-02-02           449.81  \n",
      "9     2024-02-17           344.21  \n",
      "10    2024-03-02           845.86  \n",
      "11    2024-08-03           421.08  \n",
      "12    2024-10-19           352.84  \n",
      "13    2024-04-08           588.43  \n",
      "14    2024-06-20           226.83  \n",
      "15    2024-12-25           821.98  \n",
      "16    2024-08-01           167.10  \n",
      "17    2024-02-04           988.20  \n",
      "18    2024-08-14           795.02  \n",
      "19    2024-04-10              NaN  \n",
      "20          None           104.97  \n",
      "21    2024-09-13           833.92  \n",
      "22          None           736.17  \n",
      "23    2024-08-05           756.11  \n",
      "24    2024-09-11           794.14  \n"
     ]
    }
   ],
   "source": [
    "# Select all columns example\n",
    "select_all_query = \"\"\"\n",
    "SELECT * \n",
    "FROM customers\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(select_all_query, conn)\n",
    "print(\"SELECT * - All columns:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca022924-82fa-4db6-9e2e-0f73c46f2644",
   "metadata": {},
   "source": [
    "üîî **Question:** When might using SELECT * be problematic in a real database?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ab7a4-3380-46ba-9e88-7684aff33430",
   "metadata": {},
   "source": [
    "### 5.2 Column Aliases\n",
    "\n",
    "    Sometimes tables have very complicated/uninformative/ambiguous names. We can rename the header of a column using an alias, which is a new name to columns in our result. This is useful when:\n",
    "    - Making technical names more readable\n",
    "    - Clarifying the meaning of computed columns\n",
    "    - Creating reports for non-technical users\n",
    "\n",
    "Syntax using AS (recommended for clarity):\n",
    "\n",
    "   ```sql\n",
    "   SELECT column AS new_name,\n",
    "   FROM   table;\n",
    "   ```\n",
    "\n",
    "Alternative syntax (implicit):\n",
    "\n",
    "   ```sql\n",
    "   SELECT column new_name,\n",
    "   FROM   table;\n",
    "   ```\n",
    "\n",
    "‚ö†Ô∏è **Warning:** If your new name contains spaces or is a special name, you must wrap it in quotes:\n",
    "\n",
    "   ```sql\n",
    "   SELECT column AS \"Full Name\",\n",
    "   FROM   table;\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3722d850-ce58-4134-9dce-1978c0687fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT with Aliases: \n",
      "\n",
      "       customer_name     location  quantity  unit_price\n",
      "0       Maria Garcia       London      15.0         NaN\n",
      "1             Li Wei        Tokyo      11.0       14.18\n",
      "2         Emma Brown        Paris       8.0       64.68\n",
      "3       Ahmed Hassan       Sydney       7.0       25.35\n",
      "4      Sarah Johnson       Berlin      19.0       15.85\n",
      "5   Carlos Rodriguez       Mumbai      11.0       95.40\n",
      "6      Anna Kowalski    S√£o Paulo      11.0       96.91\n",
      "7       James Wilson      Toronto       4.0       82.76\n",
      "8        Yuki Tanaka     Shanghai       8.0       37.42\n",
      "9        Elena Popov       Madrid       3.0       18.79\n",
      "10     Michel Dubois       Moscow       2.0       71.58\n",
      "11      Sofia Santos        Dubai      12.0       49.61\n",
      "12     Lars Andersen         None       6.0       20.98\n",
      "13       Aisha Patel  Mexico City       2.0       54.57\n",
      "14    Diego Martinez    Amsterdam       1.0       13.09\n",
      "15         Lucy Chen        Cairo      12.0       91.84\n",
      "16       Ivan Petrov    Stockholm      12.0       33.29\n",
      "17     Mary Williams         None      17.0       69.63\n",
      "18         Raj Kumar  Los Angeles      10.0       38.05\n",
      "19      Hans Schmidt         Rome      16.0         NaN\n",
      "20    Isabella Silva    Hong Kong      15.0       59.20\n",
      "21          Jun Park        Seoul      19.0       97.26\n",
      "22      Anna Ivanova      Bangkok      12.0       79.76\n"
     ]
    }
   ],
   "source": [
    "# Column aliases example\n",
    "alias_query = \"\"\"\n",
    "SELECT \n",
    "    name AS customer_name,\n",
    "    city AS location,\n",
    "    items_purchased AS quantity,\n",
    "    price_per_item AS unit_price\n",
    "FROM customers\n",
    "WHERE items_purchased IS NOT NULL\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(alias_query, conn)\n",
    "print(\"SELECT with Aliases: \\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408e250-286c-4d0c-8278-390f5815f3e2",
   "metadata": {},
   "source": [
    "### 5.3 Derived Columns\n",
    "\n",
    "Very commonly we might want to create what we call a \"Derived Column\" - which is a column that modifies/combines information from columns of the original table. \n",
    "\n",
    "The way that we do this is usually by using functions. The syntax is function(column_name). We treat them as if they were regular columns.\n",
    "\n",
    "Common Uses:\n",
    "- Perform calculations\n",
    "- Combine text\n",
    "- Transform data\n",
    "\n",
    "Basic syntax:\n",
    "```sql\n",
    "SELECT \n",
    "    original_column,\n",
    "    expression AS new_column\n",
    "FROM table;\n",
    "```\n",
    "\n",
    "Common uses:\n",
    "1. Arithmetic: `price * quantity AS total`\n",
    "2. Text concatenation: `first_name || ' ' || last_name AS full_name`\n",
    "3. Simple calculations: `price * 1.2 AS price_with_tax`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9318d-95a3-4840-be95-394629ea41fd",
   "metadata": {},
   "source": [
    "![Workflow diagram showing SQL string concatenation that creates a ‚Äúsummary‚Äù column from name, price and quantity fields.](../Images/derivedcolumn.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3c085d-8760-429e-b3d0-57645af0b094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Example with Derived Columns: \n",
      "\n",
      "          lower_name   upper_city  double_items\n",
      "0         john smith     NEW YORK           NaN\n",
      "1       maria garcia       LONDON          30.0\n",
      "2             li wei        TOKYO          22.0\n",
      "3         emma brown        PARIS          16.0\n",
      "4       ahmed hassan       SYDNEY          14.0\n",
      "5      sarah johnson       BERLIN          38.0\n",
      "6   carlos rodriguez       MUMBAI          22.0\n",
      "7      anna kowalski    S√£O PAULO          22.0\n",
      "8       james wilson      TORONTO           8.0\n",
      "9        yuki tanaka     SHANGHAI          16.0\n",
      "10       elena popov       MADRID           6.0\n",
      "11     michel dubois       MOSCOW           4.0\n",
      "12      sofia santos        DUBAI          24.0\n",
      "13     lars andersen         None          12.0\n",
      "14       aisha patel  MEXICO CITY           4.0\n",
      "15    diego martinez    AMSTERDAM           2.0\n",
      "16         lucy chen        CAIRO          24.0\n",
      "17       ivan petrov    STOCKHOLM          24.0\n",
      "18     mary williams         None          34.0\n",
      "19         raj kumar  LOS ANGELES          20.0\n",
      "20      hans schmidt         ROME          32.0\n",
      "21    isabella silva    HONG KONG          30.0\n",
      "22    fatima al-said     ISTANBUL           NaN\n",
      "23          jun park        SEOUL          38.0\n",
      "24      anna ivanova      BANGKOK          24.0\n"
     ]
    }
   ],
   "source": [
    "# Derived columns with string functions\n",
    "select_derived_query = \"\"\"\n",
    "SELECT \n",
    "    LOWER(name) AS lower_name,\n",
    "    UPPER(city) AS upper_city,\n",
    "    items_purchased * 2 AS double_items\n",
    "FROM customers\n",
    "\"\"\"\n",
    "print(\"SELECT Example with Derived Columns: \\n\")\n",
    "print(pd.read_sql_query(select_derived_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca7862-2846-4241-b347-0f9b3b1fcd8e",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Using double quotes for string values\n",
    "\n",
    "```sql\n",
    "--‚ùå WRONG: Double quotes for string values\n",
    "WHERE country = \"USA\"\n",
    "\n",
    "-- ‚úÖ CORRECT: Single quotes for strings\n",
    "WHERE country = 'USA'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4b702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Example with Combined Columns: \n",
      "\n",
      "                name    full_location  total_spent\n",
      "0         John Smith     New York, US          NaN\n",
      "1       Maria Garcia       London, GB          NaN\n",
      "2             Li Wei        Tokyo, JP       155.98\n",
      "3         Emma Brown        Paris, FR       517.44\n",
      "4       Ahmed Hassan       Sydney, AU       177.45\n",
      "5      Sarah Johnson       Berlin, DE       301.15\n",
      "6   Carlos Rodriguez       Mumbai, IN      1049.40\n",
      "7      Anna Kowalski    S√£o Paulo, BR      1066.01\n",
      "8       James Wilson      Toronto, CA       331.04\n",
      "9        Yuki Tanaka     Shanghai, CN       299.36\n",
      "10       Elena Popov       Madrid, ES        56.37\n",
      "11     Michel Dubois       Moscow, RU       143.16\n",
      "12      Sofia Santos        Dubai, AE       595.32\n",
      "13     Lars Andersen             None       125.88\n",
      "14       Aisha Patel  Mexico City, MX       109.14\n",
      "15    Diego Martinez    Amsterdam, NL        13.09\n",
      "16         Lucy Chen        Cairo, EG      1102.08\n",
      "17       Ivan Petrov    Stockholm, SE       399.48\n",
      "18     Mary Williams             None      1183.71\n",
      "19         Raj Kumar  Los Angeles, US       380.50\n",
      "20      Hans Schmidt         Rome, IT          NaN\n",
      "21    Isabella Silva    Hong Kong, HK       888.00\n",
      "22    Fatima Al-Said     Istanbul, TR          NaN\n",
      "23          Jun Park        Seoul, KR      1847.94\n",
      "24      Anna Ivanova      Bangkok, TH       957.12\n"
     ]
    }
   ],
   "source": [
    "# Combining columns and calculations\n",
    "select_combined_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    city || ', ' || country AS full_location,\n",
    "    items_purchased * price_per_item AS total_spent\n",
    "FROM customers\n",
    "\"\"\"\n",
    "print(\"SELECT Example with Combined Columns: \\n\")\n",
    "print(pd.read_sql_query(select_combined_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e57b8-710e-4c2d-bb7a-b88a98a4f993",
   "metadata": {},
   "source": [
    "üîî **Question:** Why are we getting NaN's in the last column? How can we avoid this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a76a50-e6e9-47ba-9d27-615b72724c2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>üí° Best Practices</b><br><br>\n",
    "1. Always explicitly list the columns you need <br>\n",
    "2. Use meaningful aliases for clarity<br>\n",
    "3. Format queries with proper indentation<br>\n",
    "4. Add comments for complex queries<br>\n",
    "5. Test queries with a subset of data first\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb407d",
   "metadata": {},
   "source": [
    "### Commonly used Functions\n",
    "\n",
    "We won't be able to cover all of them, but for future reference, here is a quick list of commonly used functions:\n",
    "\n",
    "#### Text functions\n",
    "\n",
    "| Function | Description | Example |\n",
    "|----------|-------------|---------|\n",
    "| `SUBSTR(text, start, len)` | Substring (1‚Äëbased index) | `SUBSTR('Market',1,3)` ‚Üí `Mar` |\n",
    "| `INSTR(text, pattern)` | Position (0 if not found) | `INSTR('abcdef','cd')` ‚Üí `3` |\n",
    "| `LOWER(text)` / `UPPER(text)` | Case conversion | `LOWER('SQL')` ‚Üí `sql` |\n",
    "| `REPLACE(text, old, new)` | Global substitution | `REPLACE('foo','o','0')` |\n",
    "| `TRIM(text)` | Strip leading/trailing spaces | |\n",
    "| `str1 \\|\\| ' merging character ' \\|\\| str2 \\|\\|` | Concatenates strings\n",
    "\n",
    "#### Date / time Functions\n",
    "\n",
    "| Function | What it does | Example |\n",
    "|----------|--------------|---------|\n",
    "| `DATE('now')` | Current date (UTC) | `DATE('now')` ‚Üí `'2025‚Äë05‚Äë07'` |\n",
    "| `DATETIME('now','localtime')` | Current local datetime | `DATETIME('now','localtime')` ‚Üí `'2025‚Äë05‚Äë07¬†13:25:00'` |\n",
    "| `STRFTIME(fmt, ts)` | Format timestamp ‚Üí text | `STRFTIME('%Y‚Äë%m', OrderDate)` ‚Üí `'1997‚Äë07'` |\n",
    "| `JULIANDAY(ts)` | Days since noon¬†4714‚ÄØBC | `JULIANDAY('2025‚Äë05‚Äë07')` ‚Üí `2460457.5` |\n",
    "\n",
    "\n",
    "#### Integer and Float Functions\n",
    "\n",
    "| Expression | Result |\n",
    "|------------|--------|\n",
    "| `column1 + column2` | Add two numbers |\n",
    "| `ROUND(total * 0.15, 2)` | Round to 2‚ÄØdecimal places |\n",
    "| `COALESCE(price, 0)` | Replace NULL with 0 |\n",
    "\n",
    "üí°**Tip:** Need a float? Multiply an `INTEGER` by **`1.0`**.\n",
    "\n",
    "Now that we know the basics of querying, we can start dealing with more complex query commands - such as filtering, grouping, aggregate functions, sorting and paginating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f442956",
   "metadata": {},
   "source": [
    "<a id='where'></a>\n",
    "## 6‚ÄØ¬∑‚ÄØFiltering Rows with `WHERE` \n",
    "\n",
    "Esimated time: 10 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Learn how to filter data using WHERE clauses, including comparison operators, logical operators (AND, OR), and pattern matching with LIKE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8569fc",
   "metadata": {},
   "source": [
    "üîë **Key Concept:** Filtering\n",
    "\n",
    "Filtering is the process of selecting a subset of rows that match a certain condition. Think of it as answering the question \"Which rows do I want to keep?\"\n",
    "\n",
    "Filtering happens before selecting. This is at the crux of our desk-bookshelf analogy - we just want to query the data that we will be needing, so that it is manageable when it gets to our desk.\n",
    "\n",
    "\n",
    "**üìö Real-World Applications**\n",
    "- Finding transactions above a certain amount for audit <br>\n",
    "- Filtering customer data by region for targeted marketing<br>\n",
    "- Identifying high-value products for inventory management<br>\n",
    "- Finding recent orders for shipping prioritization<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd7f88-bdf1-46c7-996f-7d77c238fe22",
   "metadata": {},
   "source": [
    "The basic filtering method is WHERE, and the syntax is as follows:\n",
    "\n",
    "```sql\n",
    "SELECT columns\n",
    "FROM table\n",
    "WHERE condition\n",
    "\n",
    "```\n",
    "\n",
    "As we will see in a bit, the conditions we can impose are very flexible. But, as a first example, let's filter our previous database to those consumers who purchased more than one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0301cf2-8ceb-478a-b6a5-827771ffe791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers who purchased more than one item:\n",
      "                name         city  items_purchased  price_per_item\n",
      "0       Maria Garcia       London             15.0             NaN\n",
      "1             Li Wei        Tokyo             11.0           14.18\n",
      "2         Emma Brown        Paris              8.0           64.68\n",
      "3       Ahmed Hassan       Sydney              7.0           25.35\n",
      "4      Sarah Johnson       Berlin             19.0           15.85\n",
      "5   Carlos Rodriguez       Mumbai             11.0           95.40\n",
      "6      Anna Kowalski    S√£o Paulo             11.0           96.91\n",
      "7       James Wilson      Toronto              4.0           82.76\n",
      "8        Yuki Tanaka     Shanghai              8.0           37.42\n",
      "9        Elena Popov       Madrid              3.0           18.79\n",
      "10     Michel Dubois       Moscow              2.0           71.58\n",
      "11      Sofia Santos        Dubai             12.0           49.61\n",
      "12     Lars Andersen         None              6.0           20.98\n",
      "13       Aisha Patel  Mexico City              2.0           54.57\n",
      "14         Lucy Chen        Cairo             12.0           91.84\n",
      "15       Ivan Petrov    Stockholm             12.0           33.29\n",
      "16     Mary Williams         None             17.0           69.63\n",
      "17         Raj Kumar  Los Angeles             10.0           38.05\n",
      "18      Hans Schmidt         Rome             16.0             NaN\n",
      "19    Isabella Silva    Hong Kong             15.0           59.20\n",
      "20          Jun Park        Seoul             19.0           97.26\n",
      "21      Anna Ivanova      Bangkok             12.0           79.76\n"
     ]
    }
   ],
   "source": [
    "# Customers who purchased more than one item\n",
    "where_query = \"\"\"\n",
    "SELECT name, city, items_purchased, price_per_item\n",
    "FROM customers\n",
    "WHERE items_purchased > 1\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(where_query, conn)\n",
    "print(\"Customers who purchased more than one item:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a89b7-c0a0-4acd-8e6a-f2cd700e18f2",
   "metadata": {},
   "source": [
    "This example actually shows something very interesting about the ```WHERE``` statement - we don't need to keep the column we are using to filter the rows.\n",
    "\n",
    "In other words, let's say that we don't care too much about how much an user actually purchase, but we only want those who spend above a threshold. We can use ```WHERE``` combined with ```SELECT``` to use the information for filtering, but not keep it with us after it has been used, saving a lot of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f58621-700f-4569-b70e-005520d37886",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** List the `name`, `country`, and `age` of all customers **older than 50** who live in **Brazil**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ee135-2d11-434f-a455-c135c89941ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will raise a sqlite3.OperationalError ‚Äî fix it!\n",
    "bad_query = \"\"\"\n",
    "SELECT name country age\n",
    "FROM customers\n",
    "WHERE age > 50\n",
    "  country = 'Brazil';\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eccfd4",
   "metadata": {},
   "source": [
    "### 6.1 Filtering Methods\n",
    "\n",
    "Here are some commonly used filtering methods:\n",
    "\n",
    "1. **Equality / inequality**  \n",
    "   ```sql\n",
    "   ‚Ä¶ WHERE country = 'Germany';\n",
    "   ```\n",
    "‚ö†Ô∏è **Warning:** Be careful with string comparisons - they might be case-sensitive depending on the SQL engine and version you are using.\n",
    "\n",
    "2. **Set membership**  \n",
    "   ```sql\n",
    "   ‚Ä¶ WHERE country IN ('USA','UK','Germany');\n",
    "   ```\n",
    "\n",
    "3. **Comparison**  \n",
    "   ```sql\n",
    "   ‚Ä¶ WHERE freight > 100;\n",
    "   ```\n",
    "\n",
    "üìù **Poll 4:** Which comparison operator would you use to find values in a specific range?\n",
    "\n",
    "4. **NULL checks**  \n",
    "   ```sql\n",
    "   ‚Ä¶ WHERE fax IS NULL;\n",
    "   ```\n",
    "\n",
    "‚ö†Ô∏è **Warning:** When you compare anything to NULL, the result isn't TRUE or FALSE, it is a special third type called UNKNOWN. \n",
    "\n",
    "This feature can be quite confusing, especially in the beginning. A common mistake is to try to find rows with missing values by using \n",
    "\n",
    "'WHERE column = NULL'\n",
    "\n",
    "This leads to the comparison  `NULL` **‚â†** `NULL` , which yields *UNKNOWN*, rather than TRUE.\n",
    "\n",
    "In SQL, we should instead use dedicated functions:\n",
    "\n",
    "```sql\n",
    "‚Ä¶ WHERE fax IS NULL\n",
    "‚Ä¶ WHERE fax IS NOT NULL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ea13a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>‚ö†Ô∏è Other common WHERE mistakes</b><br><br>\n",
    "\n",
    "1. String matching\n",
    "   - ‚ùå Forgetting quotes: `WHERE name = John`\n",
    "   - ‚úÖ Using quotes: `WHERE name = 'John'`\n",
    "\n",
    "2. Date comparisons\n",
    "   - ‚ùå `WHERE date >= '2024-01-01'` AND `date <= '2024-12-31'` \n",
    "   - ‚úÖ `WHERE date >= '2024-01-01'` AND `date < '2025-01-01'`\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b864978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect NULL check (city = NULL):\n",
      "Empty DataFrame\n",
      "Columns: [name, city]\n",
      "Index: []\n",
      "Correct NULL check (city IS NULL):\n",
      "            name  city country\n",
      "0  Lars Andersen  None    None\n",
      "1  Mary Williams  None    None\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating NULL handling with our customers data\n",
    "# First, the incorrect way\n",
    "incorrect_null_query = \"\"\"\n",
    "SELECT name, city\n",
    "FROM customers\n",
    "WHERE city = NULL\n",
    "\"\"\"\n",
    "print(\"Incorrect NULL check (city = NULL):\")\n",
    "print(pd.read_sql_query(incorrect_null_query, conn))\n",
    "\n",
    "# Now the correct way\n",
    "correct_null_query = \"\"\"\n",
    "SELECT name, city, country\n",
    "FROM customers\n",
    "WHERE city IS NULL\n",
    "\"\"\"\n",
    "print(\"Correct NULL check (city IS NULL):\")\n",
    "print(pd.read_sql_query(correct_null_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdf0eb-c0c0-4d25-94a6-cb5634888124",
   "metadata": {},
   "source": [
    "### 6.2 Multiple Conditions\n",
    "We can combine multiple conditions using logical operators:\n",
    "\n",
    "- AND: Both conditions must be true\n",
    "- OR: At least one condition must be true\n",
    "- NOT: Reverses a condition\n",
    "Use parentheses to make the order of operations clear:\n",
    "\n",
    "```sql\n",
    "WHERE (country = 'US' OR country = 'GB')\n",
    "  AND account_balance > 200\n",
    "```\n",
    "\n",
    "\n",
    "üí° **Tip:** You can combine multiple logical expressions using AND/OR/NOT. When doing so, use parentheses to clarify when one logical expressions begins and the other ends.\n",
    "‚ö†Ô∏è **Warning:** Without parentheses, AND takes precedence over OR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c103574",
   "metadata": {},
   "source": [
    "<a id='groupby'></a>\n",
    "## 7‚ÄØ¬∑‚ÄØAggregate Functions & `GROUP BY` \n",
    "\n",
    "Estimated Time: 25 minutes\n",
    "\n",
    "### Learning Objective\n",
    "Master data aggregation and grouping using SQL's aggregate functions (COUNT, SUM, AVG, etc.) and GROUP BY clause to analyze and summarize data effectively.\n",
    "\n",
    "### Real World Application  \n",
    "Given a list of individual transactions, calculate total sales per region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd18089-cd28-4048-b140-d4d8edf4b57f",
   "metadata": {},
   "source": [
    "### 7.1 The Basic Idea\n",
    "\n",
    "Another way of pre-processing data so that the end result is more manageable is to summarize it according to a given statistic.\n",
    "\n",
    "One common example is the use of aggreggate functions, combined with GROUP BY, to collapse many rows into one, while keeping the information contained in them, only now summarized in a single row.\n",
    "\n",
    "First let's understand what the ```GROUP BY``` statement does: it creates subsets of the entire table that are similar in a given way. The most common way of doing so is to pass a column - and then SQL will automatically group the rows according to the values in that column\n",
    "\n",
    "Second, ```GROUP BY``` statements are used in conjunction with aggreggate functions. By grouping rows according to a given column, we can guarantee that the values of these rows match **for that particular column**. But what about the others? They might be different, in which case there is no obvious way of combining them. Aggregate functions do exactly this - they tell SQL what to do with mismatching information inside the group - for example by counting the number of occurrences, summing or taking averages:\n",
    "\n",
    "```sql\n",
    "SELECT country,\n",
    "       COUNT(*)        AS n_orders,\n",
    "       ROUND(AVG(freight),2) AS avg_freight\n",
    "FROM   orders\n",
    "GROUP  BY country\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9ea5a-65e2-443f-9f9d-cdc9aae7b874",
   "metadata": {},
   "source": [
    "![Three-step graphic: raw sales rows, grouped by Department, then totals‚Äîillustrates how GROUP BY collapses data.](../Images/groupby.svg) \n",
    "\n",
    "Let's break it down what is going on with the GROUP BY command.\n",
    "\n",
    "- First, SQL will look into the column indicated on GROUP BY - in this case \"department\"\n",
    "- It will then create buckets given the entries in this column. In other words, for each possible value in this column, it will group the rows based on these values.\n",
    "- For each of these groups, it will run an aggregate function - in this case COUNT(*), which counts how many entries each group has, and SUM(salary), which will sum the column salary across all rows in a given group\n",
    "- It will finally return the values of each group, and of the aggregate functions. Notice that each group only has one row in the resulting column - we have \"collapsed\" the table!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3baf6-d850-43d3-9328-33bf0187b288",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** For each country, list the number of customers (COUNT(*)) and the total items purchased (SUM(items_purchased)).\n",
    "Show only countries with at least 5 customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2e70c-e6e3-4c94-aa94-998b9fcb22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug this intentional error\n",
    "\n",
    "bad_query = \"\"\"\n",
    "SELECT country,\n",
    "       COUNT(*),                       \n",
    "       SUM(items_purchased)\n",
    "FROM customers\n",
    "WHERE COUNT(*) >= 5                    \n",
    "GROUP BY country;\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de76ed3-1bfa-41bc-b853-ae3e47c775df",
   "metadata": {},
   "source": [
    "### 7.2 More Advanced Ideas\n",
    "Another very important thing - which is a bit tough to get used to in the beginning, is that we can only include in the SELECT statement columns that are either used to group by observations, or ones that are used as inputs of aggregate functions. This is exactly because of what we discussed previously - if there is a mismatch between rows, SQL doesn't know how to handle these values when it collapses all the rows in the group into a single one.\n",
    "\n",
    "A bit more advanced, but we can also pass more than one column to the GROUP BY statement - which would then create groups in which rows have the same values for all columns passed.\n",
    "\n",
    "üí° **Tip**: By using GROUP BY, we obtain a collapsed version of the table - we only retain information on the aggregate values. While this is very useful for summarizing information, sometimes we want to keep the detailed data and the summary statistics for more in-depth analysis. This is exactly the problem that Window Functions solve - and something we will be dealing with in the intermediate workshop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd53d24-3eac-469a-b62b-02ce1a4f2d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer statistics by country:\n",
      "\n",
      "   country  customer_count  avg_items  avg_balance  max_balance\n",
      "0       AE               1       12.0       352.84       352.84\n",
      "1       AU               1        7.0       179.64       179.64\n",
      "2       BR               1       11.0       392.80       392.80\n",
      "3       CA               1        4.0       449.81       449.81\n",
      "4       CN               1        8.0       344.21       344.21\n",
      "5       DE               1       19.0          NaN          NaN\n",
      "6       EG               1       12.0       167.10       167.10\n",
      "7       ES               1        3.0       845.86       845.86\n",
      "8       FR               1        8.0       929.69       929.69\n",
      "9       GB               1       15.0       905.34       905.34\n",
      "10      HK               1       15.0       833.92       833.92\n",
      "11      IN               1       11.0       140.70       140.70\n",
      "12      IT               1       16.0       104.97       104.97\n",
      "13      JP               1       11.0       638.11       638.11\n",
      "14      KR               1       19.0       756.11       756.11\n",
      "15      MX               1        2.0       226.83       226.83\n",
      "16      NL               1        1.0       821.98       821.98\n",
      "17      RU               1        2.0       421.08       421.08\n",
      "18      SE               1       12.0       988.20       988.20\n",
      "19      TH               1       12.0       794.14       794.14\n",
      "20      TR               1        NaN       736.17       736.17\n",
      "21      US               2       10.0       945.55       945.55\n"
     ]
    }
   ],
   "source": [
    "# GROUP BY country with aggregations\n",
    "group_by_query = \"\"\"\n",
    "SELECT \n",
    "    country,\n",
    "    COUNT(*) AS customer_count,\n",
    "    ROUND(AVG(items_purchased), 2) AS avg_items,\n",
    "    ROUND(AVG(account_balance), 2) AS avg_balance,\n",
    "    ROUND(MAX(account_balance), 2) AS max_balance\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL\n",
    "GROUP BY country\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(group_by_query, conn)\n",
    "print(\"Customer statistics by country:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d0eb1-1862-4d94-a06a-d2e663b4bb3c",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Using aggregate functions without GROUP BY\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Mixing aggregate and non-aggregate columns\n",
    "SELECT country, COUNT(*), AVG(account_balance)\n",
    "FROM customers\n",
    "\n",
    "-- ‚úÖ CORRECT: Add GROUP BY for non-aggregate columns\n",
    "SELECT country, COUNT(*), AVG(account_balance)\n",
    "FROM customers\n",
    "GROUP BY country\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7be3e-f54d-4edf-aeee-50e8101b7ae0",
   "metadata": {},
   "source": [
    "### 7.3: Summarizing\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Every non-aggregated column in SELECT must be in GROUP BY\n",
    "- GROUP BY comes after WHERE but before ORDER BY\n",
    "- Can group by multiple columns\n",
    "- Can use expressions in GROUP BY \n",
    "\n",
    "For future reference, here is a list of the most commonly used aggregate functions:\n",
    "\n",
    "| Aggregate¬†Function                                   | What it returns (typical usage)                    |\n",
    "| ---------------------------------------------------- | -------------------------------------------------- |\n",
    "| `COUNT(*)`                                           | Total number of rows in the group/query            |\n",
    "| `SUM(col)`                                           | Arithmetic sum of a numeric column                 |\n",
    "| `AVG(col)`                                           | Mean (average) of numeric values                   |\n",
    "| `MAX(col)`                                           | Largest value (numeric *or* lexicographic)         |\n",
    "| `MIN(col)`                                           | Smallest value (numeric *or* lexicographic)        |\n",
    "| `STRING_AGG(col, ', ')` / `GROUP_CONCAT` / `LISTAGG` | Concatenates strings in the group with a delimiter |\n",
    "| `COUNT(DISTINCT col)`                                | Count of unique, non-NULL values                   |\n",
    "\n",
    "üí° **Tip**: Mathematical functions ignore `NULL` in aggregates (`AVG`, `SUM`), which is usually what you want. `COUNT(column)` counts only non‚Äënull values, whereas `COUNT(*)` counts *all* rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b27d-9739-4b0c-99d5-dfd95bcaa12c",
   "metadata": {},
   "source": [
    "### 7.4‚ÄØ¬∑‚ÄØFiltering Groups with `HAVING` \n",
    "\n",
    "Remember that when we discussed filtering, we used the WHERE command, which was run before the ```GROUP BY```.\n",
    "\n",
    "Sometimes, we want to filter rows given an aggregate statement. For example, we might want to choose only the customers whose average expenditure is larger than a certain amount.\n",
    "\n",
    "`HAVING` is evaluated **after** grouping ‚Äì it filters *groups*, whereas `WHERE` filters *rows*.\n",
    "\n",
    "Two Observations:\n",
    "- We cannot use aggregate functions on ```WHERE``` statements\n",
    "- We must use ```HAVING``` after the ```GROUP BY``` statement\n",
    "\n",
    "```sql\n",
    "SELECT country,\n",
    "       COUNT(*) AS n_orders\n",
    "FROM   orders\n",
    "GROUP  BY country\n",
    "HAVING n_orders > 20          -- aggregate in condition\n",
    "ORDER  BY n_orders DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935ebc58-21b3-4fd7-b58a-08c167eb13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with average purchases > 5 and at least 2 customers:\n",
      "Empty DataFrame\n",
      "Columns: [country, customer_count, avg_items_purchased, avg_balance]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Countries with high average purchases using HAVING\n",
    "having_query = \"\"\"\n",
    "SELECT \n",
    "    country,\n",
    "    COUNT(*) AS customer_count,\n",
    "    ROUND(AVG(items_purchased), 2) AS avg_items_purchased,\n",
    "    ROUND(AVG(account_balance), 2) AS avg_balance\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL \n",
    "  AND items_purchased IS NOT NULL\n",
    "GROUP BY country\n",
    "HAVING AVG(items_purchased) > 5 \n",
    "   AND COUNT(*) >= 2\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(having_query, conn)\n",
    "print(\"Countries with average purchases > 5 and at least 2 customers:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116b3ac-d82b-45e0-86f2-f24c9ae7d7b9",
   "metadata": {},
   "source": [
    "![Split-path illustration contrasting filtering rows with WHERE before grouping versus HAVING after aggregation.](../Images/wherehaving.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe9abf-8d57-4507-a1c4-2a98f0378f13",
   "metadata": {},
   "source": [
    "üôã **Hands-Up:** Can you explain‚Äîin one sentence‚Äîwhat `HAVING` does that `WHERE` can‚Äôt? A. Yes‚ÄÉB. Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f02a85",
   "metadata": {},
   "source": [
    "<a id='orderby'></a>\n",
    "## 8‚ÄØ¬∑‚ÄØSorting & Paginating Results \n",
    "\n",
    "Estimate Time: 10 minutes\n",
    "\n",
    "### Real World Application: \n",
    "\n",
    "Find the top 10 selling items on a given year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae469c7c-8110-4642-bd22-b2af888c5648",
   "metadata": {},
   "source": [
    "After we have processed our data, we might want to start preparing it for visualization. In SQL, this is done mostly through sorting - ordering the data according to one or more columns - or paginating - retrieving only a fixed number of observations\n",
    "\n",
    "`ORDER¬†BY` is evaluated *after* `SELECT`.  \n",
    "\n",
    "* Default sort is **ASC** (ascending).  \n",
    "* Use **DESC** for descending order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "810def20-e967-457e-ba60-77cf835b6d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple ORDER BY Example - Top 10 Customers by Items Purchased:\n",
      "\n",
      "                name         city  items_purchased\n",
      "0      Sarah Johnson       Berlin             19.0\n",
      "1           Jun Park        Seoul             19.0\n",
      "2      Mary Williams         None             17.0\n",
      "3       Hans Schmidt         Rome             16.0\n",
      "4       Maria Garcia       London             15.0\n",
      "5     Isabella Silva    Hong Kong             15.0\n",
      "6       Sofia Santos        Dubai             12.0\n",
      "7          Lucy Chen        Cairo             12.0\n",
      "8        Ivan Petrov    Stockholm             12.0\n",
      "9       Anna Ivanova      Bangkok             12.0\n",
      "10            Li Wei        Tokyo             11.0\n",
      "11  Carlos Rodriguez       Mumbai             11.0\n",
      "12     Anna Kowalski    S√£o Paulo             11.0\n",
      "13         Raj Kumar  Los Angeles             10.0\n",
      "14        Emma Brown        Paris              8.0\n",
      "15       Yuki Tanaka     Shanghai              8.0\n",
      "16      Ahmed Hassan       Sydney              7.0\n",
      "17     Lars Andersen         None              6.0\n",
      "18      James Wilson      Toronto              4.0\n",
      "19       Elena Popov       Madrid              3.0\n",
      "20     Michel Dubois       Moscow              2.0\n",
      "21       Aisha Patel  Mexico City              2.0\n",
      "22    Diego Martinez    Amsterdam              1.0\n"
     ]
    }
   ],
   "source": [
    "# Simple ORDER BY example with one column\n",
    "simple_order_query = \"\"\"\n",
    "SELECT \n",
    "   name,\n",
    "   city,\n",
    "   items_purchased\n",
    "FROM customers\n",
    "WHERE items_purchased IS NOT NULL\n",
    "ORDER BY items_purchased DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Simple ORDER BY Example - Top 10 Customers by Items Purchased:\\n\")\n",
    "result = pd.read_sql_query(simple_order_query, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55280696-136d-4238-8b09-f34dc966e44c",
   "metadata": {},
   "source": [
    "You can order by *multiple* columns ‚Äì the second acts as a tie‚Äëbreaker.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "       name,\n",
    "       country,\n",
    "       total_spent,\n",
    "       ROW_NUMBER() OVER (\n",
    "           PARTITION BY country\n",
    "           ORDER BY total_spent DESC,   -- primary\n",
    "                    name               -- secondary tie-breaker\n",
    "       ) AS spend_rank\n",
    "FROM   customer_spending;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a938f7d5-3cef-485b-a9fe-ce65aa41bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORDER BY Example - Sorting by Country then Balance:\n",
      "            name country  account_balance\n",
      "0  Mary Williams    None           795.02\n",
      "1  Lars Andersen    None           588.43\n",
      "2   Sofia Santos      AE           352.84\n",
      "3   Ahmed Hassan      AU           179.64\n",
      "4  Anna Kowalski      BR           392.80\n",
      "5   James Wilson      CA           449.81\n",
      "6    Yuki Tanaka      CN           344.21\n",
      "7      Lucy Chen      EG           167.10\n",
      "8    Elena Popov      ES           845.86\n",
      "9     Emma Brown      FR           929.69\n"
     ]
    }
   ],
   "source": [
    "# Example showing ORDER BY with multiple columns\n",
    "order_by_example = \"\"\"\n",
    "SELECT \n",
    "   name,\n",
    "   country,\n",
    "   account_balance\n",
    "FROM customers\n",
    "WHERE account_balance IS NOT NULL\n",
    "ORDER BY country ASC, account_balance DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"ORDER BY Example - Sorting by Country then Balance:\")\n",
    "result = pd.read_sql_query(order_by_example, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3222d1e",
   "metadata": {},
   "source": [
    "\n",
    "### Pagination Pattern\n",
    "\n",
    "LIMIT is used to restrict how many observations we want to retrieve <br>\n",
    "OFFSET will skill a certain number of rows before displaying the number of results delimited by LIMIT\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT company_name, country, city\n",
    "FROM   customers\n",
    "ORDER  BY country\n",
    "LIMIT  n\n",
    "OFFSET m;\n",
    "```\n",
    "\n",
    "`LIMIT` must appear *before* `OFFSET` in SQLite.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** It is very hard to predict what the ordering will be after applying filtering or other methods. So remember to always ORDER BY before using LIMIT/OFFSET!\n",
    "\n",
    "üí° **Tip**: Interestingly, LIMIT doesn't restrict the data being retrived by SQL, only the data being showed. The difference is crucial to understand when thinking about factors such as speed, memory constraints and computing budgeting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a668312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers ranked 6-10 by account balance:\n",
      "\n",
      "             name country  account_balance\n",
      "0  Isabella Silva      HK           833.92\n",
      "1  Diego Martinez      NL           821.98\n",
      "2   Mary Williams    None           795.02\n",
      "3    Anna Ivanova      TH           794.14\n",
      "4        Jun Park      KR           756.11\n"
     ]
    }
   ],
   "source": [
    "# Sorting and pagination example\n",
    "pagination_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    country,\n",
    "    account_balance\n",
    "FROM customers\n",
    "WHERE account_balance IS NOT NULL\n",
    "ORDER BY account_balance DESC\n",
    "LIMIT 5\n",
    "OFFSET 5;\n",
    "\"\"\"\n",
    "result = pd.read_sql_query(pagination_query, conn)\n",
    "print(\"Customers ranked 6-10 by account balance:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a8432-3f9c-446c-be8a-deb5008cbffc",
   "metadata": {},
   "source": [
    "üôã **Hands-Up:** Which clause actually *sorts* the result set? A. `SELECT`‚ÄÉB. `ORDER BY`‚ÄÉC. `LIMIT`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c7fb5-1e02-4c70-8433-49b3b780e080",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Wrong clause order\n",
    "\n",
    "-- ‚ùå WRONG: ORDER BY must come before LIMIT\n",
    "```sql\n",
    "SELECT * FROM customers\n",
    "LIMIT 10\n",
    "ORDER BY account_balance DESC \n",
    "```\n",
    "\n",
    "-- ‚úÖ CORRECT: Proper SQL clause order\n",
    "```sql\n",
    "SELECT * FROM customers\n",
    "ORDER BY account_balance DESC\n",
    "LIMIT 10 ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094a318-876c-4b39-b2e1-10c568067e98",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We learned quite a few different commands for queries. Let's see one example that includes all of them:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    Country,\n",
    "    COUNT(OrderID)                AS total_orders,\n",
    "    ROUND(AVG(Freight), 2)        AS avg_freight\n",
    "FROM    customers \n",
    "WHERE   Country IN ('USA','UK','Germany')\n",
    "GROUP BY Country\n",
    "HAVING   COUNT(OrderID) >= 10\n",
    "ORDER BY total_orders DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "The diagram reiterates the **logical query order**, not the command order, helping you remember the order in which the operations are actually made.\n",
    "\n",
    "![Horizontal flowchart listing SQL clause execution order: FROM ‚Üí WHERE ‚Üí GROUP BY ‚Üí HAVING ‚Üí SELECT ‚Üí ORDER BY ‚Üí LIMIT](../Images/sql-execution-order.svg)\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8bb1-5c3c-40ae-9538-84a635a5febc",
   "metadata": {},
   "source": [
    "### A visualization of the order of query operations\n",
    "\n",
    "Let's go through an example of how the order of query operations look like in practice. \n",
    "\n",
    "**FROM¬†`customers`** ‚Äì full table (7¬†rows).\n",
    "\n",
    "| CustID | Country | Orders |\n",
    "|-------|---------|--------|\n",
    "| C1 | USA | 5 |\n",
    "| C2 | USA | 7 |\n",
    "| C3 | UK  | 3 |\n",
    "| C4 | UK  | 7 |\n",
    "| C5 | FRA | 15 |\n",
    "| C6 | GER | 2 |\n",
    "| C7 | CAN | 6 |\n",
    "\n",
    "---\n",
    "\n",
    "**WHERE¬†`Country IN ('USA','UK','FRA','GER')`** ‚Äì drop the Canadian row.\n",
    "\n",
    "| CustID | Country | Orders |\n",
    "|-------|---------|--------|\n",
    "| C1 | USA | 5 |\n",
    "| C2 | USA | 7 |\n",
    "| C3 | UK  | 3 |\n",
    "| C4 | UK  | 7 |\n",
    "| C5 | FRA | 15 |\n",
    "| C6 | GER | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "**GROUP¬†BY¬†`Country`** ‚Äì aggregate rows, summing `Orders` - Drops CustID!\n",
    "\n",
    "| Country | TotalOrders |\n",
    "|---------|-------------|\n",
    "| USA | 12 |\n",
    "| UK  | 10 |\n",
    "| FRA | 15 |\n",
    "| GER | 2  |\n",
    "\n",
    "---\n",
    "\n",
    "**HAVING¬†`TotalOrders > 5`** ‚Äì keep only groups with large order volume; Germany drops out.\n",
    "\n",
    "| Country | TotalOrders |\n",
    "|---------|-------------|\n",
    "| FRA | 15 |\n",
    "| USA | 12 |\n",
    "| UK  | 10 |\n",
    "\n",
    "---\n",
    "\n",
    "**SELECT¬†`Country, TotalOrders`** ‚Äì project just the columns we care about (already those two).\n",
    "\n",
    "| Country | TotalOrders |\n",
    "|---------|-------------|\n",
    "| FRA | 15 |\n",
    "| USA | 12 |\n",
    "| UK  | 10 |\n",
    "\n",
    "---\n",
    "\n",
    "**ORDER¬†BY¬†`TotalOrders DESC`** ‚Äì rank countries by order volume.\n",
    "\n",
    "| Country | TotalOrders |\n",
    "|---------|-------------|\n",
    "| FRA | 15 |\n",
    "| USA | 12 |\n",
    "| UK  | 10 |\n",
    "\n",
    "---\n",
    "\n",
    "**LIMIT¬†2** ‚Äì return only the top two performers.\n",
    "\n",
    "| Country | TotalOrders |\n",
    "|---------|-------------|\n",
    "| FRA | 15 |\n",
    "| USA | 12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f5fa8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<a id='keypoints'></a>\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "## 11‚ÄØ¬∑‚ÄØKey Points & Next Steps \n",
    "\n",
    "Estimate Time: 2 minutes\n",
    "\n",
    "* Use SQL to select and pre-process only the data you really need, then use this smaller dataset for analysis with pandas\n",
    "* SQLite is a zero‚Äëinstall, single‚Äëfile engine that still speaks standard SQL.  \n",
    "* Remember the logical query order to avoid confusion (`WHERE` vs `HAVING`).  \n",
    "\n",
    "### What‚Äôs Next?  \n",
    "In the **Intermediate SQL** workshop we will tackle:\n",
    "\n",
    "* Creating & altering tables  \n",
    "* `JOIN`s (```INNER```, ```LEFT```, ```RIGHT```, ```FULL```) and set operations\n",
    "* `JOIN` as selection  \n",
    "* Subqueries & Common Table Expressions (`WITH`)  \n",
    "* Window functions (`ROW_NUMBER`, `LAG`, `LEAD`)  \n",
    "* ```UNION```\n",
    "* Pivoting\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d82f06-d014-4671-8958-fd59bd9a6d68",
   "metadata": {},
   "source": [
    "## üé¨ Demo ‚Äî Customer Spending vs. Income Ranking  \n",
    "\n",
    "üí° **Goal**  \n",
    "Show how window functions, CTEs, and JOINs can be combined to answer a practical business question:\n",
    "\n",
    "> *‚ÄúHow much does each customer spend relative to the income they report?‚Äù*\n",
    "\n",
    "### What the query does\n",
    "1. **`customer_spending` CTE**  \n",
    "   *Calculates* each customer‚Äôs total spending (`items_purchased √ó price_per_item`).\n",
    "\n",
    "2. **`income_ranking` CTE**  \n",
    "   Aggregates income **and** ranks customers by `SUM(amount)` using  \n",
    "   `RANK() OVER (ORDER BY total_income DESC)`.\n",
    "\n",
    "3. **Main SELECT**  \n",
    "   *Joins* the two CTEs on `name` and computes  \n",
    "   **`spending_pct_of_income`** = spending √∑ income √ó 100.\n",
    "\n",
    "Run the cell below and inspect the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd6e6ee-1726-4d00-b46a-d57be792b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ DEMO: Customer Spending Analysis with Income Ranking\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>total_income</th>\n",
       "      <th>income_rank</th>\n",
       "      <th>spending_pct_of_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>177.45</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>125.88</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>299.36</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>13.09</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>56.37</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>1183.71</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>13</td>\n",
       "      <td>98.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>301.15</td>\n",
       "      <td>400.0</td>\n",
       "      <td>14</td>\n",
       "      <td>75.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>155.98</td>\n",
       "      <td>200.0</td>\n",
       "      <td>15</td>\n",
       "      <td>77.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  total_spent  total_income  income_rank  \\\n",
       "0    Ahmed Hassan       177.45        5800.0            1   \n",
       "1   Lars Andersen       125.88        4500.0            2   \n",
       "2     Yuki Tanaka       299.36        3200.0            8   \n",
       "3  Diego Martinez        13.09        2700.0           10   \n",
       "4     Elena Popov        56.37        1800.0           12   \n",
       "5   Mary Williams      1183.71        1200.0           13   \n",
       "6   Sarah Johnson       301.15         400.0           14   \n",
       "7          Li Wei       155.98         200.0           15   \n",
       "\n",
       "   spending_pct_of_income  \n",
       "0                    3.06  \n",
       "1                    2.80  \n",
       "2                    9.36  \n",
       "3                    0.48  \n",
       "4                    3.13  \n",
       "5                   98.64  \n",
       "6                   75.29  \n",
       "7                   77.99  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEMO: Combining Window Functions with JOIN\n",
    "income_df = pd.read_sql_query('SELECT * FROM income', conn)\n",
    "\n",
    "demo_query = \"\"\"\n",
    "WITH customer_spending AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        items_purchased * price_per_item AS total_spent\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL \n",
    "      AND price_per_item IS NOT NULL\n",
    "),\n",
    "income_ranking AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(amount) AS total_income,\n",
    "        RANK() OVER (ORDER BY SUM(amount) DESC) AS income_rank\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    ")\n",
    "SELECT \n",
    "    cs.name,\n",
    "    cs.total_spent,\n",
    "    ir.total_income,\n",
    "    ir.income_rank,\n",
    "    CASE \n",
    "        WHEN ir.total_income > 0 \n",
    "        THEN ROUND(cs.total_spent * 100.0 / ir.total_income, 2)\n",
    "        ELSE NULL \n",
    "    END AS spending_pct_of_income\n",
    "FROM customer_spending cs\n",
    "JOIN income_ranking ir ON cs.name = ir.name\n",
    "ORDER BY ir.income_rank;\n",
    "\"\"\"\n",
    "\n",
    "print(\"üé¨ DEMO: Customer Spending Analysis with Income Ranking\\n\")\n",
    "display(pd.read_sql_query(demo_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e48856-15c7-4e1b-8c6c-823fca1f9880",
   "metadata": {},
   "source": [
    "üôã **Hands-Up:** How was that exercise? A. Easy‚ÄÉB. Okay‚ÄÉC. Tough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b5244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "---\n",
    "# Workshop‚ÄØ2 ‚Äì SQL for Data Analysis (Part‚ÄØ2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a0aee",
   "metadata": {},
   "source": [
    "# SQL for Data Analysis  \n",
    "## Advanced Workshop\n",
    "*D‚ÄëLab, UC¬†Berkeley*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7ed73-be41-41e1-a27b-75904a6b04eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Prerequisites for SQL Advanced Workshop</b><br><br>\n",
    "Completion of \"SQL for Data Analysis: Introductory Workshop\" or equivalent experience:\n",
    "<ul>\n",
    "<li>Understanding of basic SQL syntax including SELECT, FROM, WHERE, GROUP BY</li>\n",
    "<li>Familiarity with basic data filtering and sorting in SQL</li>\n",
    "<li>Experience with simple aggregations (COUNT, SUM, AVG)</li>\n",
    "</ul>\n",
    "    \n",
    "These prerequisites ensure participants have the foundational knowledge needed to succeed in learning the more advanced concepts like JOINs, subqueries, CTEs, and window functions covered in the second workshop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528f6e4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Relational Joins](#joins)\n",
    "3. [Subqueries](#subqueries)\n",
    "4. [Common Table Expressions (CTEs)](#ctes)\n",
    "5. [Pivoting and Unpivoting](#pivot)\n",
    "6. [Window Functions](#window)\n",
    "7. [Key Points](#keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5e214-796a-48e2-8af6-5f8e39ddec79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<b>Learning Goals</b><br><br>\n",
    "By the end of this workshop you will be able to:\n",
    "<ul>\n",
    "<li>Combine data from multiple tables using different types of JOINs (INNER, LEFT, SELF).</li>\n",
    "<li>Understand the role of primary and foreign keys in establishing table relationships.</li>\n",
    "<li>Write and use subqueries to break down complex queries with multiple logical steps.</li>\n",
    "<li>Simplify complex queries using Common Table Expressions (CTEs).</li>\n",
    "<li>Transform data between row and column orientations with pivoting and unpivoting techniques.</li>\n",
    "<li>Apply window functions to perform calculations across specified sets of rows.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eb5e2e4-0c1e-48c0-9fef-029bcf52fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some packages that we will need:\n",
    "\n",
    "from IPython.display import SVG\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad7acb",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1 ¬∑ Introduction \n",
    "\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "In the last workshop we covered many different operations one can perform in a given table. However, going back to the idea of Relational Databases, we often won't have all of the information we need on a single table, and we will need to find a way of cross-referencing the information from two (or more) different tables together. This processing is called \"Joining\", and it is an essential aspect of querying with SQL. \n",
    "\n",
    "Another common procedure is to first modify the data, and then perform some operations in the modified data. This is what Subqueries are used for - they allow us to quickly reference a modified version of the dataset we are querying, or compare information between two tables without joining them. Common Table Expressions are a way of doing many subqueries simultaneously, or very complex subqueries, while keeping things readable and organized.\n",
    "\n",
    "Lastly, we will cover a class of operations called Window Functions, which are some of the most powerful tools in SQL. Instead of performing the same function to all rows, they allow the functions to only be applied to *windows*, which are a subset of the observations that are related in a prespecified way. This allows us to calculate summary statistics similar to what we did using ` GROUP BY `, but still retain the original disaggregated information.\n",
    "\n",
    "üìù **Poll 1:** How confident do you feel about everything we covered in Part 1 (basic SELECT / JOIN / GROUP BY)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b63d8",
   "metadata": {},
   "source": [
    "<a id='joins'></a>\n",
    "## 2 ¬∑ Relational¬†Joins \n",
    "\n",
    "Estimated Time: 35 minutes\n",
    "\n",
    "\n",
    "**Purpose:** ```JOINs``` allow us to combine information from multiple different tables into one. When used with Querying, this also provides a way of retaining only the information required for a particular analysis into a single, organized table, even if originally this information was spread out  \n",
    "\n",
    "**Example:** Say that we have two tables, one recording the information about Managers, and another one with the information about Employees. Each employee produces a certain amount of revenue to the firm, but a higher-up is interested in understanding what Manager had direct employees that produce the most revenue. \n",
    "\n",
    "If we had a table that listed each manager, their respective employees and how much they earned, we could use ```SUM``` + ```GROUP BY``` to quickly do this analysis. But we don't - the information is stored in two separate tables. This is exactly the context of `JOIN`s - creating a new table combining the information from two (or more) other tables. Here is a quick diagram to understand what is happening:\n",
    "\n",
    "![Entity Relationship diagram linking Employees, Departments and Managers tables with Primary and Foreign Key arrows to show one-to-many relationships.](../Images/database-relationship-diagram2.svg)\n",
    "---\n",
    "\n",
    "Notice that in this example, employee_id would be the *primary key* of each table - a unique identifier for each row - while the column \"report_to\" on the Employees table serves as a *foreign key* - it serves as a reference to values on the table Managers which can be used to cross-reference information.\n",
    "\n",
    "Notice also that we didn't keep all of the information from each table in the end result. By querying the table after joining, we can decide what to retain, and even perform operations or filtering directly! This again illustrates the principle we laid out at the beginning - SQL is a tool capable of preparing large amounts of data to be analyzed with libraries such as pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9266c29-4d4f-442d-8eaf-86ecd62d131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables available in database:\n",
      "\n",
      "['customers', 'income', 'demo_customers', 'demo_orders']\n",
      "\n",
      " JOIN Example - Customers with Income: \n",
      "\n",
      "    customer_name       city  total_spent income_source  income_amount\n",
      "0    Ahmed Hassan     Sydney       177.45    Consulting         1500.0\n",
      "1    Ahmed Hassan     Sydney       177.45        Rental          800.0\n",
      "2    Ahmed Hassan     Sydney       177.45        Salary         3500.0\n",
      "3  Diego Martinez  Amsterdam        13.09        Salary         2700.0\n",
      "4     Elena Popov     Madrid        56.37       Pension         1800.0\n",
      "5   Lars Andersen       None       125.88        Salary         4500.0\n",
      "6          Li Wei      Tokyo       155.98    Investment          200.0\n",
      "7          Li Wei      Tokyo       155.98        Salary            NaN\n",
      "8   Mary Williams       None      1183.71     Part-time         1200.0\n",
      "9   Sarah Johnson     Berlin       301.15        Salary            NaN\n"
     ]
    }
   ],
   "source": [
    "# Using the unified dataset for JOIN examples\n",
    "# First, ensure we have both customers and income tables loaded\n",
    "print(\"Tables available in database:\\n\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print([table[0] for table in cursor.fetchall()])\n",
    "\n",
    "# Basic JOIN example - customers with their income\n",
    "join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name AS customer_name,\n",
    "    c.city,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    i.income_source,\n",
    "    i.amount AS income_amount\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    income i ON c.name = i.name\n",
    "WHERE \n",
    "    c.items_purchased IS NOT NULL \n",
    "    AND c.price_per_item IS NOT NULL\n",
    "ORDER BY \n",
    "    c.name, i.income_source\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n JOIN Example - Customers with Income: \\n\")\n",
    "result = pd.read_sql_query(join_query, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c194c-2a3c-4b7d-a453-95fe9a552a71",
   "metadata": {},
   "source": [
    "Let's Break down exactly what is happening as we join tables using the example below::\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    m.name AS \"Manager Name\",\n",
    "    e.name AS \"Employee Name\",\n",
    "    e.revenue AS \"Revenue\"\n",
    "FROM \n",
    "    Managers m \n",
    "JOIN \n",
    "    Employees e \n",
    "ON \n",
    "    m.employee_id = e.reports_to\n",
    "ORDER BY\n",
    "    m.name, e.name\n",
    "```\n",
    "\n",
    "Let's start from the inside out:\n",
    "- The first operation being performed is \"FROM\". This is telling us which Table we will be using as our \"main\" table - which in SQL is usually referred to as the Left Table. It is useful to think of any JOIN operation as acting \"to the right\" of this table.\n",
    "    - Notice that we include a \"m\" right next to the name of the table we are importing - this is what we call an alias. While this is not necessary for a simple JOIN statement like this, it help keep the query clean and organized. And, as we will see later on, it is necessary for more advanced JOIN statements (such as self-joins).\n",
    "- The second operation being performed is \"JOIN\". This is essentially telling SQL to consider a table that is a combination of the Left and Right tables.\n",
    "    - One very important aspect is that we must tell SQL how to merge these two tables - which is what we do by using the ON statement.\n",
    "    - In more advanced queries, ON statements can use very complex conditions, including compound ones using logical operators such as AND/OR/NOT, but for now we are just saying \"I want a row that combines the information from the two tables whenever the column \"employeed_id\" on the Manager table matches the \"reports_to\" column on the Employees table.\n",
    "    - Notice that we had to specify which table each column came from here - and we can already see why aliases can come in handy.\n",
    "    - A very important thing to notice is that, if there are multiple matches to the condition on the ON statement, any rows that satisfy it will be joined. This is what happened in our example - there are two rows in the Merged table that have Bob as the Manager Name, since he had two employees that reported to him.\n",
    "    - Another important thing to know is that, in a basic JOIN statement, any rows that go unmatched are not included in the final result. We will see how more advanced JOIN statements - such as LEFT JOIN - allow us to bypass this.\n",
    "- We then go to our SELECT statement. As before, this is just telling SQL which information we actually want it to retrieve for us. But, since the information now might come from more than one source, we again need to specify from which table each column is coming from.\n",
    "- The rest of the statements is similar to a basic Query - we can filter, order, limit/offset, etc - just like we did in the previous workshop\n",
    "\n",
    "\n",
    "![Flow diagram with FROM, JOIN, WHERE, SELECT and ORDER BY blocks connected by arrows to show query processing order.](../Images/sql-query-flow.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac684b1-e579-4026-9f06-aaa1f3802080",
   "metadata": {
    "language": "sql",
    "scrolled": true
   },
   "source": [
    "üèãÔ∏è‚Äç‚ôÇÔ∏è **Challenge:** Your First Real JOIN \n",
    "\n",
    "Individuall, the tables \"customers\" tells us who our clients are and \"orders\" tells us what they bought and when.\n",
    "\n",
    "Task: Using a single JOIN, produce a table that answers\n",
    "‚ÄúWhich orders were placed by which customer, in what country?‚Äù\n",
    "\n",
    "```sql\n",
    "-- Fill in the JOIN condition\n",
    "SELECT\n",
    "       o.order_id,\n",
    "       o.order_date,\n",
    "       c.name,\n",
    "       c.country\n",
    "FROM   orders    AS o\n",
    "JOIN   customers AS c\n",
    "       ON  _____________          \n",
    "ORDER  BY o.order_date;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b1c6d-555b-4fa2-a179-64c9463f4655",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Always make sure that your joining condition is present and accurate. Otherwise SQL will perform what we call a Cartesian Product - making every possible combination between the rows - which can become huge really fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adf4c70-2b4d-4565-9fe6-00e1a2e5e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartesian Product Demonstration:\n",
      "\n",
      "Number of customers: 25\n",
      "Number of income records: 22\n",
      "Cartesian product rows (WRONG): 550 (25 √ó 22 = every possible combination) \n",
      "\n",
      "Correct JOIN rows: 16 (only matching names)\n",
      "\n",
      "         name      city income_source  amount\n",
      "0  John Smith  New York        Salary  3000.0\n",
      "1  John Smith  New York     Freelance   500.0\n",
      "2  John Smith  New York        Salary  2500.0\n",
      "3  John Smith  New York         Bonus  1000.0\n",
      "4  John Smith  New York        Salary     NaN\n",
      "5  John Smith  New York    Investment   200.0\n",
      "6  John Smith  New York        Salary  3500.0\n",
      "7  John Smith  New York    Consulting  1500.0\n",
      "8  John Smith  New York        Rental   800.0\n",
      "9  John Smith  New York        Salary     NaN\n"
     ]
    }
   ],
   "source": [
    "# Example showing cartesian product vs correct join\n",
    "# Using our existing customers and income tables\n",
    "\n",
    "# Cartesian product (WRONG - no join condition)\n",
    "cartesian_query = \"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM customers, income;\n",
    "\"\"\"\n",
    "cartesian_result = pd.read_sql_query(cartesian_query, conn)\n",
    "\n",
    "# Correct join\n",
    "correct_join_query = \"\"\"\n",
    "SELECT COUNT(*) as row_count\n",
    "FROM customers c\n",
    "JOIN income i ON c.name = i.name;\n",
    "\"\"\"\n",
    "correct_result = pd.read_sql_query(correct_join_query, conn)\n",
    "\n",
    "# Get actual counts\n",
    "customer_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM customers\", conn).iloc[0,0]\n",
    "income_count = pd.read_sql_query(\"SELECT COUNT(*) as count FROM income\", conn).iloc[0,0]\n",
    "\n",
    "print(\"Cartesian Product Demonstration:\\n\")\n",
    "print(f\"Number of customers: {customer_count}\")\n",
    "print(f\"Number of income records: {income_count}\")\n",
    "print(f\"Cartesian product rows (WRONG): {cartesian_result.iloc[0,0]} ({customer_count} √ó {income_count} = every possible combination) \\n\")\n",
    "print(f\"Correct JOIN rows: {correct_result.iloc[0,0]} (only matching names)\\n\")\n",
    "\n",
    "# Show a sample of what cartesian product looks like\n",
    "cartesian_detail = \"\"\"\n",
    "SELECT c.name, c.city, i.income_source, i.amount\n",
    "FROM customers c, income i\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(cartesian_detail, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29c624-9513-47cc-ab73-84786761110e",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Always verify your JOIN produces the expected number of rows! A missing or incorrect JOIN condition can create a Cartesian product, multiplying your data exponentially. For example, joining two 1,000-row tables without proper conditions creates 1,000,000 rows! Always check row counts after JOINs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b9dd7",
   "metadata": {},
   "source": [
    "## 2.1¬†`ON` versus `USING`\n",
    "While the ON clause is extremely flexible, if the two tables share a column, we can use the simpler USING() method.\n",
    "\n",
    "‚ö†Ô∏è  **Warning:** When using \"USING\" to JOIN tables, always make sure that the names are identical in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d969b9",
   "metadata": {
    "language": "sql",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOIN with USING clause:\n",
      "\n",
      "             name       city  account_balance  total_income\n",
      "0    Ahmed Hassan     Sydney           179.64        5800.0\n",
      "1   Lars Andersen       None           588.43        4500.0\n",
      "2      John Smith   New York           945.55        3500.0\n",
      "3    Maria Garcia     London           905.34        3500.0\n",
      "4     Yuki Tanaka   Shanghai           344.21        3200.0\n",
      "5  Diego Martinez  Amsterdam           821.98        2700.0\n",
      "6     Elena Popov     Madrid           845.86        1800.0\n",
      "7   Mary Williams       None           795.02        1200.0\n",
      "8          Li Wei      Tokyo           638.11         200.0\n",
      "\n",
      " Same query with ON clause (identical results):\n",
      "\n",
      "             name       city  account_balance  total_income\n",
      "0    Ahmed Hassan     Sydney           179.64        5800.0\n",
      "1   Lars Andersen       None           588.43        4500.0\n",
      "2      John Smith   New York           945.55        3500.0\n",
      "3    Maria Garcia     London           905.34        3500.0\n",
      "4     Yuki Tanaka   Shanghai           344.21        3200.0\n",
      "5  Diego Martinez  Amsterdam           821.98        2700.0\n",
      "6     Elena Popov     Madrid           845.86        1800.0\n",
      "7   Mary Williams       None           795.02        1200.0\n",
      "8          Li Wei      Tokyo           638.11         200.0\n"
     ]
    }
   ],
   "source": [
    "# USING clause with a subquery that creates matching column names\n",
    "# Create a subquery that has a 'name' column matching customers table\n",
    "using_join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance,\n",
    "    i.total_income\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    (SELECT name, SUM(amount) AS total_income \n",
    "     FROM income \n",
    "     GROUP BY name) i\n",
    "USING (name)\n",
    "WHERE \n",
    "    c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"JOIN with USING clause:\\n\")\n",
    "print(pd.read_sql_query(using_join_query, conn))\n",
    "\n",
    "# Compare with ON clause - same results, different syntax\n",
    "on_join_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance,\n",
    "    i.total_income\n",
    "FROM \n",
    "    customers c\n",
    "JOIN \n",
    "    (SELECT name, SUM(amount) AS total_income \n",
    "     FROM income \n",
    "     GROUP BY name) i\n",
    "ON c.name = i.name\n",
    "WHERE \n",
    "    c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Same query with ON clause (identical results):\\n\")\n",
    "print(pd.read_sql_query(on_join_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d8dfb-2270-4252-905b-ebe641481b1d",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Ambiguous column names in JOINs\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Unclear which table's 'name' column\n",
    "SELECT name, amount\n",
    "FROM customers\n",
    "JOIN income ON customers.name = income.name\n",
    "\n",
    "-- ‚úÖ CORRECT: Qualify columns with table names or aliases\n",
    "SELECT c.name, i.amount\n",
    "FROM customers c\n",
    "JOIN income i ON c.name = i.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d6294-f9ec-4cef-929f-23c10ec5f100",
   "metadata": {},
   "source": [
    "But is important to be careful - using a non-primary key column can lead to issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "158883ad-c959-4172-a097-ba763b526d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problematic USING() JOIN Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>177.45</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>13.09</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>56.37</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>125.88</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>155.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>155.98</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>301.15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>301.15</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>299.36</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name       city  total_spent  amount\n",
       "0     Ahmed Hassan     Sydney       177.45   800.0\n",
       "1     Ahmed Hassan     Sydney       177.45  1500.0\n",
       "2     Ahmed Hassan     Sydney       177.45  3500.0\n",
       "3   Diego Martinez  Amsterdam        13.09  2700.0\n",
       "4      Elena Popov     Madrid        56.37  1800.0\n",
       "5       John Smith   New York          NaN   500.0\n",
       "6       John Smith   New York          NaN  3000.0\n",
       "7    Lars Andersen       None       125.88  4500.0\n",
       "8           Li Wei      Tokyo       155.98     NaN\n",
       "9           Li Wei      Tokyo       155.98   200.0\n",
       "10    Maria Garcia     London          NaN  1000.0\n",
       "11    Maria Garcia     London          NaN  2500.0\n",
       "12   Mary Williams       None      1183.71  1200.0\n",
       "13   Sarah Johnson     Berlin       301.15     NaN\n",
       "14   Sarah Johnson     Berlin       301.15   400.0\n",
       "15     Yuki Tanaka   Shanghai       299.36  3200.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "problematic_using_join = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    i.amount\n",
    "FROM   customers AS c\n",
    "INNER JOIN income AS i            -- multiple income rows per customer\n",
    "USING  (name)                     -- alias *i* is required for i.amount\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    \"\\nProblematic USING() JOIN Results:\"\n",
    ")\n",
    "display(pd.read_sql_query(problematic_using_join, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142be246-ebcc-43ea-bc9d-1e2a163ad391",
   "metadata": {},
   "source": [
    "üîî **Question:** Can anyone figure out what went wrong here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ed1ad-fb26-4ec4-aadf-df51ef64784a",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** Join customers with income to list each customer‚Äôs name, country, and total income (sum of amount) sorted by total income descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a0926c-6e13-4a46-8570-bcb639a37be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT name, country, SUM(amount) AS total_income\nFROM customers\nJOIN income\n  ON name = name          -- ‚ùå ambiguous columns!\nGROUP BY name, country\nORDER BY total_income DESC;\n': ambiguous column name: name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: ambiguous column name: name",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debug this intentional error\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bad_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT name, country, SUM(amount) AS total_income\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM customers\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mORDER BY total_income DESC;\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_sql_query(bad_query, conn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    527\u001b[0m         sql,\n\u001b[0;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    535\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT name, country, SUM(amount) AS total_income\nFROM customers\nJOIN income\n  ON name = name          -- ‚ùå ambiguous columns!\nGROUP BY name, country\nORDER BY total_income DESC;\n': ambiguous column name: name"
     ]
    }
   ],
   "source": [
    "# Debug this intentional error\n",
    "\n",
    "bad_query = \"\"\"\n",
    "SELECT name, country, SUM(amount) AS total_income\n",
    "FROM customers\n",
    "JOIN income\n",
    "  ON name = name          -- ‚ùå ambiguous columns!\n",
    "GROUP BY name, country\n",
    "ORDER BY total_income DESC;\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf25be",
   "metadata": {},
   "source": [
    "## 2.2¬†Multiple¬†Joins - Advanced Topic, Time Permitting üï§\n",
    "We don't have to stop at two - we can use multiple joins at once. And what is interesting is that we can get pretty creative with the ON conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615bd16e",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-join demonstration ‚Äì customers plus derived totals:\n",
      "\n",
      "                name country  total_spent  total_income  avg_balance\n",
      "0       Ahmed Hassan      AU       177.45        5800.0       179.64\n",
      "1      Lars Andersen    None       125.88        4500.0          NaN\n",
      "2         John Smith      US          NaN        3500.0       945.55\n",
      "3       Maria Garcia      GB          NaN        3500.0       905.34\n",
      "4        Yuki Tanaka      CN       299.36        3200.0       344.21\n",
      "5     Diego Martinez      NL        13.09        2700.0       821.98\n",
      "6        Elena Popov      ES        56.37        1800.0       845.86\n",
      "7      Mary Williams    None      1183.71        1200.0          NaN\n",
      "8      Sarah Johnson      DE       301.15         400.0          NaN\n",
      "9             Li Wei      JP       155.98         200.0       638.11\n",
      "10        Emma Brown      FR       517.44           NaN       929.69\n",
      "11  Carlos Rodriguez      IN      1049.40           NaN       140.70\n",
      "12     Anna Kowalski      BR      1066.01           NaN       392.80\n",
      "13      James Wilson      CA       331.04           NaN       449.81\n",
      "14     Michel Dubois      RU       143.16           NaN       421.08\n",
      "15      Sofia Santos      AE       595.32           NaN       352.84\n",
      "16       Aisha Patel      MX       109.14           NaN       226.83\n",
      "17         Lucy Chen      EG      1102.08           NaN       167.10\n",
      "18       Ivan Petrov      SE       399.48           NaN       988.20\n",
      "19         Raj Kumar      US       380.50           NaN       945.55\n",
      "20      Hans Schmidt      IT          NaN           NaN       104.97\n",
      "21    Isabella Silva      HK       888.00           NaN       833.92\n",
      "22    Fatima Al-Said      TR          NaN           NaN       736.17\n",
      "23          Jun Park      KR      1847.94           NaN       756.11\n",
      "24      Anna Ivanova      TH       957.12           NaN       794.14\n"
     ]
    }
   ],
   "source": [
    "multi_join_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "\n",
    "    /* total income per customer, pre-aggregated in an inline view */\n",
    "    it.total_income,\n",
    "\n",
    "    /* country-level average balance for extra JOIN practice */\n",
    "    cs.avg_balance\n",
    "\n",
    "FROM   customers AS c\n",
    "\n",
    "/* 1) Join to per-customer income totals */\n",
    "LEFT JOIN (\n",
    "    SELECT name,\n",
    "           SUM(amount) AS total_income\n",
    "    FROM   income\n",
    "    GROUP  BY name\n",
    ") AS it\n",
    "  ON c.name = it.name\n",
    "\n",
    "/* 2) Join to per-country balance stats */\n",
    "LEFT JOIN (\n",
    "    SELECT country,\n",
    "           AVG(account_balance) AS avg_balance\n",
    "    FROM   customers\n",
    "    WHERE  account_balance IS NOT NULL\n",
    "    GROUP  BY country\n",
    ") AS cs\n",
    "  ON c.country = cs.country\n",
    "\n",
    "ORDER BY it.total_income DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nMulti-join demonstration ‚Äì customers plus derived totals:\\n\")\n",
    "print(pd.read_sql_query(multi_join_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d742f-354f-44d7-acd0-4b99a82654e4",
   "metadata": {},
   "source": [
    "üîî **Question:** In the results above, notice how some customers have NULL values for `total_income` while others have NULL for `total_spent`. Why is this happening?\n",
    "\n",
    "This demonstrates the difference between ```INNER JOIN``` and ```LEFT JOIN```.\n",
    "\n",
    "If we had used ```INNER JOIN``` for both joins, we would only see customers who have entries in ALL three tables. But the query above uses ```LEFT JOIN```, which preserves all rows from the left table (customers) even when there's no match in the joined tables.\n",
    "\n",
    "Looking at our results:\n",
    "- Emma Brown, Carlos Rodriguez, and others have `total_spent` values but NULL for `total_income` - they made purchases but have no income records\n",
    "- Hans Schmidt and Fatima Al-Said have NULL for both - they're in the customers table but have neither purchase nor income data\n",
    "- The first 10 customers have values for both - they exist in all three derived tables\n",
    "\n",
    "This flexibility of ```LEFT JOIN``` is exactly why it's so useful - it helps us identify missing data relationships while still showing all our customers.\n",
    "\n",
    "In this workshop we will explore both ```INNER JOIN``` (which we saw in the basic example) and ```LEFT JOIN``` in detail, along with ```SELF JOIN``` - but for those curious, there are others, such as ```RIGHT JOIN```, ```FULL JOIN``` AND ```CROSS JOIN```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfc799-e0cc-4276-9f5b-2163d390fa23",
   "metadata": {},
   "source": [
    "üìù **Poll 2:** You want *all* customers, even those with no orders. Which join keeps them? \n",
    "- customers `LEFT JOIN` orders\n",
    "- orders `LEFT JOIN` customers\n",
    "- Either order gives same result\n",
    "- Neither command would return all customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8d04f",
   "metadata": {},
   "source": [
    "## 2.3¬†Advanced Join Variants\n",
    "- **LEFT JOIN**: retains all rows from the left table.\n",
    "    - This is why it is important to distinguish which table is being used in the FROM statement, and which is being brought by the JOIN statement\n",
    "    - Specially useful to handle missing data \n",
    "- **SELF¬†JOIN**: the table is joined to itself\n",
    "    - We essentially deal with two tables - one of them being a duplicate of the first - and then JOIN them\n",
    "    - Very useful as a filtering tool\n",
    "\n",
    "üîî **Question:** Can anyone think of an example in which we might want to join a table with itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff72c16-fd7f-459d-b479-af7d0f14e3c3",
   "metadata": {},
   "source": [
    "![Set-logic Venn diagrams for LEFT, INNER and FULL joins with tiny code snippets and labelled A/B table overlaps.](../Images/sql-joins-venn-diagram.svg)\n",
    "---\n",
    "\n",
    "Let's understand what is going on with the different type of joins:\n",
    "- In a ```LEFT JOIN```, we start with all rows from table 1 - the \"left\" table. If there is a match in table 2, bring the extra columns, otherwise leave these columns ```NULL```\n",
    "- In a ```INNER JOIN```, we only keep the records that are on both tables! In other words, if one row is in one table but not the other, do not bring it to the merged table.\n",
    "- For a ```FULL JOIN```, we treat both tables as the \"left\" one - we keep every row from both sides, and if it does not have a matching record, fill the gaps with ```NULL```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4dcf00-96a2-4306-9dda-9090bf1aff07",
   "metadata": {},
   "source": [
    "üôã **Hands-Up:** True or False ‚Äî `INNER JOIN` can drop rows that exist in only one table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43d623-1e4a-4720-ba7c-5b171bab3fd3",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Why `LEFT JOIN` Order Matters\n",
    "\n",
    "`customers` lists everyone in our sample.  \n",
    "`income` records dollar amounts **only for those customers who reported income**.\n",
    "\n",
    "1. **Write two queries**  \n",
    "   * **Query A**: `customers  LEFT JOIN income`  \n",
    "   * **Query B**: `income     LEFT JOIN customers`\n",
    "2. For each query return just one column: `COUNT(*) AS row_count`.\n",
    "3. Compare the two row counts‚Äîexplain the difference in one sentence *(code comment is fine)*.\n",
    "\n",
    "> **Hint**  \n",
    "> In a `LEFT JOIN`, **all** rows from the table **before** `LEFT JOIN` are preserved, even when no match exists in the table **after** `LEFT JOIN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf27cc1f-a3da-4c75-b54c-50355784aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Query A rows: 31  |  Query B rows: 22 ‚Äî order matters!\n"
     ]
    }
   ],
   "source": [
    "qA = \"\"\"SELECT COUNT(*) AS n FROM customers\n",
    "        LEFT JOIN income ON customers.name = income.name\"\"\"\n",
    "qB = \"\"\"SELECT COUNT(*) AS n FROM income\n",
    "        LEFT JOIN customers ON customers.name = income.name\"\"\"\n",
    "nA = pd.read_sql_query(qA, conn)[\"n\"][0]\n",
    "nB = pd.read_sql_query(qB, conn)[\"n\"][0]\n",
    "\n",
    "assert nA >= nB, textwrap.dedent(f\"\"\"\n",
    "    Expected Query A (customers left) to have ‚â• rows than Query B,\n",
    "    but got {nA} vs {nB}.\n",
    "\"\"\")\n",
    "print(f\"‚úî Query A rows: {nA}  |  Query B rows: {nB} ‚Äî order matters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4bf71-1050-4c9b-b537-1c52b1a9d5a1",
   "metadata": {},
   "source": [
    "One consideration when using ```LEFT JOIN``` is that it returns ```NULL``` values for unmatched records - but we might want to handle these NULLs differently depending on our analysis needs. \n",
    "\n",
    "For example, in our data:\n",
    "- When Li Wei has a NULL amount in some income records, it means the income amount is unknown\n",
    "- When Emma Brown has NULL for total_income after our JOIN, it means she has no income records at all\n",
    "\n",
    "Sometimes we want to treat these NULLs as zeros (e.g., \"no income records\" = \"0 total income\" for a spending analysis), while other times we want to preserve them as unknown values.\n",
    "\n",
    "Luckily, SQL includes a function that allows us to tailor the behavior of ```NULL``` entries for a given query - the ```COALESCE``` command. The first argument of the function tells us which column to analyze, and the second entry what to replace ```NULL``` values by. Let's take a look at an example in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc6b67c5-bb7d-4f91-949d-a2afce6deb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name country  amount\n",
      "0       Ahmed Hassan      AU   800.0\n",
      "1       Ahmed Hassan      AU  1500.0\n",
      "2       Ahmed Hassan      AU  3500.0\n",
      "3        Aisha Patel      MX     NaN\n",
      "4       Anna Ivanova      TH     NaN\n",
      "5      Anna Kowalski      BR     NaN\n",
      "6   Carlos Rodriguez      IN     NaN\n",
      "7     Diego Martinez      NL  2700.0\n",
      "8        Elena Popov      ES  1800.0\n",
      "9         Emma Brown      FR     NaN\n",
      "10    Fatima Al-Said      TR     NaN\n",
      "11      Hans Schmidt      IT     NaN\n",
      "12    Isabella Silva      HK     NaN\n",
      "13       Ivan Petrov      SE     NaN\n",
      "14      James Wilson      CA     NaN\n",
      "15        John Smith      US   500.0\n",
      "16        John Smith      US  3000.0\n",
      "17          Jun Park      KR     NaN\n",
      "18     Lars Andersen    None  4500.0\n",
      "19            Li Wei      JP     NaN\n"
     ]
    }
   ],
   "source": [
    "# LEFT JOIN without COALESCE ‚Äì observe the NULL \"amount\" rows\n",
    "no_coalesce_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    i.amount            -- will be NULL if the customer has no income rows\n",
    "FROM   customers AS c\n",
    "LEFT JOIN income AS i\n",
    "       ON c.name = i.name\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(no_coalesce_query, conn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a611c37b-5dbb-4257-af31-8cb6d63e21f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name country  amount_received\n",
      "0       Ahmed Hassan      AU            800.0\n",
      "1       Ahmed Hassan      AU           1500.0\n",
      "2       Ahmed Hassan      AU           3500.0\n",
      "3        Aisha Patel      MX              0.0\n",
      "4       Anna Ivanova      TH              0.0\n",
      "5      Anna Kowalski      BR              0.0\n",
      "6   Carlos Rodriguez      IN              0.0\n",
      "7     Diego Martinez      NL           2700.0\n",
      "8        Elena Popov      ES           1800.0\n",
      "9         Emma Brown      FR              0.0\n",
      "10    Fatima Al-Said      TR              0.0\n",
      "11      Hans Schmidt      IT              0.0\n",
      "12    Isabella Silva      HK              0.0\n",
      "13       Ivan Petrov      SE              0.0\n",
      "14      James Wilson      CA              0.0\n",
      "15        John Smith      US            500.0\n",
      "16        John Smith      US           3000.0\n",
      "17          Jun Park      KR              0.0\n",
      "18     Lars Andersen    None           4500.0\n",
      "19            Li Wei      JP              0.0\n"
     ]
    }
   ],
   "source": [
    "# Same JOIN, but fill the missing income amounts with 0\n",
    "coalesce_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.country,\n",
    "    COALESCE(i.amount, 0) AS amount_received   -- replaces NULL with 0\n",
    "FROM   customers AS c\n",
    "LEFT JOIN income AS i\n",
    "       ON c.name = i.name\n",
    "ORDER  BY c.name\n",
    "LIMIT  20;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(coalesce_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de03177-d5d9-4548-8ca8-d23b76e3f101",
   "metadata": {},
   "source": [
    "### SELF JOIN - Advanced Topic, Time Permitting üï§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37315fe8-7ed7-4c04-85fc-b9a32020ca84",
   "metadata": {},
   "source": [
    "Now let's talk a bit about SELF JOIN. As we mentioned before, the idea here is to join a table with itself. This is one of the cases in which using aliases is extremely important, since, by construction, both of the tables will have all columns in common!\n",
    "\n",
    "Let's see an example on how this might work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "907ffd7a-e45e-4321-b17c-9ca66e7cca2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers with Similar Spending (within $50):\n",
      "\n",
      "        customer_1        customer_2  customer_1_spent  customer_2_spent  \\\n",
      "0    Sarah Johnson       Yuki Tanaka            301.15            299.36   \n",
      "1           Li Wei     Michel Dubois            155.98            143.16   \n",
      "2    Anna Kowalski  Carlos Rodriguez           1066.01           1049.40   \n",
      "3      Aisha Patel     Lars Andersen            109.14            125.88   \n",
      "4    Lars Andersen     Michel Dubois            125.88            143.16   \n",
      "5      Ivan Petrov         Raj Kumar            399.48            380.50   \n",
      "6     Ahmed Hassan            Li Wei            177.45            155.98   \n",
      "7     James Wilson     Sarah Johnson            331.04            301.15   \n",
      "8    Lars Andersen            Li Wei            125.88            155.98   \n",
      "9     James Wilson       Yuki Tanaka            331.04            299.36   \n",
      "10     Aisha Patel     Michel Dubois            109.14            143.16   \n",
      "11    Ahmed Hassan     Michel Dubois            177.45            143.16   \n",
      "12   Anna Kowalski         Lucy Chen           1066.01           1102.08   \n",
      "13  Diego Martinez       Elena Popov             13.09             56.37   \n",
      "14     Aisha Patel            Li Wei            109.14            155.98   \n",
      "15    James Wilson         Raj Kumar            331.04            380.50   \n",
      "\n",
      "    spending_difference  \n",
      "0                  1.79  \n",
      "1                 12.82  \n",
      "2                 16.61  \n",
      "3                 16.74  \n",
      "4                 17.28  \n",
      "5                 18.98  \n",
      "6                 21.47  \n",
      "7                 29.89  \n",
      "8                 30.10  \n",
      "9                 31.68  \n",
      "10                34.02  \n",
      "11                34.29  \n",
      "12                36.07  \n",
      "13                43.28  \n",
      "14                46.84  \n",
      "15                49.46  \n"
     ]
    }
   ],
   "source": [
    "# A self-join example: customers with similar spending levels\n",
    "spending_similarity_query = \"\"\"\n",
    "SELECT \n",
    "    c1.name AS customer_1,\n",
    "    c2.name AS customer_2,\n",
    "    c1.items_purchased * c1.price_per_item AS customer_1_spent,\n",
    "    c2.items_purchased * c2.price_per_item AS customer_2_spent,\n",
    "    ABS((c1.items_purchased * c1.price_per_item) - \n",
    "        (c2.items_purchased * c2.price_per_item)) AS spending_difference\n",
    "FROM \n",
    "    customers c1\n",
    "JOIN \n",
    "    customers c2 ON c1.name < c2.name\n",
    "WHERE \n",
    "    c1.items_purchased IS NOT NULL \n",
    "    AND c1.price_per_item IS NOT NULL\n",
    "    AND c2.items_purchased IS NOT NULL \n",
    "    AND c2.price_per_item IS NOT NULL\n",
    "    AND ABS((c1.items_purchased * c1.price_per_item) - \n",
    "            (c2.items_purchased * c2.price_per_item)) < 50\n",
    "ORDER BY \n",
    "    spending_difference;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Customers with Similar Spending (within $50):\\n\")\n",
    "print(pd.read_sql_query(spending_similarity_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9bb68-b741-4363-8a54-2e1ae415c1e8",
   "metadata": {},
   "source": [
    "There are a few things worth noticing here:\n",
    "- When we selected the columns from each table, it was important to rename them - since they had the same original names!\n",
    "- Notice that John did not report to anyone - so he was not included as an employee in the merged table. This could be adapted by using a LEFT JOIN\n",
    "- Some managers appear many times, since more than one employee reports to them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d9ff0-f6d2-46ed-8c95-1a26a3a013f4",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Customers Who Out-Spend Everyone Else in Their Country\n",
    "‚ÄúWho is the top spender in each country?‚Äù\n",
    "\n",
    "Idea:\n",
    "Self-JOIN customers to itself on country.\n",
    "For each row c1, look for a matching row c2 in the same country whose total_spent is higher.\n",
    "If no such c2 exists, c1 must be that country‚Äôs top spender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922c9ca4-1047-4d72-9416-b4bf989274af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>total_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>125.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sofia Santos</td>\n",
       "      <td>AE</td>\n",
       "      <td>595.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>AU</td>\n",
       "      <td>177.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anna Kowalski</td>\n",
       "      <td>BR</td>\n",
       "      <td>1066.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>James Wilson</td>\n",
       "      <td>CA</td>\n",
       "      <td>331.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>CN</td>\n",
       "      <td>299.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>DE</td>\n",
       "      <td>301.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lucy Chen</td>\n",
       "      <td>EG</td>\n",
       "      <td>1102.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>ES</td>\n",
       "      <td>56.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Emma Brown</td>\n",
       "      <td>FR</td>\n",
       "      <td>517.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Isabella Silva</td>\n",
       "      <td>HK</td>\n",
       "      <td>888.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Carlos Rodriguez</td>\n",
       "      <td>IN</td>\n",
       "      <td>1049.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>JP</td>\n",
       "      <td>155.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jun Park</td>\n",
       "      <td>KR</td>\n",
       "      <td>1847.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Aisha Patel</td>\n",
       "      <td>MX</td>\n",
       "      <td>109.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>NL</td>\n",
       "      <td>13.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Michel Dubois</td>\n",
       "      <td>RU</td>\n",
       "      <td>143.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ivan Petrov</td>\n",
       "      <td>SE</td>\n",
       "      <td>399.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Anna Ivanova</td>\n",
       "      <td>TH</td>\n",
       "      <td>957.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Raj Kumar</td>\n",
       "      <td>US</td>\n",
       "      <td>380.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name country  total_spent\n",
       "0      Mary Williams    None      1183.71\n",
       "1      Lars Andersen    None       125.88\n",
       "2       Sofia Santos      AE       595.32\n",
       "3       Ahmed Hassan      AU       177.45\n",
       "4      Anna Kowalski      BR      1066.01\n",
       "5       James Wilson      CA       331.04\n",
       "6        Yuki Tanaka      CN       299.36\n",
       "7      Sarah Johnson      DE       301.15\n",
       "8          Lucy Chen      EG      1102.08\n",
       "9        Elena Popov      ES        56.37\n",
       "10        Emma Brown      FR       517.44\n",
       "11    Isabella Silva      HK       888.00\n",
       "12  Carlos Rodriguez      IN      1049.40\n",
       "13            Li Wei      JP       155.98\n",
       "14          Jun Park      KR      1847.94\n",
       "15       Aisha Patel      MX       109.14\n",
       "16    Diego Martinez      NL        13.09\n",
       "17     Michel Dubois      RU       143.16\n",
       "18       Ivan Petrov      SE       399.48\n",
       "19      Anna Ivanova      TH       957.12\n",
       "20         Raj Kumar      US       380.50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_spenders_query = \"\"\"\n",
    "-- 1) Build total spending per customer\n",
    "WITH customer_spending AS (\n",
    "    SELECT\n",
    "           name,\n",
    "           country,\n",
    "           items_purchased * price_per_item AS total_spent\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    "       AND price_per_item  IS NOT NULL\n",
    ")\n",
    "\n",
    "-- 2) Self-JOIN filter: keep only rows with no higher spender in the same country\n",
    "SELECT\n",
    "       c1.name,\n",
    "       c1.country,\n",
    "       c1.total_spent\n",
    "FROM   customer_spending AS c1\n",
    "LEFT JOIN customer_spending AS c2\n",
    "       ON  c1.country      = c2.country\n",
    "       AND c2.total_spent  > c1.total_spent\n",
    "WHERE  c2.name IS NULL              -- ‚Üê nobody beats c1 in that country\n",
    "ORDER  BY c1.country, c1.total_spent DESC;\n",
    "\"\"\"\n",
    "pd.read_sql_query(top_spenders_query, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4927941-cc29-4616-881f-09d8000a270a",
   "metadata": {},
   "source": [
    "Of course, SELF JOINs can be used for purposes other than filtering. Another classic application is to find pairs that satisfy some criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f04d4262-d602-4e13-8c2e-ebb1819b64c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF JOIN Example - Customers from the Same City:\n",
      "Empty DataFrame\n",
      "Columns: [customer_1, customer_2, shared_city, country]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "self_join_query = \"\"\"\n",
    "SELECT \n",
    "    c1.name AS customer_1,\n",
    "    c2.name AS customer_2,\n",
    "    c1.city AS shared_city,\n",
    "    c1.country\n",
    "FROM \n",
    "    customers c1\n",
    "JOIN \n",
    "    customers c2 ON c1.city = c2.city \n",
    "                 AND c1.name < c2.name  -- Avoid duplicates and self-matches\n",
    "WHERE \n",
    "    c1.city IS NOT NULL\n",
    "ORDER BY \n",
    "    c1.city, c1.name;\n",
    "\"\"\"\n",
    "\n",
    "print(\"SELF JOIN Example - Customers from the Same City:\")\n",
    "result = pd.read_sql_query(self_join_query, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a43e1-367f-4754-9f42-300fe2846509",
   "metadata": {},
   "source": [
    "üìù **Poll 3:** When self-joining `customers` to find same-country pairs, what extra condition avoids duplicate & mirror rows? - `c1.id <> c2.id` - `c1.id < c2.id` - `c1.country IS NOT NULL`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1071b0",
   "metadata": {},
   "source": [
    "<a id='subqueries'></a>\n",
    "## 3¬†¬∑¬†Subqueries \n",
    "\n",
    "Estimated Time: 20 minutes\n",
    "\n",
    "**Definition.** As the name says, a subquery is a query contained in another query. This allows us to perform auxiliary queries, and then use the results of these queries in our main query. Unlike the main query, subqueries are temporary - they only exist while the instance of the query is being worked out. \n",
    "\n",
    "**Purpose.** Subqueries are very useful when one needs to break down a complex question into multiple manageable individual parts.For example, one might want to summarize or filter a given table, and use the summarized/filtered results as the input of another query. \n",
    "\n",
    "**Common Use¬†Case.** Getting summary statistics for each individual, while keeping information unrelated from the variable we are using to aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db71b04-8952-42cb-a923-bda8226e739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers ranked by total recorded income:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>total_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>AU</td>\n",
       "      <td>5800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Andersen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>London</td>\n",
       "      <td>GB</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>CN</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diego Martinez</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>NL</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>ES</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>DE</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Li Wei</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>JP</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name       city country  total_income\n",
       "0    Ahmed Hassan     Sydney      AU        5800.0\n",
       "1   Lars Andersen       None    None        4500.0\n",
       "2      John Smith   New York      US        3500.0\n",
       "3    Maria Garcia     London      GB        3500.0\n",
       "4     Yuki Tanaka   Shanghai      CN        3200.0\n",
       "5  Diego Martinez  Amsterdam      NL        2700.0\n",
       "6     Elena Popov     Madrid      ES        1800.0\n",
       "7   Mary Williams       None    None        1200.0\n",
       "8   Sarah Johnson     Berlin      DE         400.0\n",
       "9          Li Wei      Tokyo      JP         200.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Total income per person, then attach all customer metadata\n",
    "income_totals_query = \"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    it.total_income\n",
    "FROM   customers AS c\n",
    "JOIN  ( SELECT name,\n",
    "               SUM(amount) AS total_income\n",
    "        FROM   income\n",
    "        WHERE  amount IS NOT NULL      -- defensive\n",
    "        GROUP  BY name ) AS it\n",
    "      ON c.name = it.name\n",
    "ORDER BY it.total_income DESC;\n",
    "\"\"\"\n",
    "print(\"Customers ranked by total recorded income:\")\n",
    "display(pd.read_sql_query(income_totals_query, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4fe0-ab4a-4038-a0e8-49574684cb0d",
   "metadata": {},
   "source": [
    "There are two main ways of using subqueries:\n",
    "\n",
    "1) To Filter results.\n",
    "\n",
    "A classic example is to find all customers who spend more than average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a013097e-4c6d-479f-b147-74f3f89b9cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers who spend more than average:\n",
      "\n",
      "               name       city country  total_spent  average_spending\n",
      "0          Jun Park      Seoul      KR      1847.94            557.12\n",
      "1     Mary Williams       None    None      1183.71            557.12\n",
      "2         Lucy Chen      Cairo      EG      1102.08            557.12\n",
      "3     Anna Kowalski  S√£o Paulo      BR      1066.01            557.12\n",
      "4  Carlos Rodriguez     Mumbai      IN      1049.40            557.12\n",
      "5      Anna Ivanova    Bangkok      TH       957.12            557.12\n",
      "6    Isabella Silva  Hong Kong      HK       888.00            557.12\n",
      "7      Sofia Santos      Dubai      AE       595.32            557.12\n",
      "\n",
      " Average spending: $557.12\n"
     ]
    }
   ],
   "source": [
    "# Subquery example - Find customers who spend more than average\n",
    "subquery_example = \"\"\"\n",
    "WITH spending_stats AS (\n",
    "    SELECT \n",
    "        AVG(items_purchased * price_per_item) AS avg_spent,\n",
    "        MAX(items_purchased * price_per_item) AS max_spent,\n",
    "        MIN(items_purchased * price_per_item) AS min_spent\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL \n",
    "      AND price_per_item IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    c.items_purchased * c.price_per_item AS total_spent,\n",
    "    ROUND((SELECT avg_spent FROM spending_stats), 2) AS average_spending\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.items_purchased IS NOT NULL \n",
    "    AND c.price_per_item IS NOT NULL\n",
    "    AND (c.items_purchased * c.price_per_item) > (\n",
    "        SELECT avg_spent FROM spending_stats\n",
    "    )\n",
    "ORDER BY \n",
    "    total_spent DESC;\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(subquery_example, conn)\n",
    "print(\"Customers who spend more than average:\\n\")\n",
    "print(result)\n",
    "\n",
    "avg_spent = pd.read_sql_query(\"\"\"\n",
    "    SELECT ROUND(AVG(items_purchased * price_per_item), 2) as avg_spent\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL AND price_per_item IS NOT NULL\n",
    "\"\"\", conn).iloc[0,0]\n",
    "print(f\"\\n Average spending: ${avg_spent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114d258-6a7e-41a5-9b01-68943d9cac65",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Incorrect subquery references\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Cannot reference CTE columns directly\n",
    "WITH high_spenders AS (\n",
    "    SELECT name, SUM(amount) as total FROM income GROUP BY name\n",
    ")\n",
    "SELECT * FROM customers\n",
    "WHERE account_balance > high_spenders.total\n",
    "\n",
    "-- ‚úÖ CORRECT: Use subquery or JOIN with CTE\n",
    "WITH high_spenders AS (\n",
    "    SELECT name, SUM(amount) as total FROM income GROUP BY name\n",
    ")\n",
    "SELECT c.* FROM customers c\n",
    "JOIN high_spenders h ON c.name = h.name\n",
    "WHERE c.account_balance > h.total\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaab118-4ade-400c-8b7e-31666ee71539",
   "metadata": {},
   "source": [
    "2) As a derived table to query from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eb7cad2-619f-402c-b6df-7e9ff50b6296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>city_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  city  items_purchased  city_avg\n",
       "0  Mary Williams  None             17.0      11.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_query_sqlite = \"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    city,\n",
    "    items_purchased,\n",
    "    city_avg\n",
    "FROM (\n",
    "    SELECT\n",
    "        name,\n",
    "        city,\n",
    "        items_purchased,\n",
    "        AVG(items_purchased) OVER (PARTITION BY city) AS city_avg\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    ")\n",
    "WHERE  items_purchased > city_avg\n",
    "ORDER  BY city, items_purchased DESC;\n",
    "\"\"\"\n",
    "\n",
    "display(pd.read_sql_query(window_query_sqlite, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b4160-6b11-42f1-a8bb-20bf35c00d8f",
   "metadata": {},
   "source": [
    "As promised in the first workshop, we can also use subqueries in combination with IN to check for membership against entire tables - usually the result of a subquery.\n",
    "\n",
    "For example, let's say that we want to select the names of all customers who have purchased on Electronic item, but the information between consumers, products and purchases are all on separate tables. We could first JOIN them, then use a WHERE statement to filter them. But this would create a very large merged table. Instead, we can just use an IN statement with a subquery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656e9337-d9ab-4329-ba83-cf6c5217cea0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High income earners (>$5000):\n",
      "\n",
      "           name  total_income\n",
      "0  Ahmed Hassan        5800.0\n",
      "\n",
      " Customer details for high income earners: \n",
      "\n",
      "           name    city country  account_balance  total_spent\n",
      "0  Ahmed Hassan  Sydney      AU           179.64       177.45\n",
      "\n",
      " Customers with income data but earning <= $5000:\n",
      "\n",
      "             name       city  account_balance\n",
      "0      John Smith   New York           945.55\n",
      "1    Maria Garcia     London           905.34\n",
      "2     Elena Popov     Madrid           845.86\n",
      "3  Diego Martinez  Amsterdam           821.98\n",
      "4   Mary Williams       None           795.02\n"
     ]
    }
   ],
   "source": [
    "# Using IN with subquery to find customers with high income\n",
    "# First, let's see what income levels we have\n",
    "income_summary = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(amount) as total_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING SUM(amount) > 5000\n",
    "ORDER BY total_income DESC;\n",
    "\"\"\"\n",
    "print(\"High income earners (>$5000):\\n\")\n",
    "print(pd.read_sql_query(income_summary, conn))\n",
    "\n",
    "# Find customer details for high income earners\n",
    "high_income_customers_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.country,\n",
    "    c.account_balance,\n",
    "    c.items_purchased * c.price_per_item as total_spent\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.name IN (\n",
    "        SELECT name\n",
    "        FROM income\n",
    "        WHERE amount IS NOT NULL\n",
    "        GROUP BY name\n",
    "        HAVING SUM(amount) > 5000\n",
    "    )\n",
    "    AND c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    c.account_balance DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Customer details for high income earners: \\n\")\n",
    "result = pd.read_sql_query(high_income_customers_query, conn)\n",
    "print(result)\n",
    "\n",
    "# Compare with NOT IN - customers without high income\n",
    "low_income_customers_query = \"\"\"\n",
    "SELECT \n",
    "    c.name,\n",
    "    c.city,\n",
    "    c.account_balance\n",
    "FROM \n",
    "    customers c\n",
    "WHERE \n",
    "    c.name NOT IN (\n",
    "        SELECT name\n",
    "        FROM income\n",
    "        WHERE amount IS NOT NULL\n",
    "        GROUP BY name\n",
    "        HAVING SUM(amount) > 5000\n",
    "    )\n",
    "    AND c.name IN (SELECT DISTINCT name FROM income)  -- Only those with some income data\n",
    "    AND c.account_balance IS NOT NULL\n",
    "ORDER BY \n",
    "    c.account_balance DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Customers with income data but earning <= $5000:\\n\")\n",
    "print(pd.read_sql_query(low_income_customers_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b55343-835f-406a-958c-3fcc323ee2bf",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Using NOT IN with subqueries that may contain NULL values can return no results! This is because NULL comparisons are undefined. Use NOT EXISTS or filter out NULLs in your subquery with WHERE column IS NOT NULL to avoid this trap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bbebf-9552-4dd0-a4fe-947840ca76e4",
   "metadata": {},
   "source": [
    "ü•ä **Challenge:** List the name, country, and total_income of every customer whose total income is below the overall average customer income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b63d566f-6e4f-4668-bdc7-eebf010c3257",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT c.name, c.country, SUM(i.amount) AS total_income\nFROM customers AS c\nJOIN income AS i\n  ON c.name = i.name\nGROUP BY c.name, c.country\nWHERE SUM(i.amount) <\n      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n': near \"WHERE\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \"WHERE\": syntax error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Debug this intentional error\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bad_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT c.name, c.country, SUM(i.amount) AS total_income\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM customers AS c\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_sql_query(bad_query, conn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    527\u001b[0m         sql,\n\u001b[0;32m    528\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    529\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    530\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    531\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    532\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    533\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    534\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    535\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT c.name, c.country, SUM(i.amount) AS total_income\nFROM customers AS c\nJOIN income AS i\n  ON c.name = i.name\nGROUP BY c.name, c.country\nWHERE SUM(i.amount) <\n      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n': near \"WHERE\": syntax error"
     ]
    }
   ],
   "source": [
    "# Debug this intentional error\n",
    "\n",
    "bad_query = \"\"\"\n",
    "SELECT c.name, c.country, SUM(i.amount) AS total_income\n",
    "FROM customers AS c\n",
    "JOIN income AS i\n",
    "  ON c.name = i.name\n",
    "GROUP BY c.name, c.country\n",
    "WHERE SUM(i.amount) <\n",
    "      ( SELECT AVG(SUM(amount)) FROM income );  -- ‚ùå aggregate inside AVG; also WHERE not HAVING\n",
    "\"\"\"\n",
    "pd.read_sql_query(bad_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b769b",
   "metadata": {},
   "source": [
    "<a id='ctes'></a>\n",
    "## 4¬†¬∑¬†Common¬†Table¬†Expressions (CTEs) \n",
    "\n",
    "Estimated Time: 15 minutes\n",
    "\n",
    "Subqueries are a great way of breaking down a complex query into smaller, more manageable subparts. But very often these subqueries can become quite long, and given that one must include the full query inside another query, they can become very hard to read. \n",
    "\n",
    "CTEs are a way of solving this issue. Instead of rewriting the entire subquery inside the main query, we first give aliases to our subqueries, and then refer to them in the main query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fe831-d7d0-49b6-9900-441a4dd7b0da",
   "metadata": {},
   "source": [
    "![Infographic ‚ÄúCTEs vs Subqueries‚Äù showing when to use a readable CTE versus an inline subquery, with tiny code examples.](../Images/ctesubquery.svg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5921-5505-443d-95cf-71fb99815845",
   "metadata": {},
   "source": [
    "\n",
    "Basic Syntax   \n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte_name AS (\n",
    "    SELECT column1, column2, ...\n",
    "    FROM table\n",
    "    WHERE condition)\n",
    "SELECT * \n",
    "FROM cte_name;\n",
    "Key Components\n",
    "```\n",
    "\n",
    "WITH clause: Introduces one or more CTEs\n",
    "cte_name: Gives a name to the temporary result set\n",
    "Main query: References the CTE like a regular table\n",
    "\n",
    "üîî **Question:** Would you say that CTEs improve readability? Can you think of an example in which the code is easier to read using subqueries instead?\n",
    "\n",
    "Multiple CTEs\n",
    "\n",
    "```sql\n",
    "\n",
    "WITH cte1 AS (\n",
    "    SELECT column1 FROM table1\n",
    "),\n",
    "cte2 AS (\n",
    "    SELECT column2 FROM table2\n",
    ")\n",
    "SELECT *\n",
    "FROM cte1\n",
    "JOIN cte2 ON cte1.column = cte2.column;\n",
    "```\n",
    "\n",
    "üí° **Tip:** CTE's can be referenced multiple times in the same query, which improve readability and prevents errors!\n",
    "\n",
    "üôã **Hands-Up:** Which style feels clearer for multi-step queries so far? A. A nested subquery‚ÄÉB. A CTE (`WITH ‚Ä¶`) above the main query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "defeea3d",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "# CTE example using customers and income data\n",
    "cte_query = \"\"\"\n",
    "WITH customer_spending AS (\n",
    "    -- Calculate total spending per customer\n",
    "    SELECT \n",
    "        name,\n",
    "        items_purchased * price_per_item AS total_spent,\n",
    "        city,\n",
    "        country\n",
    "    FROM customers\n",
    "    WHERE items_purchased IS NOT NULL \n",
    "      AND price_per_item IS NOT NULL\n",
    "),\n",
    "customer_income AS (\n",
    "    -- Calculate total income per customer\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(amount) AS total_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    "),\n",
    "spending_analysis AS (\n",
    "    -- Combine spending and income data\n",
    "    SELECT \n",
    "        cs.name,\n",
    "        cs.city,\n",
    "        cs.country,\n",
    "        cs.total_spent,\n",
    "        ci.total_income,\n",
    "        ROUND(cs.total_spent * 100.0 / ci.total_income, 2) AS spending_rate\n",
    "    FROM customer_spending cs\n",
    "    JOIN customer_income ci ON cs.name = ci.name\n",
    "    WHERE ci.total_income > 0\n",
    ")\n",
    "-- Final query using the CTEs\n",
    "SELECT \n",
    "    name,\n",
    "    city,\n",
    "    country,\n",
    "    total_spent,\n",
    "    total_income,\n",
    "    spending_rate,\n",
    "    CASE \n",
    "        WHEN spending_rate > 50 THEN 'High Spender'\n",
    "        WHEN spending_rate > 20 THEN 'Moderate Spender'\n",
    "        ELSE 'Low Spender'\n",
    "    END AS spending_category\n",
    "FROM spending_analysis\n",
    "ORDER BY spending_rate DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87d802-6a5f-4140-8cc4-9901cc65b6af",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** We cannot reference CTEs directly in a query - this is what we call (lack of) \"portability\".\n",
    "\n",
    "```sql\n",
    "WITH average_data AS (\n",
    "    SELECT AVG(value) AS avg_value FROM table\n",
    ")\n",
    "SELECT *\n",
    "FROM other_table\n",
    "WHERE value > average_data.avg_value \n",
    "```\n",
    "Whenever we want to reference a CTE, we must either use a subquery:\n",
    "\n",
    "```sql\n",
    "WHERE value > (SELECT avg_value FROM average_data)\n",
    "```\n",
    "\n",
    "Or JOIN with the CTE:\n",
    "\n",
    "```sql\n",
    "JOIN average_data ON 1=1\n",
    "WHERE value > average_data.avg_value\n",
    "```\n",
    "\n",
    "üí° **Tip:** In this last example, we used a common trick: choose an expression that is always true to add a constant column to the table.\n",
    "\n",
    "üôã **Hands-Up:** Which keyword starts a Common Table Expression? A. `WITH`‚ÄÉB. `WHERE`‚ÄÉC. `AS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5ac0",
   "metadata": {},
   "source": [
    "<a id='pivot'></a>\n",
    "## 5¬†¬∑¬†Pivoting and Unpivoting (Or Melting) \n",
    "\n",
    "Estimated Time: 15 minutes\n",
    "\n",
    "Pivoting is the process of turning rows into columns, Unpivoting (also called Melting) is the inverse process. \n",
    "\n",
    "A common application is when we would like to group information that applies to the same individual. For example, we might have a list of all the different transactions that different clients made, including amount and date. But maybe we would like to understand how purchases vary across the days of the week for each given customer. So we can turn a table from having many rows and three columns, to having one row for each consumer, and 8 columns: one representing the customer name (or any other identification), and one with the transaction amount for that customer on each day of the week. \n",
    "\n",
    "üìù **Poll 4:** In data-reshaping lingo, which operation produces a **tall (long-format)** table, and which yields a **wide (spread)** table?\n",
    "\n",
    "- Melt (gather) ‚Üí tall‚ÄÉ|‚ÄÉPivot (spread) ‚Üí wide  \n",
    "- Melt (gather) ‚Üí wide‚ÄÉ|‚ÄÉPivot (spread) ‚Üí tall  \n",
    "- Both operations create tall tables  \n",
    "- Both operations create wide tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1cb8c-c211-4a9d-8a2e-dbf1865208c6",
   "metadata": {},
   "source": [
    "![Side-by-side pivot vs melt diagram converting wide daily-sales columns into tall name-date-sales rows and back again.](../Images/pivot-melt-diagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6310f20-e0f7-482e-9b02-fb108c55722f",
   "metadata": {},
   "source": [
    "This is done by using the ```UNION``` (or `UNION ALL`) functions - which are analogues to `JOIN`s, but instead of putting different columns side by side, they combine results from different queries on top of each other. \n",
    "\n",
    "The main difference between `UNION` and `UNION ALL` is that the former removes duplicate rows, which increases the computational cost of the function.\n",
    "\n",
    "Two additional observations:\n",
    "- The column names will be taken from the first one selected\n",
    "- All `SELECT` statements must have the same number of columns, and corresponding columns must have the same data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd85fc76-f262-4ceb-8811-34eef58e8412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Income Data (first 10 rows):\n",
      "            name income_source  amount        day\n",
      "0     John Smith        Salary  3000.0     Monday\n",
      "1     John Smith     Freelance   500.0     Friday\n",
      "2   Maria Garcia        Salary  2500.0     Monday\n",
      "3   Maria Garcia         Bonus  1000.0     Friday\n",
      "4         Li Wei        Salary     NaN     Monday\n",
      "5         Li Wei    Investment   200.0  Wednesday\n",
      "6   Ahmed Hassan        Salary  3500.0     Monday\n",
      "7   Ahmed Hassan    Consulting  1500.0    Tuesday\n",
      "8   Ahmed Hassan        Rental   800.0   Thursday\n",
      "9  Sarah Johnson        Salary     NaN     Monday\n",
      "\\nPivoted Income Data (by day of week):\n",
      "               name  Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  \\\n",
      "0      Ahmed Hassan  3500.0   1500.0        0.0     800.0     0.0       0.0   \n",
      "1     Lars Andersen  4500.0      0.0        0.0       0.0     0.0       0.0   \n",
      "2    Patricia Davis     0.0      0.0     4000.0       0.0     0.0       0.0   \n",
      "3     Robert Taylor  2900.0      0.0        0.0       0.0     0.0     600.0   \n",
      "4      Maria Garcia  2500.0      0.0        0.0       0.0  1000.0       0.0   \n",
      "5        John Smith  3000.0      0.0        0.0       0.0   500.0       0.0   \n",
      "6     Michael Brown  3400.0      0.0        0.0       0.0     0.0       0.0   \n",
      "7       Yuki Tanaka  3200.0      0.0        0.0       0.0     0.0       0.0   \n",
      "8   Jennifer Wilson  3100.0      0.0        0.0       0.0     0.0       0.0   \n",
      "9    Diego Martinez  2700.0      0.0        0.0       0.0     0.0       0.0   \n",
      "10       Amanda Lee     0.0      0.0        0.0    2200.0     0.0       0.0   \n",
      "11      Elena Popov     0.0      0.0        0.0       0.0  1800.0       0.0   \n",
      "12    Mary Williams     0.0   1200.0        0.0       0.0     0.0       0.0   \n",
      "13    Sarah Johnson     0.0      0.0      400.0       0.0     0.0       0.0   \n",
      "14           Li Wei     0.0      0.0      200.0       0.0     0.0       0.0   \n",
      "\n",
      "    total_weekly_income  \n",
      "0                5800.0  \n",
      "1                4500.0  \n",
      "2                4000.0  \n",
      "3                3500.0  \n",
      "4                3500.0  \n",
      "5                3500.0  \n",
      "6                3400.0  \n",
      "7                3200.0  \n",
      "8                3100.0  \n",
      "9                2700.0  \n",
      "10               2200.0  \n",
      "11               1800.0  \n",
      "12               1200.0  \n",
      "13                400.0  \n",
      "14                200.0  \n"
     ]
    }
   ],
   "source": [
    "# Using the income table for pivot example\n",
    "print(\"Original Income Data (first 10 rows):\")\n",
    "income_sample = pd.read_sql_query(\"SELECT * FROM income LIMIT 10\", conn)\n",
    "print(income_sample)\n",
    "\n",
    "# Pivot income by day of week\n",
    "pivot_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(CASE WHEN day = 'Monday' THEN amount ELSE 0 END) AS Monday,\n",
    "    SUM(CASE WHEN day = 'Tuesday' THEN amount ELSE 0 END) AS Tuesday,\n",
    "    SUM(CASE WHEN day = 'Wednesday' THEN amount ELSE 0 END) AS Wednesday,\n",
    "    SUM(CASE WHEN day = 'Thursday' THEN amount ELSE 0 END) AS Thursday,\n",
    "    SUM(CASE WHEN day = 'Friday' THEN amount ELSE 0 END) AS Friday,\n",
    "    SUM(CASE WHEN day = 'Saturday' THEN amount ELSE 0 END) AS Saturday,\n",
    "    SUM(amount) AS total_weekly_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING total_weekly_income > 0\n",
    "ORDER BY total_weekly_income DESC;\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql_query(pivot_query, conn)\n",
    "print(\"\\\\nPivoted Income Data (by day of week):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b69a3-735b-415d-ae8d-97dac450b683",
   "metadata": {},
   "source": [
    "Melting, the opposite process, is very useful when we want to do analysis regarding a variable that is not the one determining the rows. For example, say that we have sales data in which each row corresponds to a different product, and different columns represent different years. If we instead want to analyze the expenditure in different years, we can melt the table, and then use grouping or filtering to select a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bdaf070-917d-4960-b7b0-3dabfb81ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted Income Data (Wide Format):\n",
      "\n",
      "              name  salary_income  freelance_income  investment_income  \\\n",
      "0     Ahmed Hassan         3500.0               0.0                  0   \n",
      "1       Amanda Lee            0.0            2200.0                  0   \n",
      "2   Diego Martinez         2700.0               0.0                  0   \n",
      "3  Jennifer Wilson         3100.0               0.0                  0   \n",
      "4       John Smith         3000.0             500.0                  0   \n",
      "\n",
      "   consulting_income  \n",
      "0             1500.0  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n",
      "\n",
      " Melted Income Data (Long Format):\n",
      "\n",
      "              name income_type  amount\n",
      "0     Ahmed Hassan  Consulting  1500.0\n",
      "1     Ahmed Hassan      Salary  3500.0\n",
      "2       Amanda Lee   Freelance  2200.0\n",
      "3   Diego Martinez      Salary  2700.0\n",
      "4  Jennifer Wilson      Salary  3100.0\n",
      "5       John Smith   Freelance   500.0\n",
      "6       John Smith      Salary  3000.0\n",
      "7    Lars Andersen      Salary  4500.0\n",
      "8           Li Wei  Investment   200.0\n",
      "9     Maria Garcia      Salary  2500.0\n",
      "\n",
      " This demonstrates how UNION ALL can transform wide data back to long format!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a pivoted view of income by source\n",
    "pivoted_income = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    SUM(CASE WHEN income_source = 'Salary' THEN amount ELSE 0 END) AS salary_income,\n",
    "    SUM(CASE WHEN income_source = 'Freelance' THEN amount ELSE 0 END) AS freelance_income,\n",
    "    SUM(CASE WHEN income_source = 'Investment' THEN amount ELSE 0 END) AS investment_income,\n",
    "    SUM(CASE WHEN income_source = 'Consulting' THEN amount ELSE 0 END) AS consulting_income\n",
    "FROM income\n",
    "WHERE amount IS NOT NULL\n",
    "GROUP BY name\n",
    "HAVING (salary_income + freelance_income + investment_income + consulting_income) > 0\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pivoted Income Data (Wide Format):\\n\")\n",
    "pivoted_df = pd.read_sql_query(pivoted_income, conn)\n",
    "print(pivoted_df)\n",
    "\n",
    "# Now melt it back to long format using UNION ALL\n",
    "melt_query = \"\"\"\n",
    "WITH pivoted_data AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(CASE WHEN income_source = 'Salary'    THEN amount ELSE 0 END) AS salary_income,\n",
    "        SUM(CASE WHEN income_source = 'Freelance' THEN amount ELSE 0 END) AS freelance_income,\n",
    "        SUM(CASE WHEN income_source = 'Investment' THEN amount ELSE 0 END) AS investment_income,\n",
    "        SUM(CASE WHEN income_source = 'Consulting' THEN amount ELSE 0 END) AS consulting_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    "),\n",
    "melted AS (\n",
    "    SELECT name, 'Salary'    AS income_type, salary_income    AS amount FROM pivoted_data WHERE salary_income    > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Freelance' AS income_type, freelance_income AS amount FROM pivoted_data WHERE freelance_income > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Investment' AS income_type, investment_income AS amount FROM pivoted_data WHERE investment_income > 0\n",
    "    UNION ALL\n",
    "    SELECT name, 'Consulting' AS income_type, consulting_income AS amount FROM pivoted_data WHERE consulting_income > 0\n",
    ")\n",
    "SELECT *\n",
    "FROM   melted\n",
    "ORDER  BY name, income_type\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n Melted Income Data (Long Format):\\n\")\n",
    "melted_df = pd.read_sql_query(melt_query, conn)\n",
    "print(melted_df)\n",
    "\n",
    "print(\"\\n This demonstrates how UNION ALL can transform wide data back to long format!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c8c90",
   "metadata": {},
   "source": [
    "<a id='window'></a>\n",
    "## 6¬†¬∑¬†Window Functions \n",
    "\n",
    "Estimated Time: 25 minutes\n",
    "\n",
    "**Definition.** A window is a subset of the table that is related in some prespecified way, in a very similar way that `GROUP BY` operates - with the crucial difference that it does not collapse the different rows into one, which allows us to preserve information. \n",
    "\n",
    "Window Functions operate separately into different windows. Common examples are\n",
    "\n",
    "- `RANK` - allow us to obtain the rank for different observations inside a given group. For example, we might want to see how much a consumer spent on his first purchase. So we would create a window for each consumer, apply the RANK window function, and then select those that have rank = 1.\n",
    "- `DENSE_RANK` - Similar to rank, but RANK jumps ranks if there are ties, while DENSE_RANK always has rankings as consecutive numbers. You might have seen this happening in college rankings!\n",
    "- `ROW_NUMBER()` - returns the row number of each observation. Very useful when the table lacks a primary key.\n",
    "- `SUM` - when used as a window function, allow us to perform cumulative sums - for example, cumulative sales up to a given date by each salesperson.\n",
    "- `AVG`/`MAX`/`MIN` - same as their aggregate versions, but allowing to keep information rather than collapsing rows.\n",
    "- `LAG`/`LEAD` - very useful in the context of time series, allow us to look at the previous/next value of a series.\n",
    "\n",
    "Basic Syntax: `WINDOW_FUNCTION() OVER (PARTITION BY columns ORDER BY columns)`\n",
    "\n",
    "Exception: `SUM(column) OVER (PARTITION BY columns)`\n",
    "\n",
    "üôã **Hands-Up:** Is the difference between a *window function* and an *aggregate function* clear? A. Yes‚ÄÉB. Not yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb058ef2-7ac0-4545-b861-9b596ad2e0bc",
   "metadata": {},
   "source": [
    "![Table annotated with running totals and row numbers to visualize how SUM() OVER and RANK window functions scan partitions.\n",
    "](../Images/window-function-visualization.svg)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51754da-779a-4dd4-a8e4-1db0f519ab41",
   "metadata": {},
   "source": [
    "### Example 1: ```ROW_NUMBER``` - Rank customers by account balance within each country\n",
    "\n",
    "Unfortunately, sometimes we work with tables that do not have a primary key. A classic example is one listing many different transactions, including information such as customer, store, amount, method of payment - but no order id. ```ROW_NUMBER``` is a useful window function in this scenario, as it creates an additional column with, as the name suggests, the numbers of each column in the particular ordering the table is in. This works as a \"fake\" primary key that we can still use to perform operations such as ```JOIN```, even without a true primary key.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Forgetting the PARTITION BY clause in window functions will calculate across the entire table, not within groups. This is a common mistake that can lead to incorrect business logic. Always double-check that your window function includes the appropriate PARTITION BY when you need group-wise calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "346024b6-6310-4d46-af29-b399023b4ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ROW_NUMBER Example - Customer Rankings by Country:\n",
      "                name country  account_balance  balance_rank_in_country\n",
      "0       Sofia Santos      AE           352.84                        1\n",
      "1       Ahmed Hassan      AU           179.64                        1\n",
      "2      Anna Kowalski      BR           392.80                        1\n",
      "3       James Wilson      CA           449.81                        1\n",
      "4        Yuki Tanaka      CN           344.21                        1\n",
      "5          Lucy Chen      EG           167.10                        1\n",
      "6        Elena Popov      ES           845.86                        1\n",
      "7         Emma Brown      FR           929.69                        1\n",
      "8       Maria Garcia      GB           905.34                        1\n",
      "9     Isabella Silva      HK           833.92                        1\n",
      "10  Carlos Rodriguez      IN           140.70                        1\n",
      "11      Hans Schmidt      IT           104.97                        1\n",
      "12            Li Wei      JP           638.11                        1\n",
      "13          Jun Park      KR           756.11                        1\n",
      "14       Aisha Patel      MX           226.83                        1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "row_number_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    country,\n",
    "    account_balance,\n",
    "    ROW_NUMBER() OVER (PARTITION BY country ORDER BY account_balance DESC) as balance_rank_in_country\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL \n",
    "  AND account_balance IS NOT NULL\n",
    "ORDER BY country, balance_rank_in_country\n",
    "LIMIT 15;\n",
    "\"\"\"\n",
    "print(\"1. ROW_NUMBER Example - Customer Rankings by Country:\")\n",
    "print(pd.read_sql_query(row_number_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde79739-2e95-4867-8ce8-d7d8dffb00ab",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: Common Mistake - Missing PARTITION BY in window functions\n",
    "\n",
    "```sql\n",
    "-- ‚ùå WRONG: Ranks ALL rows together, not by group\n",
    "SELECT name, country, balance,\n",
    "       RANK() OVER (ORDER BY balance DESC) as rank\n",
    "FROM customers\n",
    "\n",
    "-- ‚úÖ CORRECT: Use PARTITION BY to rank within groups\n",
    "SELECT name, country, balance,\n",
    "       RANK() OVER (PARTITION BY country ORDER BY balance DESC) as rank\n",
    "FROM customers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b75f2-922e-4309-91f1-2c6d11513b33",
   "metadata": {},
   "source": [
    "### Example 2: Running total - Cumulative spending by purchase date\n",
    "\n",
    "A very common application in business is to understand not just the spending or revenue in a given day, but the total amount spent/received up to a given date. This is the idea of a cumulative sum - the running total including previous purchases. In SQL, we can use the Window Function versiom of ```SUM``` - which is just like its aggregate function counterpart, but includes the characteristic OVER () component of window functions. This can be used to indicate both how we want to partition the dataset, and how we would like to order the subgroups, since the order matters when calculating cumulative sums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6062ae42-4f68-4db2-91eb-ba52042d6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Running Total Example - Cumulative Spending Over Time:\n",
      "            name last_purchase  purchase_amount  running_total\n",
      "0     Emma Brown    2024-01-28           517.44         517.44\n",
      "1   James Wilson    2024-02-02           331.04         848.48\n",
      "2    Ivan Petrov    2024-02-04           399.48        1247.96\n",
      "3         Li Wei    2024-02-10           155.98        1403.94\n",
      "4    Yuki Tanaka    2024-02-17           299.36        1703.30\n",
      "5    Elena Popov    2024-03-02            56.37        1759.67\n",
      "6  Lars Andersen    2024-04-08           125.88        1885.55\n",
      "7      Raj Kumar    2024-04-10           380.50        2266.05\n",
      "8   Ahmed Hassan    2024-05-14           177.45        2443.50\n",
      "9    Aisha Patel    2024-06-20           109.14        2552.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "running_total_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    last_purchase,\n",
    "    items_purchased * price_per_item as purchase_amount,\n",
    "    SUM(items_purchased * price_per_item) \n",
    "        OVER (ORDER BY last_purchase ROWS UNBOUNDED PRECEDING) as running_total\n",
    "FROM customers\n",
    "WHERE last_purchase IS NOT NULL \n",
    "  AND items_purchased IS NOT NULL \n",
    "  AND price_per_item IS NOT NULL\n",
    "ORDER BY last_purchase\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n2. Running Total Example - Cumulative Spending Over Time:\")\n",
    "print(pd.read_sql_query(running_total_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862a74c-002b-42f7-9488-76c22bea1a52",
   "metadata": {},
   "source": [
    "### Example 3: AVG with PARTITION BY - Compare to group average\n",
    "\n",
    "One of the main motivations for the use of window functions is precisely to have an alternative to ```GROUP BY``` that still allow us to retain individual level information. This is very useful when trying to compare each individual element to a group it belongs - for example, compare the customer balance with the average balance for customers in his country. Without window functions, we would need to first aggregate the data using an aggregate function with ```GROUP BY```, and then use ```JOIN``` to make a new table, and then perform the comparison. Window functions allow us to do this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd58ac6-6561-4202-a116-527659c8a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3. Window AVG Example - Customer Balance vs Country Average:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>account_balance</th>\n",
       "      <th>country_avg_balance</th>\n",
       "      <th>diff_from_country_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sofia Santos</td>\n",
       "      <td>AE</td>\n",
       "      <td>352.84</td>\n",
       "      <td>352.84</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmed Hassan</td>\n",
       "      <td>AU</td>\n",
       "      <td>179.64</td>\n",
       "      <td>179.64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anna Kowalski</td>\n",
       "      <td>BR</td>\n",
       "      <td>392.80</td>\n",
       "      <td>392.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James Wilson</td>\n",
       "      <td>CA</td>\n",
       "      <td>449.81</td>\n",
       "      <td>449.81</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yuki Tanaka</td>\n",
       "      <td>CN</td>\n",
       "      <td>344.21</td>\n",
       "      <td>344.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lucy Chen</td>\n",
       "      <td>EG</td>\n",
       "      <td>167.10</td>\n",
       "      <td>167.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elena Popov</td>\n",
       "      <td>ES</td>\n",
       "      <td>845.86</td>\n",
       "      <td>845.86</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Emma Brown</td>\n",
       "      <td>FR</td>\n",
       "      <td>929.69</td>\n",
       "      <td>929.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maria Garcia</td>\n",
       "      <td>GB</td>\n",
       "      <td>905.34</td>\n",
       "      <td>905.34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Isabella Silva</td>\n",
       "      <td>HK</td>\n",
       "      <td>833.92</td>\n",
       "      <td>833.92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name country  account_balance  country_avg_balance  \\\n",
       "0    Sofia Santos      AE           352.84               352.84   \n",
       "1    Ahmed Hassan      AU           179.64               179.64   \n",
       "2   Anna Kowalski      BR           392.80               392.80   \n",
       "3    James Wilson      CA           449.81               449.81   \n",
       "4     Yuki Tanaka      CN           344.21               344.21   \n",
       "5       Lucy Chen      EG           167.10               167.10   \n",
       "6     Elena Popov      ES           845.86               845.86   \n",
       "7      Emma Brown      FR           929.69               929.69   \n",
       "8    Maria Garcia      GB           905.34               905.34   \n",
       "9  Isabella Silva      HK           833.92               833.92   \n",
       "\n",
       "   diff_from_country_avg  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "5                    0.0  \n",
       "6                    0.0  \n",
       "7                    0.0  \n",
       "8                    0.0  \n",
       "9                    0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_comparison_query = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    country,\n",
    "    account_balance,\n",
    "    ROUND(AVG(account_balance) OVER (PARTITION BY country), 2) as country_avg_balance,\n",
    "    ROUND(account_balance - AVG(account_balance) OVER (PARTITION BY country), 2) as diff_from_country_avg\n",
    "FROM customers\n",
    "WHERE country IS NOT NULL \n",
    "  AND account_balance IS NOT NULL\n",
    "ORDER BY country, diff_from_country_avg DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n 3. Window AVG Example - Customer Balance vs Country Average:\")\n",
    "pd.read_sql_query(avg_comparison_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a5d20-deb4-4493-9782-005b44b74094",
   "metadata": {},
   "source": [
    "### Example 4: RANK vs DENSE_RANK - Ranking with ties\n",
    "\n",
    "Another common example is to obtain the relative rank of observations in a given group. This is an incredibly complex task without window function - one would need to find all the possible groups, use ```WHERE``` commands to isolate them, order them, and then merge all of the isolated groups together. ```RANK``` allows us to perform this operation in a single command. \n",
    "\n",
    "One interesting feature of ```RANK``` is that it leaves gaps after ties. This is common procedure in some applications - for example college rankings. However we might want to not allow for these gaps, and just allow for multiple entries at the same rank. This is exactly what ```DENSE_RANK``` does - dense here referring to the fact that there are no gaps in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55f5cacb",
   "metadata": {
    "language": "sql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4. RANK vs DENSE_RANK Example:\n",
      "              name  total_income  rank_with_gaps  dense_rank_no_gaps\n",
      "0     Ahmed Hassan        5800.0               1                   1\n",
      "1    Lars Andersen        4500.0               2                   2\n",
      "2   Patricia Davis        4000.0               3                   3\n",
      "3       John Smith        3500.0               4                   4\n",
      "4     Maria Garcia        3500.0               4                   4\n",
      "5    Robert Taylor        3500.0               4                   4\n",
      "6    Michael Brown        3400.0               7                   5\n",
      "7      Yuki Tanaka        3200.0               8                   6\n",
      "8  Jennifer Wilson        3100.0               9                   7\n",
      "9   Diego Martinez        2700.0              10                   8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rank_comparison_query = \"\"\"\n",
    "WITH income_totals AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        SUM(amount) as total_income\n",
    "    FROM income\n",
    "    WHERE amount IS NOT NULL\n",
    "    GROUP BY name\n",
    ")\n",
    "SELECT \n",
    "    name,\n",
    "    total_income,\n",
    "    RANK() OVER (ORDER BY total_income DESC) as rank_with_gaps,\n",
    "    DENSE_RANK() OVER (ORDER BY total_income DESC) as dense_rank_no_gaps\n",
    "FROM income_totals\n",
    "ORDER BY total_income DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"\\n 4. RANK vs DENSE_RANK Example:\")\n",
    "print(pd.read_sql_query(rank_comparison_query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba51a9-2c3d-4851-861f-92e3a59ebc90",
   "metadata": {},
   "source": [
    "üèãÔ∏è‚Äç‚ôÇÔ∏è Challenge ‚Äî Customers Who Out-Spend Their Country Average\n",
    "(Window-Function Edition)\n",
    "\n",
    "In the previous challenge you solved this task with the standard method:\n",
    "GROUP BY ‚Üí sub-query ‚Üí JOIN.\n",
    "\n",
    "Now let‚Äôs do the same calculation with a single window function.\n",
    "\n",
    "Goal: List every customer whose total spending ( items_purchased √ó price_per_item ) is strictly higher than the average spending of customers in the same country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdbbdd0c-11be-4a7d-8dd6-28e51d3e9070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>total_spent</th>\n",
       "      <th>country_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary Williams</td>\n",
       "      <td>None</td>\n",
       "      <td>1183.71</td>\n",
       "      <td>654.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name country  total_spent  country_avg\n",
       "0  Mary Williams    None      1183.71      654.795"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_avg_window_sql = \"\"\"\n",
    "WITH spending AS (\n",
    "    SELECT\n",
    "           name,\n",
    "           country,\n",
    "           items_purchased * price_per_item AS total_spent,\n",
    "           AVG(items_purchased * price_per_item)\n",
    "                 OVER (PARTITION BY country) AS country_avg\n",
    "    FROM   customers\n",
    "    WHERE  items_purchased IS NOT NULL\n",
    "      AND  price_per_item  IS NOT NULL\n",
    ")\n",
    "SELECT *\n",
    "FROM   spending\n",
    "WHERE  total_spent > country_avg\n",
    "ORDER  BY country, total_spent DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(country_avg_window_sql, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5bb3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "## Key Points\n",
    "\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "* **JOINs combine tables on matching values** - Use ON or USING to specify the relationship between tables\n",
    "* **LEFT JOIN preserves all rows from the left table** - Even when there's no match in the right table, making it ideal for finding missing relationships\n",
    "* **CTEs improve readability** - Use WITH clauses to break complex queries into named, reusable components\n",
    "* **Window functions maintain row-level detail** - Unlike GROUP BY, they calculate aggregates while keeping all original rows\n",
    "* **PARTITION BY creates calculation groups** - Similar to GROUP BY but for window functions\n",
    "* **Subqueries can filter or transform data** - Use them in WHERE clauses for filtering or FROM clauses as derived tables\n",
    "* **COALESCE handles NULL values gracefully** - Replace NULLs with meaningful defaults in JOINs and calculations\n",
    "* **Self-JOINs compare rows within the same table** - Essential for finding relationships between records in a single table\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
